{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T22:34:41.806855Z",
     "start_time": "2025-02-16T22:34:39.774177Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv  # Changed from DenseSAGEConv\n",
    "from torch_geometric.nn import global_mean_pool  # For sparse pooling\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define data directory\n",
    "data_dir = 'data/SCOP'  # Base directory for SCOP data\n",
    "\n",
    "# Rest of the model code remains the same..."
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:50:02.735310Z",
     "start_time": "2025-02-16T22:50:02.700554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SparseSCOPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        # Set up the root path first\n",
    "        self.root = root\n",
    "\n",
    "        # Now we can set up the class_info_path\n",
    "        self.class_info_path = os.path.join(root, 'raw/class_info.csv')\n",
    "\n",
    "        # Dictionary to map SCOP classes to indices\n",
    "        self.class_mapping = {\n",
    "            'a': 0,  # All-alpha\n",
    "            'b': 1,  # All-beta\n",
    "            'c': 2,  # Alpha/beta\n",
    "            'd': 3,  # Alpha+beta\n",
    "            'e': 4,  # Multi-domain\n",
    "            'f': 5,  # Membrane\n",
    "            'g': 6   # Small proteins\n",
    "        }\n",
    "\n",
    "        # Dictionary to map amino acids to indices\n",
    "        self.amino_acids = {\n",
    "            'ALA': 0, 'ARG': 1, 'ASN': 2, 'ASP': 3, 'CYS': 4,\n",
    "            'GLN': 5, 'GLU': 6, 'GLY': 7, 'HIS': 8, 'ILE': 9,\n",
    "            'LEU': 10, 'LYS': 11, 'MET': 12, 'PHE': 13, 'PRO': 14,\n",
    "            'SER': 15, 'THR': 16, 'TRP': 17, 'TYR': 18, 'VAL': 19,\n",
    "            'UNK': 20  # Unknown amino acid\n",
    "        }\n",
    "\n",
    "        # Load class information\n",
    "        if os.path.exists(self.class_info_path):\n",
    "            self.class_info = pd.read_csv(self.class_info_path)\n",
    "            print(f\"Found class info file with {len(self.class_info)} entries\")\n",
    "        else:\n",
    "            print(f\"Warning: class_info.csv not found at {self.class_info_path}\")\n",
    "            self.class_info = None\n",
    "\n",
    "        # Initialize the base class last\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def raw_file_names(self):\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"List of raw file names in the dataset.\"\"\"\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"List of processed file names in the dataset.\"\"\"\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        pass  # We already have the files\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Process the raw data into the internal format with comprehensive logging.\"\"\"\n",
    "        data_list = []\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        skipped_reasons = {}\n",
    "\n",
    "        # Log total number of entries in class_info\n",
    "        print(f\"Total entries in class_info: {len(self.class_info)}\")\n",
    "\n",
    "        # Verify raw directory contents\n",
    "        raw_dir = os.path.join(self.root, 'raw')\n",
    "        raw_files = [f for f in os.listdir(raw_dir) if f.endswith('.pdb')]\n",
    "        print(f\"Total PDB files in raw directory: {len(raw_files)}\")\n",
    "\n",
    "        for idx, row in self.class_info.iterrows():\n",
    "            pdb_id = str(row['scop_id'])\n",
    "\n",
    "            try:\n",
    "                # Validate class mapping\n",
    "                class_label = row['class']\n",
    "                if class_label not in self.class_mapping:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = f\"Invalid class: {class_label}\"\n",
    "                    continue\n",
    "\n",
    "                class_label = self.class_mapping[class_label]\n",
    "\n",
    "                # Load structure\n",
    "                pdb_file = f\"{pdb_id}.pdb\"\n",
    "                pdb_path = os.path.join(raw_dir, pdb_file)\n",
    "\n",
    "                # Check if file exists\n",
    "                if not os.path.exists(pdb_path):\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"PDB file not found\"\n",
    "                    continue\n",
    "\n",
    "                structure = parser.get_structure('protein', pdb_path)\n",
    "                model = structure[0]\n",
    "\n",
    "                # Get residues and create features\n",
    "                residues = list(model.get_residues())\n",
    "\n",
    "                # Skip if too few or too many residues\n",
    "                if len(residues) < 10 or len(residues) > 1400:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = f\"Invalid residue count: {len(residues)}\"\n",
    "                    continue\n",
    "\n",
    "                # Create node features\n",
    "                node_features = []\n",
    "                for residue in residues:\n",
    "                    features = self._get_residue_features(residue)\n",
    "                    node_features.append(features)\n",
    "\n",
    "                # Create edges with 5Å cutoff\n",
    "                edges = []\n",
    "                for i in range(len(residues)):\n",
    "                    for j in range(i+1, len(residues)):\n",
    "                        if 'CA' in residues[i] and 'CA' in residues[j]:\n",
    "                            ca_i = residues[i]['CA'].get_coord()\n",
    "                            ca_j = residues[j]['CA'].get_coord()\n",
    "                            dist = np.linalg.norm(ca_i - ca_j)\n",
    "                            if dist < 5.0:  # 5Å cutoff\n",
    "                                edges.append([i, j])\n",
    "                                edges.append([j, i])  # Add both directions\n",
    "\n",
    "                if len(edges) == 0:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"No edges found\"\n",
    "                    continue\n",
    "\n",
    "                # Create PyG Data object\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "                x = torch.tensor(node_features, dtype=torch.float)\n",
    "                y = torch.tensor([class_label], dtype=torch.long)\n",
    "\n",
    "                data = Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    y=y,\n",
    "                    num_nodes=len(residues)\n",
    "                )\n",
    "\n",
    "                # Additional filtering if needed\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"Failed pre-filter\"\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                data_list.append(data)\n",
    "                processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                skipped_count += 1\n",
    "                skipped_reasons[pdb_id] = f\"Processing error: {str(e)}\"\n",
    "                continue\n",
    "\n",
    "        # Detailed logging of skipped reasons\n",
    "        print(f\"\\nProcessing Summary:\")\n",
    "        print(f\"Total processed: {processed_count}\")\n",
    "        print(f\"Total skipped: {skipped_count}\")\n",
    "        print(\"\\nSkipped Reasons:\")\n",
    "        for reason, count in Counter(skipped_reasons.values()).most_common():\n",
    "            print(f\"{reason}: {count}\")\n",
    "\n",
    "        # Optional: Print some skipped PDB IDs for investigation\n",
    "        print(\"\\nSample of skipped PDB IDs:\")\n",
    "        for reason, pdb_ids in groupby(sorted(skipped_reasons.items(), key=lambda x: x[1]), key=lambda x: x[1]):\n",
    "            print(f\"{reason}: {list(pdb_ids)[:5]}\")\n",
    "\n",
    "        if len(data_list) == 0:\n",
    "            raise RuntimeError(\"No data was successfully processed!\")\n",
    "\n",
    "        # Save processed data\n",
    "        torch.save(data_list, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Get a specific graph from the dataset.\"\"\"\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "            if not os.path.exists(processed_path):\n",
    "                print(\"Warning: Processed data file not found, running processing...\")\n",
    "                self.process()\n",
    "            # Add weights_only=False to allow loading PyG Data objects\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "        return self._data_list[idx]\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"Return the number of graphs in the dataset.\"\"\"\n",
    "        processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "        if not os.path.exists(processed_path):\n",
    "            print(\"Warning: Processed data file not found, running processing...\")\n",
    "            self.process()\n",
    "\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            # Add weights_only=False here as well\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "        print(f\"Actual number of processed samples: {len(self._data_list)}\")\n",
    "        return len(self._data_list)\n",
    "\n",
    "    def _get_residue_features(self, residue):\n",
    "        \"\"\"Create feature vector for a residue.\"\"\"\n",
    "        # One-hot encode amino acid type\n",
    "        aa_features = np.zeros(21)  # 20 standard amino acids + UNK\n",
    "        aa_name = residue.get_resname()\n",
    "        if aa_name in self.amino_acids:\n",
    "            aa_features[self.amino_acids[aa_name]] = 1\n",
    "        else:\n",
    "            aa_features[self.amino_acids['UNK']] = 1\n",
    "\n",
    "        # Get CA atom coordinates\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            coords = ca_atom.get_coord()\n",
    "            randtest = ca_atom.get_coord()\n",
    "            randtest2 = ca_atom.get_coord()\n",
    "        except:\n",
    "            coords = np.zeros(3)\n",
    "\n",
    "        # Combine features\n",
    "        features = np.concatenate([\n",
    "            aa_features,  # Amino acid identity (21)\n",
    "            coords,\n",
    "            randtest,\n",
    "            randtest2\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Return the number of classes in the dataset.\"\"\"\n",
    "        return len(self.class_mapping)\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        \"\"\"Return the number of node features.\"\"\"\n",
    "        return 30  # 21 for amino acids + 3 for coordinates"
   ],
   "id": "8eee4b4a0d18bcc8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Specify the root directory\n",
    "data_dir = 'data/SCOP'  # Adjust this to your actual data directory path\n",
    "\n",
    "# Create the dataset (this should trigger the process method)\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "# Create the dataset instance\n",
    "\n",
    "# Explicitly call the process method\n",
    "processed_data = dataset.process()"
   ],
   "id": "b40a91a41180d6ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:50:13.544759Z",
     "start_time": "2025-02-16T22:50:13.475567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "NUM_CLASSES = 7  # SCOP main classes\n",
    "# First, let's check protein sizes\n",
    "# First, analyze protein sizes\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "sizes = [data.num_nodes for data in data_list]\n",
    "\n",
    "print(f\"Protein size statistics:\")\n",
    "print(f\"Min size: {min(sizes)}\")\n",
    "print(f\"Max size: {max(sizes)}\")\n",
    "print(f\"Mean size: {sum(sizes)/len(sizes):.1f}\")\n",
    "print(f\"Median size: {sorted(sizes)[len(sizes)//2]}\")\n",
    "print(f\"Number of proteins > 150 residues: {sum(1 for s in sizes if s > 150)}\")\n",
    "\n",
    "# Set max_nodes to 500 since we have proteins up to 1381 residues\n",
    "max_nodes = 1400\n",
    "\n",
    "# Create dataset\n",
    "#dataset = SparseSCOPDataset(\n",
    "#    root=data_dir,\n",
    "#    pre_filter=lambda data: data.num_nodes <= max_nodes\n",
    "#)\n",
    "\n",
    "#dataset = SparseSCOPDataset(root=data_dir)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n"
   ],
   "id": "81c665801b8fe044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein size statistics:\n",
      "Min size: 25\n",
      "Max size: 1381\n",
      "Mean size: 205.4\n",
      "Median size: 155\n",
      "Number of proteins > 150 residues: 176\n",
      "Dataset size: 345\n",
      "\n",
      "Dataset size: 345\n",
      "Number of features: 24\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:34:32.260464Z",
     "start_time": "2025-02-16T19:34:32.194108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming your dataset is loaded\n",
    "filtered_dataset = [data for data in dataset if data.num_nodes < 300]\n",
    "\n",
    "# If you want to create a new dataset object\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class FilteredSCOPDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.data_list = [data for data in original_dataset if data.num_nodes < 300]\n",
    "        super().__init__(original_dataset.root)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create the filtered dataset\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Verify the filtering\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Optional: Check distribution across classes\n",
    "class_distribution = {}\n",
    "for data in filtered_dataset:\n",
    "    class_label = data.y.item()\n",
    "    class_distribution[class_label] = class_distribution.get(class_label, 0) + 1\n",
    "\n",
    "print(\"\\nClass distribution in filtered dataset:\")\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"Class {cls}: {count} proteins\")"
   ],
   "id": "755cc2d19aa5e717",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 3420\n",
      "Filtered dataset size: 2717\n",
      "\n",
      "Class distribution in filtered dataset:\n",
      "Class 1: 449 proteins\n",
      "Class 2: 368 proteins\n",
      "Class 0: 428 proteins\n",
      "Class 6: 500 proteins\n",
      "Class 5: 418 proteins\n",
      "Class 3: 382 proteins\n",
      "Class 4: 172 proteins\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:47:54.763162Z",
     "start_time": "2025-02-16T22:46:59.257804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the root directory\n",
    "data_dir = 'data/SCOP'  # Adjust this to your actual data directory path\n",
    "\n",
    "# Create the dataset (this should trigger the process method)\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "# Create the dataset instance\n",
    "\n",
    "# Explicitly call the process method\n",
    "processed_data = dataset.process()"
   ],
   "id": "326dab61158a6c1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class info file with 350 entries\n",
      "Total entries in class_info: 350\n",
      "Total PDB files in raw directory: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "Total processed: 345\n",
      "Total skipped: 5\n",
      "\n",
      "Skipped Reasons:\n",
      "Processing error: cannot access local variable 'randtest' where it is not associated with a value: 4\n",
      "Processing error: 0: 1\n",
      "\n",
      "Sample of skipped PDB IDs:\n",
      "Processing error: 0: [('410195', 'Processing error: 0')]\n",
      "Processing error: cannot access local variable 'randtest' where it is not associated with a value: [('86659', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('276088', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('309746', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('152769', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\")]\n",
      "Total entries in class_info: 350\n",
      "Total PDB files in raw directory: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "Total processed: 345\n",
      "Total skipped: 5\n",
      "\n",
      "Skipped Reasons:\n",
      "Processing error: cannot access local variable 'randtest' where it is not associated with a value: 4\n",
      "Processing error: 0: 1\n",
      "\n",
      "Sample of skipped PDB IDs:\n",
      "Processing error: 0: [('410195', 'Processing error: 0')]\n",
      "Processing error: cannot access local variable 'randtest' where it is not associated with a value: [('86659', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('276088', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('309746', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('152769', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\")]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:35:39.969356Z",
     "start_time": "2025-02-16T19:35:39.927705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the filtered dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Shuffle and split the filtered dataset\n",
    "filtered_dataset = filtered_dataset.shuffle()\n",
    "n = (len(filtered_dataset) + 9) // 10\n",
    "test_dataset = filtered_dataset[:n]\n",
    "val_dataset = filtered_dataset[n:2 * n]\n",
    "train_dataset = filtered_dataset[2 * n:]\n",
    "\n",
    "# Create data loaders using the filtered datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"\\nFiltered Dataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# The rest of your training loop remains the same\n",
    "# You can use these loaders directly in your existing training script"
   ],
   "id": "510af8d5deb60124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Dataset splits:\n",
      "Training samples: 2173\n",
      "Validation samples: 272\n",
      "Test samples: 272\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:31:11.960641Z",
     "start_time": "2025-02-16T19:31:11.819218Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "243e6c1a459fa5a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 2736\n",
      "Validation samples: 342\n",
      "Test samples: 342\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:50:25.342611Z",
     "start_time": "2025-02-16T22:50:25.205062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you've already loaded the dataset\n",
    "# Load the processed data\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "# Reverse class mapping for readable labels\n",
    "class_mapping_reverse = {\n",
    "    0: 'a (All-alpha)',\n",
    "    1: 'b (All-beta)',\n",
    "    2: 'c (Alpha/beta)',\n",
    "    3: 'd (Alpha+beta)',\n",
    "    4: 'e (Multi-domain)',\n",
    "    5: 'f (Membrane)',\n",
    "    6: 'g (Small proteins)'\n",
    "}\n",
    "\n",
    "# Separate nodes by class\n",
    "nodes_by_class = {}\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in nodes_by_class:\n",
    "        nodes_by_class[class_label] = []\n",
    "    nodes_by_class[class_label].append(data.num_nodes)\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Box plot\n",
    "plt.boxplot([nodes_by_class[key] for key in sorted(nodes_by_class.keys())],\n",
    "            labels=[class_mapping_reverse[key] for key in sorted(nodes_by_class.keys())])\n",
    "\n",
    "plt.title('Number of Nodes per SCOP Class', fontsize=16)\n",
    "plt.xlabel('SCOP Class', fontsize=12)\n",
    "plt.ylabel('Number of Nodes (Residues)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('nodes_per_class_boxplot.png')\n",
    "plt.close()\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Node count statistics per class:\")\n",
    "for class_label, nodes in nodes_by_class.items():\n",
    "    print(f\"\\n{class_mapping_reverse[class_label]}:\")\n",
    "    print(f\"  Count: {len(nodes)}\")\n",
    "    print(f\"  Min nodes: {min(nodes)}\")\n",
    "    print(f\"  Max nodes: {max(nodes)}\")\n",
    "    print(f\"  Mean nodes: {np.mean(nodes):.2f}\")\n",
    "    print(f\"  Median nodes: {np.median(nodes):.2f}\")"
   ],
   "id": "27dc0cc370f2c3dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node count statistics per class:\n",
      "\n",
      "a (All-alpha):\n",
      "  Count: 50\n",
      "  Min nodes: 43\n",
      "  Max nodes: 522\n",
      "  Mean nodes: 159.62\n",
      "  Median nodes: 133.00\n",
      "\n",
      "b (All-beta):\n",
      "  Count: 50\n",
      "  Min nodes: 55\n",
      "  Max nodes: 520\n",
      "  Mean nodes: 159.54\n",
      "  Median nodes: 118.00\n",
      "\n",
      "c (Alpha/beta):\n",
      "  Count: 50\n",
      "  Min nodes: 77\n",
      "  Max nodes: 551\n",
      "  Mean nodes: 275.04\n",
      "  Median nodes: 252.00\n",
      "\n",
      "d (Alpha+beta):\n",
      "  Count: 48\n",
      "  Min nodes: 58\n",
      "  Max nodes: 416\n",
      "  Mean nodes: 162.15\n",
      "  Median nodes: 129.00\n",
      "\n",
      "e (Multi-domain):\n",
      "  Count: 50\n",
      "  Min nodes: 186\n",
      "  Max nodes: 1381\n",
      "  Mean nodes: 399.54\n",
      "  Median nodes: 358.00\n",
      "\n",
      "f (Membrane):\n",
      "  Count: 48\n",
      "  Min nodes: 25\n",
      "  Max nodes: 691\n",
      "  Mean nodes: 214.35\n",
      "  Median nodes: 148.00\n",
      "\n",
      "g (Small proteins):\n",
      "  Count: 49\n",
      "  Min nodes: 29\n",
      "  Max nodes: 131\n",
      "  Mean nodes: 63.24\n",
      "  Median nodes: 53.00\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:48:42.206202Z",
     "start_time": "2025-02-16T22:48:41.954187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "# Count proteins per class and above 300 nodes\n",
    "class_counts = {}\n",
    "above_300_counts = {}\n",
    "\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in class_counts:\n",
    "        class_counts[class_label] = 0\n",
    "        above_300_counts[class_label] = 0\n",
    "\n",
    "    class_counts[class_label] += 1\n",
    "    if data.num_nodes > 300:\n",
    "        above_300_counts[class_label] += 1\n",
    "\n",
    "print(\"Total proteins per class:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"Class {cls}: {count} total, {above_300_counts[cls]} above 300 nodes ({above_300_counts[cls]/count*100:.2f}%)\")"
   ],
   "id": "e76168280612488f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins per class:\n",
      "Class 0: 50 total, 4 above 300 nodes (8.00%)\n",
      "Class 1: 50 total, 5 above 300 nodes (10.00%)\n",
      "Class 2: 50 total, 18 above 300 nodes (36.00%)\n",
      "Class 3: 48 total, 2 above 300 nodes (4.17%)\n",
      "Class 4: 50 total, 34 above 300 nodes (68.00%)\n",
      "Class 5: 48 total, 12 above 300 nodes (25.00%)\n",
      "Class 6: 49 total, 0 above 300 nodes (0.00%)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:53:20.350917Z",
     "start_time": "2025-02-16T22:53:20.325944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from math import ceil\n",
    "\n",
    "class SparseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # GNN layers with sparse representation\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First convolution layer\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = F.relu(h1)\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Second convolution layer\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = F.relu(h2)\n",
    "        h2 = self.dropout(h2)\n",
    "\n",
    "        # Third convolution layer\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "        h3 = self.bn3(h3)\n",
    "        h3 = F.relu(h3)\n",
    "        h3 = self.dropout(h3)\n",
    "\n",
    "        # Global mean pooling\n",
    "        out = global_mean_pool(h3, batch)\n",
    "\n",
    "        # MLP head\n",
    "        out = self.lin1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a single GNN model for classification\n",
    "        self.gnn = SparseGNN(\n",
    "            num_features=30,\n",
    "            hidden_dim=64,\n",
    "            num_classes=dataset.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Single forward pass through the GNN\n",
    "        x = self.gnn(x, edge_index, batch)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"Determine the best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    \"\"\"Training function for the model.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    \"\"\"Evaluation function for the model.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def main(dataset, train_loader, val_loader, test_loader):\n",
    "    \"\"\"Main training and evaluation loop.\"\"\"\n",
    "    device = setup_device()\n",
    "    model = Net(dataset).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for epoch in range(1, 151):\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        val_acc = test(model, val_loader, device)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = test(model, test_loader, device)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return model, best_val_acc, test_acc\n",
    "\n",
    "# Note: This function would be called after setting up the dataset, loaders, etc.\n",
    "# main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "738471d4eece3147",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:54:11.471599Z",
     "start_time": "2025-02-16T22:53:23.141164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data loaders (as you had before)\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Use DataLoader for sparse graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Train the model\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "e4d1fde02103aab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.9375, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 002, Train Loss: 1.8775, Val Acc: 0.1143, Test Acc: 0.2857\n",
      "Epoch: 003, Train Loss: 1.8617, Val Acc: 0.1143, Test Acc: 0.2857\n",
      "Epoch: 004, Train Loss: 1.8325, Val Acc: 0.1429, Test Acc: 0.2857\n",
      "Epoch: 005, Train Loss: 1.8148, Val Acc: 0.1429, Test Acc: 0.2857\n",
      "Epoch: 006, Train Loss: 1.7917, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 007, Train Loss: 1.7633, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 008, Train Loss: 1.7381, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 009, Train Loss: 1.7183, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 010, Train Loss: 1.6819, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 011, Train Loss: 1.6677, Val Acc: 0.2286, Test Acc: 0.1714\n",
      "Epoch: 012, Train Loss: 1.6343, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 013, Train Loss: 1.6001, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 014, Train Loss: 1.6009, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 015, Train Loss: 1.5384, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 016, Train Loss: 1.4962, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 017, Train Loss: 1.4695, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 018, Train Loss: 1.4716, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 019, Train Loss: 1.4231, Val Acc: 0.1143, Test Acc: 0.1714\n",
      "Epoch: 020, Train Loss: 1.4162, Val Acc: 0.1143, Test Acc: 0.1714\n",
      "Epoch: 021, Train Loss: 1.3827, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 022, Train Loss: 1.3600, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 023, Train Loss: 1.3062, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 024, Train Loss: 1.3085, Val Acc: 0.1143, Test Acc: 0.1714\n",
      "Epoch: 025, Train Loss: 1.2988, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 026, Train Loss: 1.2620, Val Acc: 0.1143, Test Acc: 0.1714\n",
      "Epoch: 027, Train Loss: 1.2401, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 028, Train Loss: 1.2024, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 029, Train Loss: 1.1772, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 030, Train Loss: 1.1690, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 031, Train Loss: 1.1348, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 032, Train Loss: 1.1407, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 033, Train Loss: 1.1550, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 034, Train Loss: 1.1415, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 035, Train Loss: 1.0784, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 036, Train Loss: 1.0096, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 037, Train Loss: 1.0115, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 038, Train Loss: 0.9874, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 039, Train Loss: 0.9416, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 040, Train Loss: 0.9455, Val Acc: 0.2286, Test Acc: 0.1714\n",
      "Epoch: 041, Train Loss: 0.9117, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 042, Train Loss: 0.9201, Val Acc: 0.0857, Test Acc: 0.1714\n",
      "Epoch: 043, Train Loss: 0.9544, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 044, Train Loss: 0.9302, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 045, Train Loss: 0.8988, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 046, Train Loss: 0.8509, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 047, Train Loss: 0.8282, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 048, Train Loss: 0.8299, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 049, Train Loss: 0.7431, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 050, Train Loss: 0.7765, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 051, Train Loss: 0.8092, Val Acc: 0.2286, Test Acc: 0.1714\n",
      "Epoch: 052, Train Loss: 0.7862, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 053, Train Loss: 0.7794, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 054, Train Loss: 0.7305, Val Acc: 0.2286, Test Acc: 0.1714\n",
      "Epoch: 055, Train Loss: 0.6998, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 056, Train Loss: 0.6977, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 057, Train Loss: 0.6751, Val Acc: 0.2857, Test Acc: 0.2000\n",
      "Epoch: 058, Train Loss: 0.7125, Val Acc: 0.1714, Test Acc: 0.2000\n",
      "Epoch: 059, Train Loss: 0.7041, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 060, Train Loss: 0.6797, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 061, Train Loss: 0.6399, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 062, Train Loss: 0.6575, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 063, Train Loss: 0.5973, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 064, Train Loss: 0.5941, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 065, Train Loss: 0.5696, Val Acc: 0.2857, Test Acc: 0.2000\n",
      "Epoch: 066, Train Loss: 0.5835, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 067, Train Loss: 0.5391, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 068, Train Loss: 0.6008, Val Acc: 0.1429, Test Acc: 0.2000\n",
      "Epoch: 069, Train Loss: 0.5829, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 070, Train Loss: 0.5375, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 071, Train Loss: 0.5632, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 072, Train Loss: 0.5779, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 073, Train Loss: 0.5881, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 074, Train Loss: 0.5550, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 075, Train Loss: 0.5740, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 076, Train Loss: 0.6838, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 077, Train Loss: 0.5191, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 078, Train Loss: 0.4836, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 079, Train Loss: 0.4418, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 080, Train Loss: 0.4254, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 081, Train Loss: 0.3855, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 082, Train Loss: 0.3799, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 083, Train Loss: 0.3897, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 084, Train Loss: 0.3477, Val Acc: 0.1714, Test Acc: 0.2000\n",
      "Epoch: 085, Train Loss: 0.3901, Val Acc: 0.3143, Test Acc: 0.1143\n",
      "Epoch: 086, Train Loss: 0.3904, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 087, Train Loss: 0.4044, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 088, Train Loss: 0.3705, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 089, Train Loss: 0.3734, Val Acc: 0.2857, Test Acc: 0.1143\n",
      "Epoch: 090, Train Loss: 0.4343, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 091, Train Loss: 0.4867, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 092, Train Loss: 0.4475, Val Acc: 0.3143, Test Acc: 0.1143\n",
      "Epoch: 093, Train Loss: 0.4218, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 094, Train Loss: 0.3893, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 095, Train Loss: 0.3811, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 096, Train Loss: 0.3832, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 097, Train Loss: 0.3125, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 098, Train Loss: 0.3632, Val Acc: 0.1429, Test Acc: 0.1143\n",
      "Epoch: 099, Train Loss: 0.4473, Val Acc: 0.3143, Test Acc: 0.1143\n",
      "Epoch: 100, Train Loss: 0.3927, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 101, Train Loss: 0.3672, Val Acc: 0.1429, Test Acc: 0.1143\n",
      "Epoch: 102, Train Loss: 0.2746, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 103, Train Loss: 0.3444, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 104, Train Loss: 0.3002, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 105, Train Loss: 0.2761, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 106, Train Loss: 0.2938, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 107, Train Loss: 0.3067, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 108, Train Loss: 0.3132, Val Acc: 0.1143, Test Acc: 0.1143\n",
      "Epoch: 109, Train Loss: 0.3515, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 110, Train Loss: 0.2988, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 111, Train Loss: 0.3137, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 112, Train Loss: 0.2974, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 113, Train Loss: 0.2643, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 114, Train Loss: 0.2354, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 115, Train Loss: 0.2369, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 116, Train Loss: 0.2382, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 117, Train Loss: 0.2527, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 118, Train Loss: 0.2557, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 119, Train Loss: 0.2670, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 120, Train Loss: 0.2646, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 121, Train Loss: 0.2532, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 122, Train Loss: 0.2767, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 123, Train Loss: 0.2864, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 124, Train Loss: 0.2352, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 125, Train Loss: 0.2151, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 126, Train Loss: 0.2408, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 127, Train Loss: 0.2405, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 128, Train Loss: 0.2357, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 129, Train Loss: 0.1975, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 130, Train Loss: 0.1896, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 131, Train Loss: 0.1919, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 132, Train Loss: 0.2190, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 133, Train Loss: 0.2935, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 134, Train Loss: 0.1734, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 135, Train Loss: 0.1619, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 136, Train Loss: 0.1743, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 137, Train Loss: 0.1820, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 138, Train Loss: 0.1646, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 139, Train Loss: 0.1563, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 140, Train Loss: 0.1570, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 141, Train Loss: 0.2084, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 142, Train Loss: 0.1483, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 143, Train Loss: 0.1295, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 144, Train Loss: 0.1283, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 145, Train Loss: 0.1470, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 146, Train Loss: 0.1818, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 147, Train Loss: 0.2037, Val Acc: 0.1429, Test Acc: 0.1143\n",
      "Epoch: 148, Train Loss: 0.2743, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 149, Train Loss: 0.2431, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 150, Train Loss: 0.2162, Val Acc: 0.2286, Test Acc: 0.1143\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:50:48.551762Z",
     "start_time": "2025-02-16T22:50:48.342214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Replace your existing DataLoader with this\n",
    "train_loader = DataLoader(train_dataset,  batch_size=20)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=20)\n",
    "test_loader = DataLoader(test_dataset,  batch_size=20)\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)\n"
   ],
   "id": "db76b9390eadc048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 275\n",
      "Validation samples: 35\n",
      "Test samples: 35\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (4338x30 and 24x64)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation samples: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(val_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest samples: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m model, best_val_acc, test_acc \u001B[38;5;241m=\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[16], line 129\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(dataset, train_loader, val_loader, test_loader)\u001B[0m\n\u001B[1;32m    126\u001B[0m test_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m151\u001B[39m):\n\u001B[0;32m--> 129\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    130\u001B[0m     val_acc \u001B[38;5;241m=\u001B[39m test(model, val_loader, device)\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m val_acc \u001B[38;5;241m>\u001B[39m best_val_acc:\n",
      "Cell \u001B[0;32mIn[16], line 93\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, optimizer, device)\u001B[0m\n\u001B[1;32m     91\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     92\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 93\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnll_loss(output, data\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     95\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[16], line 71\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, x, edge_index, batch)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index, batch):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;66;03m# Single forward pass through the GNN\u001B[39;00m\n\u001B[0;32m---> 71\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlog_softmax(x, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[16], line 29\u001B[0m, in \u001B[0;36mSparseGNN.forward\u001B[0;34m(self, x, edge_index, batch)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index, batch):\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m# First convolution layer\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m     h1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m     h1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(h1)\n\u001B[1;32m     31\u001B[0m     h1 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(h1)\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch_geometric/nn/conv/sage_conv.py:135\u001B[0m, in \u001B[0;36mSAGEConv.forward\u001B[0;34m(self, x, edge_index, size)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001B[39;00m\n\u001B[1;32m    134\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpropagate(edge_index, x\u001B[38;5;241m=\u001B[39mx, size\u001B[38;5;241m=\u001B[39msize)\n\u001B[0;32m--> 135\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlin_l\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m x_r \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_weight \u001B[38;5;129;01mand\u001B[39;00m x_r \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch_geometric/nn/dense/linear.py:147\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    142\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Forward pass.\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \n\u001B[1;32m    144\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;124;03m        x (torch.Tensor): The input features.\u001B[39;00m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: linear(): input and weight.T shapes cannot be multiplied (4338x30 and 24x64)"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
