{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T18:58:36.737582Z",
     "start_time": "2025-02-16T18:58:36.639519Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv  # Changed from DenseSAGEConv\n",
    "from torch_geometric.nn import global_mean_pool  # For sparse pooling\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define data directory\n",
    "data_dir = 'data/SCOP'  # Base directory for SCOP data\n",
    "\n",
    "# Rest of the model code remains the same..."
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T18:58:36.779503Z",
     "start_time": "2025-02-16T18:58:36.749014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SparseSCOPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        # Set up the root path first\n",
    "        self.root = root\n",
    "\n",
    "        # Now we can set up the class_info_path\n",
    "        self.class_info_path = os.path.join(root, 'raw/class_info.csv')\n",
    "\n",
    "        # Dictionary to map SCOP classes to indices\n",
    "        self.class_mapping = {\n",
    "            'a': 0,  # All-alpha\n",
    "            'b': 1,  # All-beta\n",
    "            'c': 2,  # Alpha/beta\n",
    "            'd': 3,  # Alpha+beta\n",
    "            'e': 4,  # Multi-domain\n",
    "            'f': 5,  # Membrane\n",
    "            'g': 6   # Small proteins\n",
    "        }\n",
    "\n",
    "        # Dictionary to map amino acids to indices\n",
    "        self.amino_acids = {\n",
    "            'ALA': 0, 'ARG': 1, 'ASN': 2, 'ASP': 3, 'CYS': 4,\n",
    "            'GLN': 5, 'GLU': 6, 'GLY': 7, 'HIS': 8, 'ILE': 9,\n",
    "            'LEU': 10, 'LYS': 11, 'MET': 12, 'PHE': 13, 'PRO': 14,\n",
    "            'SER': 15, 'THR': 16, 'TRP': 17, 'TYR': 18, 'VAL': 19,\n",
    "            'UNK': 20  # Unknown amino acid\n",
    "        }\n",
    "\n",
    "        # Load class information\n",
    "        if os.path.exists(self.class_info_path):\n",
    "            self.class_info = pd.read_csv(self.class_info_path)\n",
    "            print(f\"Found class info file with {len(self.class_info)} entries\")\n",
    "        else:\n",
    "            print(f\"Warning: class_info.csv not found at {self.class_info_path}\")\n",
    "            self.class_info = None\n",
    "\n",
    "        # Initialize the base class last\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def raw_file_names(self):\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"List of raw file names in the dataset.\"\"\"\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"List of processed file names in the dataset.\"\"\"\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        pass  # We already have the files\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Process the raw data into the internal format.\"\"\"\n",
    "        data_list = []\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "\n",
    "        for idx, row in self.class_info.iterrows():\n",
    "            pdb_id = str(row['scop_id'])\n",
    "            class_label = self.class_mapping[row['class']]\n",
    "\n",
    "            try:\n",
    "                # Load structure\n",
    "                pdb_file = f\"{pdb_id}.pdb\"\n",
    "                pdb_path = os.path.join(self.root, 'raw', pdb_file)\n",
    "                structure = parser.get_structure('protein', pdb_path)\n",
    "                model = structure[0]\n",
    "\n",
    "                # Get residues and create features\n",
    "                residues = list(model.get_residues())\n",
    "\n",
    "                # Create node features\n",
    "                node_features = []\n",
    "                for residue in residues:\n",
    "                    features = self._get_residue_features(residue)\n",
    "                    node_features.append(features)\n",
    "\n",
    "                # Create edges with 5Å cutoff\n",
    "                edges = []\n",
    "                for i in range(len(residues)):\n",
    "                    for j in range(i+1, len(residues)):\n",
    "                        if 'CA' in residues[i] and 'CA' in residues[j]:\n",
    "                            ca_i = residues[i]['CA'].get_coord()\n",
    "                            ca_j = residues[j]['CA'].get_coord()\n",
    "                            dist = np.linalg.norm(ca_i - ca_j)\n",
    "                            if dist < 5.0:  # 5Å cutoff\n",
    "                                edges.append([i, j])\n",
    "                                edges.append([j, i])  # Add both directions\n",
    "\n",
    "                if len(edges) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Create PyG Data object\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "                x = torch.tensor(node_features, dtype=torch.float)\n",
    "                y = torch.tensor([class_label], dtype=torch.long)\n",
    "\n",
    "                data = Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    y=y,\n",
    "                    num_nodes=len(residues)\n",
    "                )\n",
    "\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                data_list.append(data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdb_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if len(data_list) == 0:\n",
    "            raise RuntimeError(\"No data was successfully processed!\")\n",
    "\n",
    "        torch.save(data_list, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Get a specific graph from the dataset.\"\"\"\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "            if not os.path.exists(processed_path):\n",
    "                print(\"Warning: Processed data file not found, running processing...\")\n",
    "                self.process()\n",
    "            # Add weights_only=False to allow loading PyG Data objects\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "        return self._data_list[idx]\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"Return the number of graphs in the dataset.\"\"\"\n",
    "        processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "        if not os.path.exists(processed_path):\n",
    "            print(\"Warning: Processed data file not found, running processing...\")\n",
    "            self.process()\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            # Add weights_only=False here as well\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "        return len(self._data_list)\n",
    "\n",
    "    def _get_residue_features(self, residue):\n",
    "        \"\"\"Create feature vector for a residue.\"\"\"\n",
    "        # One-hot encode amino acid type\n",
    "        aa_features = np.zeros(21)  # 20 standard amino acids + UNK\n",
    "        aa_name = residue.get_resname()\n",
    "        if aa_name in self.amino_acids:\n",
    "            aa_features[self.amino_acids[aa_name]] = 1\n",
    "        else:\n",
    "            aa_features[self.amino_acids['UNK']] = 1\n",
    "\n",
    "        # Get CA atom coordinates\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            coords = ca_atom.get_coord()\n",
    "        except:\n",
    "            coords = np.zeros(3)\n",
    "\n",
    "        # Combine features\n",
    "        features = np.concatenate([\n",
    "            aa_features,  # Amino acid identity (21)\n",
    "            coords,      # 3D coordinates (3)\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Return the number of classes in the dataset.\"\"\"\n",
    "        return len(self.class_mapping)\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        \"\"\"Return the number of node features.\"\"\"\n",
    "        return 24  # 21 for amino acids + 3 for coordinates"
   ],
   "id": "8eee4b4a0d18bcc8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T18:58:38.254233Z",
     "start_time": "2025-02-16T18:58:36.807206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "\n",
    "NUM_CLASSES = 7  # SCOP main classes\n",
    "# First, let's check protein sizes\n",
    "# First, analyze protein sizes\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "sizes = [data.num_nodes for data in data_list]\n",
    "\n",
    "print(f\"Protein size statistics:\")\n",
    "print(f\"Min size: {min(sizes)}\")\n",
    "print(f\"Max size: {max(sizes)}\")\n",
    "print(f\"Mean size: {sum(sizes)/len(sizes):.1f}\")\n",
    "print(f\"Median size: {sorted(sizes)[len(sizes)//2]}\")\n",
    "print(f\"Number of proteins > 150 residues: {sum(1 for s in sizes if s > 150)}\")\n",
    "\n",
    "# Set max_nodes to 500 since we have proteins up to 1381 residues\n",
    "max_nodes = 1400\n",
    "\n",
    "# Create dataset\n",
    "dataset = SparseSCOPDataset(\n",
    "    root=data_dir,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n"
   ],
   "id": "81c665801b8fe044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein size statistics:\n",
      "Min size: 20\n",
      "Max size: 1395\n",
      "Mean size: 202.8\n",
      "Median size: 165\n",
      "Number of proteins > 150 residues: 1859\n",
      "Found class info file with 3500 entries\n",
      "\n",
      "Dataset size: 3420\n",
      "Number of features: 24\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T18:58:38.290947Z",
     "start_time": "2025-02-16T18:58:38.269493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Replace your existing DataLoader with this\n",
    "train_loader = DataLoader(train_dataset,  batch_size=20)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=20)\n",
    "test_loader = DataLoader(test_dataset,  batch_size=20)\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ],
   "id": "243e6c1a459fa5a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 2736\n",
      "Validation samples: 342\n",
      "Test samples: 342\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T18:58:38.309280Z",
     "start_time": "2025-02-16T18:58:38.299749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "class GATNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=7, heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # GAT layers with multiple attention heads\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=heads, concat=True)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True)\n",
    "        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False)\n",
    "\n",
    "        # Batch normalization\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim * heads)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim * heads)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First GAT layer\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = F.elu(h1)\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Second GAT layer\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = F.elu(h2)\n",
    "        h2 = self.dropout(h2)\n",
    "\n",
    "        # Third GAT layer\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "        h3 = self.bn3(h3)\n",
    "        h3 = F.elu(h3)\n",
    "        h3 = self.dropout(h3)\n",
    "\n",
    "        # Global pooling\n",
    "        out = global_mean_pool(h3, batch)\n",
    "\n",
    "        # Classification head\n",
    "        out = self.lin1(out)\n",
    "        out = F.elu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def main(dataset, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = GATNetwork(\n",
    "        num_features=dataset.num_features,\n",
    "        hidden_dim=64,\n",
    "        num_classes=dataset.num_classes\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for epoch in range(1, 151):\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        val_acc = test(model, val_loader, device)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = test(model, test_loader, device)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return model, best_val_acc, test_acc\n",
    "\n",
    "# Usage remains the same as previous implementation\n",
    "# main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "738471d4eece3147",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:39:37.203658Z",
     "start_time": "2025-02-16T18:58:38.322314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data loaders (as you had before)\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Use DataLoader for sparse graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Train the model\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "e4d1fde02103aab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.9287, Val Acc: 0.2339, Test Acc: 0.2222\n",
      "Epoch: 002, Train Loss: 1.9037, Val Acc: 0.2515, Test Acc: 0.2515\n",
      "Epoch: 003, Train Loss: 1.8958, Val Acc: 0.2778, Test Acc: 0.2456\n",
      "Epoch: 004, Train Loss: 1.8930, Val Acc: 0.2661, Test Acc: 0.2456\n",
      "Epoch: 005, Train Loss: 1.8838, Val Acc: 0.2632, Test Acc: 0.2456\n",
      "Epoch: 006, Train Loss: 1.8802, Val Acc: 0.2953, Test Acc: 0.2485\n",
      "Epoch: 007, Train Loss: 1.7904, Val Acc: 0.3684, Test Acc: 0.3304\n",
      "Epoch: 008, Train Loss: 1.6159, Val Acc: 0.3713, Test Acc: 0.3860\n",
      "Epoch: 009, Train Loss: 1.5791, Val Acc: 0.4152, Test Acc: 0.3977\n",
      "Epoch: 010, Train Loss: 1.5431, Val Acc: 0.4269, Test Acc: 0.4298\n",
      "Epoch: 011, Train Loss: 1.5104, Val Acc: 0.3684, Test Acc: 0.4298\n",
      "Epoch: 012, Train Loss: 1.4942, Val Acc: 0.3860, Test Acc: 0.4298\n",
      "Epoch: 013, Train Loss: 1.4840, Val Acc: 0.4064, Test Acc: 0.4298\n",
      "Epoch: 014, Train Loss: 1.4586, Val Acc: 0.3977, Test Acc: 0.4298\n",
      "Epoch: 015, Train Loss: 1.4322, Val Acc: 0.4152, Test Acc: 0.4298\n",
      "Epoch: 016, Train Loss: 1.4380, Val Acc: 0.4152, Test Acc: 0.4298\n",
      "Epoch: 017, Train Loss: 1.4023, Val Acc: 0.4444, Test Acc: 0.4444\n",
      "Epoch: 018, Train Loss: 1.4021, Val Acc: 0.4298, Test Acc: 0.4444\n",
      "Epoch: 019, Train Loss: 1.3882, Val Acc: 0.4327, Test Acc: 0.4444\n",
      "Epoch: 020, Train Loss: 1.3825, Val Acc: 0.4298, Test Acc: 0.4444\n",
      "Epoch: 021, Train Loss: 1.3666, Val Acc: 0.4152, Test Acc: 0.4444\n",
      "Epoch: 022, Train Loss: 1.3695, Val Acc: 0.4327, Test Acc: 0.4444\n",
      "Epoch: 023, Train Loss: 1.3554, Val Acc: 0.4298, Test Acc: 0.4444\n",
      "Epoch: 024, Train Loss: 1.3443, Val Acc: 0.4152, Test Acc: 0.4444\n",
      "Epoch: 025, Train Loss: 1.3427, Val Acc: 0.4269, Test Acc: 0.4444\n",
      "Epoch: 026, Train Loss: 1.3433, Val Acc: 0.4327, Test Acc: 0.4444\n",
      "Epoch: 027, Train Loss: 1.3266, Val Acc: 0.4532, Test Acc: 0.4825\n",
      "Epoch: 028, Train Loss: 1.3111, Val Acc: 0.4561, Test Acc: 0.4912\n",
      "Epoch: 029, Train Loss: 1.3226, Val Acc: 0.4620, Test Acc: 0.4942\n",
      "Epoch: 030, Train Loss: 1.2963, Val Acc: 0.4649, Test Acc: 0.5088\n",
      "Epoch: 031, Train Loss: 1.2851, Val Acc: 0.4883, Test Acc: 0.5117\n",
      "Epoch: 032, Train Loss: 1.2832, Val Acc: 0.4854, Test Acc: 0.5117\n",
      "Epoch: 033, Train Loss: 1.2703, Val Acc: 0.4912, Test Acc: 0.4942\n",
      "Epoch: 034, Train Loss: 1.2511, Val Acc: 0.4854, Test Acc: 0.4942\n",
      "Epoch: 035, Train Loss: 1.2526, Val Acc: 0.4912, Test Acc: 0.4942\n",
      "Epoch: 036, Train Loss: 1.2458, Val Acc: 0.4883, Test Acc: 0.4942\n",
      "Epoch: 037, Train Loss: 1.2228, Val Acc: 0.4942, Test Acc: 0.5409\n",
      "Epoch: 038, Train Loss: 1.2092, Val Acc: 0.4971, Test Acc: 0.5175\n",
      "Epoch: 039, Train Loss: 1.2197, Val Acc: 0.5029, Test Acc: 0.5146\n",
      "Epoch: 040, Train Loss: 1.2025, Val Acc: 0.5117, Test Acc: 0.5175\n",
      "Epoch: 041, Train Loss: 1.1976, Val Acc: 0.4883, Test Acc: 0.5175\n",
      "Epoch: 042, Train Loss: 1.1738, Val Acc: 0.4825, Test Acc: 0.5175\n",
      "Epoch: 043, Train Loss: 1.1980, Val Acc: 0.5058, Test Acc: 0.5175\n",
      "Epoch: 044, Train Loss: 1.1657, Val Acc: 0.5117, Test Acc: 0.5175\n",
      "Epoch: 045, Train Loss: 1.1542, Val Acc: 0.5234, Test Acc: 0.5205\n",
      "Epoch: 046, Train Loss: 1.1618, Val Acc: 0.5205, Test Acc: 0.5205\n",
      "Epoch: 047, Train Loss: 1.1497, Val Acc: 0.5205, Test Acc: 0.5205\n",
      "Epoch: 048, Train Loss: 1.1469, Val Acc: 0.5117, Test Acc: 0.5205\n",
      "Epoch: 049, Train Loss: 1.1489, Val Acc: 0.5029, Test Acc: 0.5205\n",
      "Epoch: 050, Train Loss: 1.1368, Val Acc: 0.5088, Test Acc: 0.5205\n",
      "Epoch: 051, Train Loss: 1.1271, Val Acc: 0.5205, Test Acc: 0.5205\n",
      "Epoch: 052, Train Loss: 1.1437, Val Acc: 0.5322, Test Acc: 0.5322\n",
      "Epoch: 053, Train Loss: 1.1288, Val Acc: 0.5205, Test Acc: 0.5322\n",
      "Epoch: 054, Train Loss: 1.1159, Val Acc: 0.5439, Test Acc: 0.5380\n",
      "Epoch: 055, Train Loss: 1.1271, Val Acc: 0.5409, Test Acc: 0.5380\n",
      "Epoch: 056, Train Loss: 1.1095, Val Acc: 0.5146, Test Acc: 0.5380\n",
      "Epoch: 057, Train Loss: 1.1249, Val Acc: 0.5409, Test Acc: 0.5380\n",
      "Epoch: 058, Train Loss: 1.1155, Val Acc: 0.5351, Test Acc: 0.5380\n",
      "Epoch: 059, Train Loss: 1.1201, Val Acc: 0.5234, Test Acc: 0.5380\n",
      "Epoch: 060, Train Loss: 1.1040, Val Acc: 0.5409, Test Acc: 0.5380\n",
      "Epoch: 061, Train Loss: 1.1225, Val Acc: 0.5468, Test Acc: 0.5497\n",
      "Epoch: 062, Train Loss: 1.0826, Val Acc: 0.5058, Test Acc: 0.5497\n",
      "Epoch: 063, Train Loss: 1.0965, Val Acc: 0.5175, Test Acc: 0.5497\n",
      "Epoch: 064, Train Loss: 1.0964, Val Acc: 0.5380, Test Acc: 0.5497\n",
      "Epoch: 065, Train Loss: 1.0785, Val Acc: 0.5468, Test Acc: 0.5497\n",
      "Epoch: 066, Train Loss: 1.0746, Val Acc: 0.5351, Test Acc: 0.5497\n",
      "Epoch: 067, Train Loss: 1.0782, Val Acc: 0.5205, Test Acc: 0.5497\n",
      "Epoch: 068, Train Loss: 1.0770, Val Acc: 0.5380, Test Acc: 0.5497\n",
      "Epoch: 069, Train Loss: 1.0634, Val Acc: 0.5146, Test Acc: 0.5497\n",
      "Epoch: 070, Train Loss: 1.0943, Val Acc: 0.5117, Test Acc: 0.5497\n",
      "Epoch: 071, Train Loss: 1.0567, Val Acc: 0.5088, Test Acc: 0.5497\n",
      "Epoch: 072, Train Loss: 1.0668, Val Acc: 0.5088, Test Acc: 0.5497\n",
      "Epoch: 073, Train Loss: 1.0665, Val Acc: 0.5146, Test Acc: 0.5497\n",
      "Epoch: 074, Train Loss: 1.0549, Val Acc: 0.5205, Test Acc: 0.5497\n",
      "Epoch: 075, Train Loss: 1.0465, Val Acc: 0.5322, Test Acc: 0.5497\n",
      "Epoch: 076, Train Loss: 1.0634, Val Acc: 0.5234, Test Acc: 0.5497\n",
      "Epoch: 077, Train Loss: 1.0514, Val Acc: 0.5322, Test Acc: 0.5497\n",
      "Epoch: 078, Train Loss: 1.0707, Val Acc: 0.5175, Test Acc: 0.5497\n",
      "Epoch: 079, Train Loss: 1.0349, Val Acc: 0.5263, Test Acc: 0.5497\n",
      "Epoch: 080, Train Loss: 1.0508, Val Acc: 0.5058, Test Acc: 0.5497\n",
      "Epoch: 081, Train Loss: 1.0379, Val Acc: 0.5234, Test Acc: 0.5497\n",
      "Epoch: 082, Train Loss: 1.0582, Val Acc: 0.5380, Test Acc: 0.5497\n",
      "Epoch: 083, Train Loss: 1.0411, Val Acc: 0.5439, Test Acc: 0.5497\n",
      "Epoch: 084, Train Loss: 1.0437, Val Acc: 0.5351, Test Acc: 0.5497\n",
      "Epoch: 085, Train Loss: 1.0473, Val Acc: 0.5468, Test Acc: 0.5497\n",
      "Epoch: 086, Train Loss: 1.0424, Val Acc: 0.5322, Test Acc: 0.5497\n",
      "Epoch: 087, Train Loss: 1.0300, Val Acc: 0.5146, Test Acc: 0.5497\n",
      "Epoch: 088, Train Loss: 1.0067, Val Acc: 0.5468, Test Acc: 0.5497\n",
      "Epoch: 089, Train Loss: 1.0353, Val Acc: 0.5322, Test Acc: 0.5497\n",
      "Epoch: 090, Train Loss: 1.0101, Val Acc: 0.5409, Test Acc: 0.5497\n",
      "Epoch: 091, Train Loss: 1.0151, Val Acc: 0.5556, Test Acc: 0.5322\n",
      "Epoch: 092, Train Loss: 1.0251, Val Acc: 0.5146, Test Acc: 0.5322\n",
      "Epoch: 093, Train Loss: 1.0221, Val Acc: 0.5146, Test Acc: 0.5322\n",
      "Epoch: 094, Train Loss: 1.0464, Val Acc: 0.5614, Test Acc: 0.5117\n",
      "Epoch: 095, Train Loss: 1.0142, Val Acc: 0.5526, Test Acc: 0.5117\n",
      "Epoch: 096, Train Loss: 1.0012, Val Acc: 0.5175, Test Acc: 0.5117\n",
      "Epoch: 097, Train Loss: 1.0177, Val Acc: 0.5351, Test Acc: 0.5117\n",
      "Epoch: 098, Train Loss: 0.9965, Val Acc: 0.5585, Test Acc: 0.5117\n",
      "Epoch: 099, Train Loss: 1.0035, Val Acc: 0.5526, Test Acc: 0.5117\n",
      "Epoch: 100, Train Loss: 1.0122, Val Acc: 0.5292, Test Acc: 0.5117\n",
      "Epoch: 101, Train Loss: 1.0176, Val Acc: 0.5439, Test Acc: 0.5117\n",
      "Epoch: 102, Train Loss: 1.0030, Val Acc: 0.5322, Test Acc: 0.5117\n",
      "Epoch: 103, Train Loss: 0.9786, Val Acc: 0.5497, Test Acc: 0.5117\n",
      "Epoch: 104, Train Loss: 0.9989, Val Acc: 0.5351, Test Acc: 0.5117\n",
      "Epoch: 105, Train Loss: 0.9883, Val Acc: 0.5292, Test Acc: 0.5117\n",
      "Epoch: 106, Train Loss: 1.0013, Val Acc: 0.5439, Test Acc: 0.5117\n",
      "Epoch: 107, Train Loss: 0.9973, Val Acc: 0.5351, Test Acc: 0.5117\n",
      "Epoch: 108, Train Loss: 1.0001, Val Acc: 0.5380, Test Acc: 0.5117\n",
      "Epoch: 109, Train Loss: 0.9749, Val Acc: 0.5380, Test Acc: 0.5117\n",
      "Epoch: 110, Train Loss: 0.9911, Val Acc: 0.5556, Test Acc: 0.5117\n",
      "Epoch: 111, Train Loss: 1.0032, Val Acc: 0.5614, Test Acc: 0.5117\n",
      "Epoch: 112, Train Loss: 0.9822, Val Acc: 0.5585, Test Acc: 0.5117\n",
      "Epoch: 113, Train Loss: 0.9560, Val Acc: 0.5409, Test Acc: 0.5117\n",
      "Epoch: 114, Train Loss: 0.9745, Val Acc: 0.5468, Test Acc: 0.5117\n",
      "Epoch: 115, Train Loss: 0.9792, Val Acc: 0.5526, Test Acc: 0.5117\n",
      "Epoch: 116, Train Loss: 0.9714, Val Acc: 0.5380, Test Acc: 0.5117\n",
      "Epoch: 117, Train Loss: 0.9871, Val Acc: 0.5731, Test Acc: 0.5760\n",
      "Epoch: 118, Train Loss: 0.9758, Val Acc: 0.5497, Test Acc: 0.5760\n",
      "Epoch: 119, Train Loss: 0.9754, Val Acc: 0.5702, Test Acc: 0.5760\n",
      "Epoch: 120, Train Loss: 0.9599, Val Acc: 0.5468, Test Acc: 0.5760\n",
      "Epoch: 121, Train Loss: 0.9698, Val Acc: 0.5760, Test Acc: 0.5526\n",
      "Epoch: 122, Train Loss: 0.9619, Val Acc: 0.5497, Test Acc: 0.5526\n",
      "Epoch: 123, Train Loss: 0.9713, Val Acc: 0.5702, Test Acc: 0.5526\n",
      "Epoch: 124, Train Loss: 0.9736, Val Acc: 0.5731, Test Acc: 0.5526\n",
      "Epoch: 125, Train Loss: 0.9591, Val Acc: 0.5906, Test Acc: 0.5439\n",
      "Epoch: 126, Train Loss: 0.9718, Val Acc: 0.5556, Test Acc: 0.5439\n",
      "Epoch: 127, Train Loss: 0.9669, Val Acc: 0.5292, Test Acc: 0.5439\n",
      "Epoch: 128, Train Loss: 0.9513, Val Acc: 0.5819, Test Acc: 0.5439\n",
      "Epoch: 129, Train Loss: 0.9602, Val Acc: 0.5789, Test Acc: 0.5439\n",
      "Epoch: 130, Train Loss: 0.9940, Val Acc: 0.5731, Test Acc: 0.5439\n",
      "Epoch: 131, Train Loss: 0.9476, Val Acc: 0.5585, Test Acc: 0.5439\n",
      "Epoch: 132, Train Loss: 0.9433, Val Acc: 0.5643, Test Acc: 0.5439\n",
      "Epoch: 133, Train Loss: 0.9551, Val Acc: 0.5673, Test Acc: 0.5439\n",
      "Epoch: 134, Train Loss: 0.9505, Val Acc: 0.5731, Test Acc: 0.5439\n",
      "Epoch: 135, Train Loss: 0.9319, Val Acc: 0.5673, Test Acc: 0.5439\n",
      "Epoch: 136, Train Loss: 0.9459, Val Acc: 0.5673, Test Acc: 0.5439\n",
      "Epoch: 137, Train Loss: 0.9310, Val Acc: 0.5702, Test Acc: 0.5439\n",
      "Epoch: 138, Train Loss: 0.9689, Val Acc: 0.5906, Test Acc: 0.5439\n",
      "Epoch: 139, Train Loss: 0.9189, Val Acc: 0.5702, Test Acc: 0.5439\n",
      "Epoch: 140, Train Loss: 0.9377, Val Acc: 0.5760, Test Acc: 0.5439\n",
      "Epoch: 141, Train Loss: 0.9377, Val Acc: 0.5702, Test Acc: 0.5439\n",
      "Epoch: 142, Train Loss: 0.9388, Val Acc: 0.5497, Test Acc: 0.5439\n",
      "Epoch: 143, Train Loss: 0.9449, Val Acc: 0.5585, Test Acc: 0.5439\n",
      "Epoch: 144, Train Loss: 0.9479, Val Acc: 0.5409, Test Acc: 0.5439\n",
      "Epoch: 145, Train Loss: 0.9323, Val Acc: 0.5789, Test Acc: 0.5439\n",
      "Epoch: 146, Train Loss: 0.9352, Val Acc: 0.5936, Test Acc: 0.5819\n",
      "Epoch: 147, Train Loss: 0.9146, Val Acc: 0.5731, Test Acc: 0.5819\n",
      "Epoch: 148, Train Loss: 0.9246, Val Acc: 0.5789, Test Acc: 0.5819\n",
      "Epoch: 149, Train Loss: 0.9281, Val Acc: 0.5556, Test Acc: 0.5819\n",
      "Epoch: 150, Train Loss: 0.9442, Val Acc: 0.5731, Test Acc: 0.5819\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:44:31.887936Z",
     "start_time": "2025-02-16T19:44:31.775796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming your dataset is loaded\n",
    "filtered_dataset = [data for data in dataset if data.num_nodes < 300]\n",
    "\n",
    "# If you want to create a new dataset object\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class FilteredSCOPDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.data_list = [data for data in original_dataset if data.num_nodes < 300]\n",
    "        super().__init__(original_dataset.root)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create the filtered dataset\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Verify the filtering\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Optional: Check distribution across classes\n",
    "class_distribution = {}\n",
    "for data in filtered_dataset:\n",
    "    class_label = data.y.item()\n",
    "    class_distribution[class_label] = class_distribution.get(class_label, 0) + 1\n",
    "\n",
    "print(\"\\nClass distribution in filtered dataset:\")\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"Class {cls}: {count} proteins\")"
   ],
   "id": "db76b9390eadc048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 3420\n",
      "Filtered dataset size: 2717\n",
      "\n",
      "Class distribution in filtered dataset:\n",
      "Class 0: 428 proteins\n",
      "Class 6: 500 proteins\n",
      "Class 2: 368 proteins\n",
      "Class 3: 382 proteins\n",
      "Class 1: 449 proteins\n",
      "Class 4: 172 proteins\n",
      "Class 5: 418 proteins\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:09:49.628415Z",
     "start_time": "2025-02-16T19:46:45.492778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the filtered dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Shuffle and split the filtered dataset\n",
    "filtered_dataset = filtered_dataset.shuffle()\n",
    "n = (len(filtered_dataset) + 9) // 10\n",
    "test_dataset = filtered_dataset[:n]\n",
    "val_dataset = filtered_dataset[n:2 * n]\n",
    "train_dataset = filtered_dataset[2 * n:]\n",
    "\n",
    "# Create data loaders using the filtered datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"\\nFiltered Dataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# The rest of your training loop remains the same\n",
    "# You can use these loaders directly in your existing training script\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "79d27a0fbe34d6c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Dataset splits:\n",
      "Training samples: 2173\n",
      "Validation samples: 272\n",
      "Test samples: 272\n",
      "\n",
      "Dataset splits:\n",
      "Training samples: 2173\n",
      "Validation samples: 272\n",
      "Test samples: 272\n",
      "Epoch: 001, Train Loss: 1.8997, Val Acc: 0.2243, Test Acc: 0.1618\n",
      "Epoch: 002, Train Loss: 1.8608, Val Acc: 0.2978, Test Acc: 0.2096\n",
      "Epoch: 003, Train Loss: 1.8426, Val Acc: 0.3125, Test Acc: 0.2243\n",
      "Epoch: 004, Train Loss: 1.8319, Val Acc: 0.3051, Test Acc: 0.2243\n",
      "Epoch: 005, Train Loss: 1.7876, Val Acc: 0.3566, Test Acc: 0.2978\n",
      "Epoch: 006, Train Loss: 1.6013, Val Acc: 0.4669, Test Acc: 0.3419\n",
      "Epoch: 007, Train Loss: 1.5338, Val Acc: 0.4522, Test Acc: 0.3419\n",
      "Epoch: 008, Train Loss: 1.5132, Val Acc: 0.4890, Test Acc: 0.4522\n",
      "Epoch: 009, Train Loss: 1.4798, Val Acc: 0.4890, Test Acc: 0.4522\n",
      "Epoch: 010, Train Loss: 1.4786, Val Acc: 0.4743, Test Acc: 0.4522\n",
      "Epoch: 011, Train Loss: 1.4585, Val Acc: 0.5037, Test Acc: 0.4154\n",
      "Epoch: 012, Train Loss: 1.4297, Val Acc: 0.5110, Test Acc: 0.4412\n",
      "Epoch: 013, Train Loss: 1.4306, Val Acc: 0.5257, Test Acc: 0.4559\n",
      "Epoch: 014, Train Loss: 1.3999, Val Acc: 0.5074, Test Acc: 0.4559\n",
      "Epoch: 015, Train Loss: 1.3902, Val Acc: 0.5404, Test Acc: 0.4485\n",
      "Epoch: 016, Train Loss: 1.3713, Val Acc: 0.5478, Test Acc: 0.4706\n",
      "Epoch: 017, Train Loss: 1.3564, Val Acc: 0.5294, Test Acc: 0.4706\n",
      "Epoch: 018, Train Loss: 1.3461, Val Acc: 0.5331, Test Acc: 0.4706\n",
      "Epoch: 019, Train Loss: 1.3437, Val Acc: 0.5404, Test Acc: 0.4706\n",
      "Epoch: 020, Train Loss: 1.3250, Val Acc: 0.5441, Test Acc: 0.4706\n",
      "Epoch: 021, Train Loss: 1.3132, Val Acc: 0.5221, Test Acc: 0.4706\n",
      "Epoch: 022, Train Loss: 1.3171, Val Acc: 0.5478, Test Acc: 0.4706\n",
      "Epoch: 023, Train Loss: 1.3093, Val Acc: 0.5441, Test Acc: 0.4706\n",
      "Epoch: 024, Train Loss: 1.2926, Val Acc: 0.5515, Test Acc: 0.4743\n",
      "Epoch: 025, Train Loss: 1.3062, Val Acc: 0.5515, Test Acc: 0.4743\n",
      "Epoch: 026, Train Loss: 1.2945, Val Acc: 0.5257, Test Acc: 0.4743\n",
      "Epoch: 027, Train Loss: 1.2771, Val Acc: 0.5331, Test Acc: 0.4743\n",
      "Epoch: 028, Train Loss: 1.2699, Val Acc: 0.5588, Test Acc: 0.4963\n",
      "Epoch: 029, Train Loss: 1.2725, Val Acc: 0.5735, Test Acc: 0.5000\n",
      "Epoch: 030, Train Loss: 1.2513, Val Acc: 0.5478, Test Acc: 0.5000\n",
      "Epoch: 031, Train Loss: 1.2467, Val Acc: 0.5551, Test Acc: 0.5000\n",
      "Epoch: 032, Train Loss: 1.2493, Val Acc: 0.5625, Test Acc: 0.5000\n",
      "Epoch: 033, Train Loss: 1.2262, Val Acc: 0.5662, Test Acc: 0.5000\n",
      "Epoch: 034, Train Loss: 1.2103, Val Acc: 0.5515, Test Acc: 0.5000\n",
      "Epoch: 035, Train Loss: 1.1899, Val Acc: 0.5331, Test Acc: 0.5000\n",
      "Epoch: 036, Train Loss: 1.1701, Val Acc: 0.5699, Test Acc: 0.5000\n",
      "Epoch: 037, Train Loss: 1.1620, Val Acc: 0.5919, Test Acc: 0.5441\n",
      "Epoch: 038, Train Loss: 1.1500, Val Acc: 0.5588, Test Acc: 0.5441\n",
      "Epoch: 039, Train Loss: 1.1409, Val Acc: 0.5882, Test Acc: 0.5441\n",
      "Epoch: 040, Train Loss: 1.1154, Val Acc: 0.5809, Test Acc: 0.5441\n",
      "Epoch: 041, Train Loss: 1.1148, Val Acc: 0.5882, Test Acc: 0.5441\n",
      "Epoch: 042, Train Loss: 1.1029, Val Acc: 0.5993, Test Acc: 0.5662\n",
      "Epoch: 043, Train Loss: 1.1091, Val Acc: 0.6066, Test Acc: 0.5441\n",
      "Epoch: 044, Train Loss: 1.0999, Val Acc: 0.5478, Test Acc: 0.5441\n",
      "Epoch: 045, Train Loss: 1.0860, Val Acc: 0.5368, Test Acc: 0.5441\n",
      "Epoch: 046, Train Loss: 1.0790, Val Acc: 0.5625, Test Acc: 0.5441\n",
      "Epoch: 047, Train Loss: 1.0757, Val Acc: 0.5956, Test Acc: 0.5441\n",
      "Epoch: 048, Train Loss: 1.0445, Val Acc: 0.5551, Test Acc: 0.5441\n",
      "Epoch: 049, Train Loss: 1.0403, Val Acc: 0.5699, Test Acc: 0.5441\n",
      "Epoch: 050, Train Loss: 1.0414, Val Acc: 0.5846, Test Acc: 0.5441\n",
      "Epoch: 051, Train Loss: 1.0410, Val Acc: 0.5515, Test Acc: 0.5441\n",
      "Epoch: 052, Train Loss: 1.0431, Val Acc: 0.6213, Test Acc: 0.5588\n",
      "Epoch: 053, Train Loss: 1.0120, Val Acc: 0.5699, Test Acc: 0.5588\n",
      "Epoch: 054, Train Loss: 1.0221, Val Acc: 0.5699, Test Acc: 0.5588\n",
      "Epoch: 055, Train Loss: 1.0194, Val Acc: 0.5882, Test Acc: 0.5588\n",
      "Epoch: 056, Train Loss: 1.0126, Val Acc: 0.5699, Test Acc: 0.5588\n",
      "Epoch: 057, Train Loss: 1.0320, Val Acc: 0.6103, Test Acc: 0.5588\n",
      "Epoch: 058, Train Loss: 0.9907, Val Acc: 0.6103, Test Acc: 0.5588\n",
      "Epoch: 059, Train Loss: 0.9853, Val Acc: 0.6250, Test Acc: 0.5772\n",
      "Epoch: 060, Train Loss: 0.9962, Val Acc: 0.6103, Test Acc: 0.5772\n",
      "Epoch: 061, Train Loss: 0.9911, Val Acc: 0.6066, Test Acc: 0.5772\n",
      "Epoch: 062, Train Loss: 0.9791, Val Acc: 0.6103, Test Acc: 0.5772\n",
      "Epoch: 063, Train Loss: 0.9719, Val Acc: 0.6066, Test Acc: 0.5772\n",
      "Epoch: 064, Train Loss: 0.9679, Val Acc: 0.6213, Test Acc: 0.5772\n",
      "Epoch: 065, Train Loss: 0.9695, Val Acc: 0.5956, Test Acc: 0.5772\n",
      "Epoch: 066, Train Loss: 0.9707, Val Acc: 0.6360, Test Acc: 0.5515\n",
      "Epoch: 067, Train Loss: 0.9846, Val Acc: 0.6324, Test Acc: 0.5515\n",
      "Epoch: 068, Train Loss: 0.9595, Val Acc: 0.6397, Test Acc: 0.5882\n",
      "Epoch: 069, Train Loss: 0.9572, Val Acc: 0.6250, Test Acc: 0.5882\n",
      "Epoch: 070, Train Loss: 0.9572, Val Acc: 0.6360, Test Acc: 0.5882\n",
      "Epoch: 071, Train Loss: 0.9477, Val Acc: 0.6544, Test Acc: 0.5882\n",
      "Epoch: 072, Train Loss: 0.9641, Val Acc: 0.6654, Test Acc: 0.5993\n",
      "Epoch: 073, Train Loss: 0.9465, Val Acc: 0.6507, Test Acc: 0.5993\n",
      "Epoch: 074, Train Loss: 0.9469, Val Acc: 0.6324, Test Acc: 0.5993\n",
      "Epoch: 075, Train Loss: 0.9482, Val Acc: 0.6250, Test Acc: 0.5993\n",
      "Epoch: 076, Train Loss: 0.9559, Val Acc: 0.6103, Test Acc: 0.5993\n",
      "Epoch: 077, Train Loss: 0.9405, Val Acc: 0.6250, Test Acc: 0.5993\n",
      "Epoch: 078, Train Loss: 0.9286, Val Acc: 0.6434, Test Acc: 0.5993\n",
      "Epoch: 079, Train Loss: 0.9240, Val Acc: 0.6176, Test Acc: 0.5993\n",
      "Epoch: 080, Train Loss: 0.9260, Val Acc: 0.6471, Test Acc: 0.5993\n",
      "Epoch: 081, Train Loss: 0.9536, Val Acc: 0.6140, Test Acc: 0.5993\n",
      "Epoch: 082, Train Loss: 0.9316, Val Acc: 0.6397, Test Acc: 0.5993\n",
      "Epoch: 083, Train Loss: 0.9228, Val Acc: 0.6691, Test Acc: 0.6103\n",
      "Epoch: 084, Train Loss: 0.9127, Val Acc: 0.6507, Test Acc: 0.6103\n",
      "Epoch: 085, Train Loss: 0.9097, Val Acc: 0.6618, Test Acc: 0.6103\n",
      "Epoch: 086, Train Loss: 0.9226, Val Acc: 0.6875, Test Acc: 0.5882\n",
      "Epoch: 087, Train Loss: 0.9034, Val Acc: 0.6765, Test Acc: 0.5882\n",
      "Epoch: 088, Train Loss: 0.9033, Val Acc: 0.6434, Test Acc: 0.5882\n",
      "Epoch: 089, Train Loss: 0.9028, Val Acc: 0.6581, Test Acc: 0.5882\n",
      "Epoch: 090, Train Loss: 0.9079, Val Acc: 0.6654, Test Acc: 0.5882\n",
      "Epoch: 091, Train Loss: 0.8864, Val Acc: 0.6471, Test Acc: 0.5882\n",
      "Epoch: 092, Train Loss: 0.8960, Val Acc: 0.6875, Test Acc: 0.5882\n",
      "Epoch: 093, Train Loss: 0.8909, Val Acc: 0.6691, Test Acc: 0.5882\n",
      "Epoch: 094, Train Loss: 0.8906, Val Acc: 0.6140, Test Acc: 0.5882\n",
      "Epoch: 095, Train Loss: 0.8931, Val Acc: 0.6618, Test Acc: 0.5882\n",
      "Epoch: 096, Train Loss: 0.8897, Val Acc: 0.6618, Test Acc: 0.5882\n",
      "Epoch: 097, Train Loss: 0.8924, Val Acc: 0.6434, Test Acc: 0.5882\n",
      "Epoch: 098, Train Loss: 0.8807, Val Acc: 0.6691, Test Acc: 0.5882\n",
      "Epoch: 099, Train Loss: 0.8844, Val Acc: 0.6360, Test Acc: 0.5882\n",
      "Epoch: 100, Train Loss: 0.8677, Val Acc: 0.6765, Test Acc: 0.5882\n",
      "Epoch: 101, Train Loss: 0.8709, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 102, Train Loss: 0.8638, Val Acc: 0.6140, Test Acc: 0.5882\n",
      "Epoch: 103, Train Loss: 0.8594, Val Acc: 0.6618, Test Acc: 0.5882\n",
      "Epoch: 104, Train Loss: 0.8518, Val Acc: 0.6765, Test Acc: 0.5882\n",
      "Epoch: 105, Train Loss: 0.8502, Val Acc: 0.6691, Test Acc: 0.5882\n",
      "Epoch: 106, Train Loss: 0.8780, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 107, Train Loss: 0.8602, Val Acc: 0.6618, Test Acc: 0.5882\n",
      "Epoch: 108, Train Loss: 0.8615, Val Acc: 0.6250, Test Acc: 0.5882\n",
      "Epoch: 109, Train Loss: 0.8668, Val Acc: 0.6397, Test Acc: 0.5882\n",
      "Epoch: 110, Train Loss: 0.8668, Val Acc: 0.6581, Test Acc: 0.5882\n",
      "Epoch: 111, Train Loss: 0.8674, Val Acc: 0.6434, Test Acc: 0.5882\n",
      "Epoch: 112, Train Loss: 0.8240, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 113, Train Loss: 0.8353, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 114, Train Loss: 0.8392, Val Acc: 0.6471, Test Acc: 0.5882\n",
      "Epoch: 115, Train Loss: 0.8468, Val Acc: 0.6838, Test Acc: 0.5882\n",
      "Epoch: 116, Train Loss: 0.8411, Val Acc: 0.6838, Test Acc: 0.5882\n",
      "Epoch: 117, Train Loss: 0.8233, Val Acc: 0.6507, Test Acc: 0.5882\n",
      "Epoch: 118, Train Loss: 0.8261, Val Acc: 0.6471, Test Acc: 0.5882\n",
      "Epoch: 119, Train Loss: 0.8299, Val Acc: 0.6581, Test Acc: 0.5882\n",
      "Epoch: 120, Train Loss: 0.8204, Val Acc: 0.6765, Test Acc: 0.5882\n",
      "Epoch: 121, Train Loss: 0.8091, Val Acc: 0.6434, Test Acc: 0.5882\n",
      "Epoch: 122, Train Loss: 0.8103, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 123, Train Loss: 0.8166, Val Acc: 0.6765, Test Acc: 0.5882\n",
      "Epoch: 124, Train Loss: 0.8263, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 125, Train Loss: 0.8713, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 126, Train Loss: 0.8356, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 127, Train Loss: 0.8181, Val Acc: 0.6912, Test Acc: 0.5993\n",
      "Epoch: 128, Train Loss: 0.7919, Val Acc: 0.6765, Test Acc: 0.5993\n",
      "Epoch: 129, Train Loss: 0.8144, Val Acc: 0.6912, Test Acc: 0.5993\n",
      "Epoch: 130, Train Loss: 0.8158, Val Acc: 0.6728, Test Acc: 0.5993\n",
      "Epoch: 131, Train Loss: 0.8121, Val Acc: 0.6728, Test Acc: 0.5993\n",
      "Epoch: 132, Train Loss: 0.8180, Val Acc: 0.6654, Test Acc: 0.5993\n",
      "Epoch: 133, Train Loss: 0.7912, Val Acc: 0.6654, Test Acc: 0.5993\n",
      "Epoch: 134, Train Loss: 0.8171, Val Acc: 0.6691, Test Acc: 0.5993\n",
      "Epoch: 135, Train Loss: 0.8270, Val Acc: 0.6765, Test Acc: 0.5993\n",
      "Epoch: 136, Train Loss: 0.7854, Val Acc: 0.6875, Test Acc: 0.5993\n",
      "Epoch: 137, Train Loss: 0.7822, Val Acc: 0.6985, Test Acc: 0.5882\n",
      "Epoch: 138, Train Loss: 0.7836, Val Acc: 0.6691, Test Acc: 0.5882\n",
      "Epoch: 139, Train Loss: 0.7937, Val Acc: 0.6875, Test Acc: 0.5882\n",
      "Epoch: 140, Train Loss: 0.8023, Val Acc: 0.6801, Test Acc: 0.5882\n",
      "Epoch: 141, Train Loss: 0.8311, Val Acc: 0.6875, Test Acc: 0.5882\n",
      "Epoch: 142, Train Loss: 0.8246, Val Acc: 0.6765, Test Acc: 0.5882\n",
      "Epoch: 143, Train Loss: 0.7762, Val Acc: 0.6728, Test Acc: 0.5882\n",
      "Epoch: 144, Train Loss: 0.7490, Val Acc: 0.6838, Test Acc: 0.5882\n",
      "Epoch: 145, Train Loss: 0.7943, Val Acc: 0.6765, Test Acc: 0.5882\n",
      "Epoch: 146, Train Loss: 0.7849, Val Acc: 0.6397, Test Acc: 0.5882\n",
      "Epoch: 147, Train Loss: 0.8117, Val Acc: 0.6654, Test Acc: 0.5882\n",
      "Epoch: 148, Train Loss: 0.7647, Val Acc: 0.6875, Test Acc: 0.5882\n",
      "Epoch: 149, Train Loss: 0.7730, Val Acc: 0.6838, Test Acc: 0.5882\n",
      "Epoch: 150, Train Loss: 0.7714, Val Acc: 0.6507, Test Acc: 0.5882\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "29c399fd73d86af1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
