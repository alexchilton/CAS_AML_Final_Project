{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T21:42:07.700809Z",
     "start_time": "2025-02-16T21:42:07.694561Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv  # Changed from DenseSAGEConv\n",
    "from torch_geometric.nn import global_mean_pool  # For sparse pooling\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define data directory\n",
    "data_dir = 'data/SCOP'  # Base directory for SCOP data\n",
    "\n",
    "# Rest of the model code remains the same..."
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T21:42:10.211729Z",
     "start_time": "2025-02-16T21:42:10.178794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "from Bio.PDB import NeighborSearch, Selection\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SparseSCOPDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root: str, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.root = root\n",
    "        self.class_info_path = os.path.join(root, 'raw/class_info.csv')\n",
    "\n",
    "        # Add normalization parameters\n",
    "        self.coord_mean: Optional[float] = None\n",
    "        self.coord_std: Optional[float] = None\n",
    "        self.mass_mean: Optional[float] = None\n",
    "        self.mass_std: Optional[float] = None\n",
    "        self.dist_mean: Optional[float] = None\n",
    "        self.dist_std: Optional[float] = None\n",
    "        self.count_mean: Optional[float] = None\n",
    "        self.count_std: Optional[float] = None\n",
    "\n",
    "        # Feature indices for easy access\n",
    "        self.feature_indices = {\n",
    "            'aa_onehot': slice(0, 21),\n",
    "            'coords': slice(21, 24),\n",
    "            'mass': 24,\n",
    "            'avg_dist': 25,\n",
    "            'max_dist': 26,\n",
    "            'neighbor_count': 27\n",
    "        }\n",
    "\n",
    "        # Dictionary to map SCOP classes to indices\n",
    "        self.class_mapping = {\n",
    "            'a': 0,  # All-alpha\n",
    "            'b': 1,  # All-beta\n",
    "            'c': 2,  # Alpha/beta\n",
    "            'd': 3,  # Alpha+beta\n",
    "            'e': 4,  # Multi-domain\n",
    "            'f': 5,  # Membrane\n",
    "            'g': 6   # Small proteins\n",
    "        }\n",
    "\n",
    "        # Dictionary to map amino acids to indices and their properties\n",
    "        self.amino_acids = {\n",
    "            'ALA': {'index': 0, 'mass': 89.1, 'name': 'Alanine'},\n",
    "            'ARG': {'index': 1, 'mass': 174.2, 'name': 'Arginine'},\n",
    "            'ASN': {'index': 2, 'mass': 132.1, 'name': 'Asparagine'},\n",
    "            'ASP': {'index': 3, 'mass': 133.1, 'name': 'Aspartic Acid'},\n",
    "            'CYS': {'index': 4, 'mass': 121.2, 'name': 'Cysteine'},\n",
    "            'GLN': {'index': 5, 'mass': 146.2, 'name': 'Glutamine'},\n",
    "            'GLU': {'index': 6, 'mass': 147.1, 'name': 'Glutamic Acid'},\n",
    "            'GLY': {'index': 7, 'mass': 75.1, 'name': 'Glycine'},\n",
    "            'HIS': {'index': 8, 'mass': 155.2, 'name': 'Histidine'},\n",
    "            'ILE': {'index': 9, 'mass': 131.2, 'name': 'Isoleucine'},\n",
    "            'LEU': {'index': 10, 'mass': 131.2, 'name': 'Leucine'},\n",
    "            'LYS': {'index': 11, 'mass': 146.2, 'name': 'Lysine'},\n",
    "            'MET': {'index': 12, 'mass': 149.2, 'name': 'Methionine'},\n",
    "            'PHE': {'index': 13, 'mass': 165.2, 'name': 'Phenylalanine'},\n",
    "            'PRO': {'index': 14, 'mass': 115.1, 'name': 'Proline'},\n",
    "            'SER': {'index': 15, 'mass': 105.1, 'name': 'Serine'},\n",
    "            'THR': {'index': 16, 'mass': 119.1, 'name': 'Threonine'},\n",
    "            'TRP': {'index': 17, 'mass': 204.2, 'name': 'Tryptophan'},\n",
    "            'TYR': {'index': 18, 'mass': 181.2, 'name': 'Tyrosine'},\n",
    "            'VAL': {'index': 19, 'mass': 117.1, 'name': 'Valine'},\n",
    "            'UNK': {'index': 20, 'mass': 0.0, 'name': 'Unknown'}\n",
    "        }\n",
    "\n",
    "        # Load class information before calling super().__init__()\n",
    "        if os.path.exists(self.class_info_path):\n",
    "            self.class_info = pd.read_csv(self.class_info_path)\n",
    "            print(f\"Found class info file with {len(self.class_info)} entries\")\n",
    "        else:\n",
    "            print(f\"Warning: class_info.csv not found at {self.class_info_path}\")\n",
    "            self.class_info = None\n",
    "\n",
    "        # Initialize the base class after setting up our attributes\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"List of raw file names.\"\"\"\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"List of processed file names.\"\"\"\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download function is not needed as we assume data is present.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Process the raw data into the internal format.\"\"\"\n",
    "        if self.class_info is None:\n",
    "            raise RuntimeError(\"No class information available. Cannot process data.\")\n",
    "\n",
    "        data_list = []\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "\n",
    "        # Collect statistics for normalization\n",
    "        all_coords = []\n",
    "        all_masses = []\n",
    "        all_distances = []\n",
    "        all_counts = []\n",
    "\n",
    "        for idx, row in self.class_info.iterrows():\n",
    "            pdb_id = str(row['scop_id'])\n",
    "            class_label = self.class_mapping[row['class']]\n",
    "\n",
    "            try:\n",
    "                # Load structure\n",
    "                pdb_file = f\"{pdb_id}.pdb\"\n",
    "                pdb_path = os.path.join(self.root, 'raw', pdb_file)\n",
    "                structure = parser.get_structure('protein', pdb_path)\n",
    "                model = structure[0]\n",
    "\n",
    "                # Get residues and create features\n",
    "                residues = list(model.get_residues())\n",
    "\n",
    "                # Create neighbor search for the entire structure\n",
    "                atom_list = Selection.unfold_entities(model, 'A')\n",
    "                ns = NeighborSearch(atom_list)\n",
    "\n",
    "                # Process residues and collect statistics\n",
    "                node_features = []\n",
    "                for residue in residues:\n",
    "                    center = residue['CA'].get_coord() if 'CA' in residue else None\n",
    "                    if center is not None:\n",
    "                        neighbors = ns.search(center, 5.0, level='R')\n",
    "                        features = self._get_residue_features(residue, neighbors)\n",
    "                        node_features.append(features)\n",
    "\n",
    "                        # Collect statistics\n",
    "                        all_coords.extend(features[21:24])\n",
    "                        all_masses.append(features[24])\n",
    "                        all_distances.extend(features[25:27])\n",
    "                        all_counts.append(features[27])\n",
    "\n",
    "                # Create edges with 5Ã… cutoff\n",
    "                edges = []\n",
    "                for i in range(len(residues)):\n",
    "                    for j in range(i+1, len(residues)):\n",
    "                        if 'CA' in residues[i] and 'CA' in residues[j]:\n",
    "                            ca_i = residues[i]['CA'].get_coord()\n",
    "                            ca_j = residues[j]['CA'].get_coord()\n",
    "                            dist = np.linalg.norm(ca_i - ca_j)\n",
    "                            if dist < 5.0:\n",
    "                                edges.append([i, j])\n",
    "                                edges.append([j, i])\n",
    "\n",
    "                if len(edges) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Create PyG Data object\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "                x = torch.tensor(node_features, dtype=torch.float)\n",
    "                y = torch.tensor([class_label], dtype=torch.long)\n",
    "\n",
    "                data = Data(x=x, edge_index=edge_index, y=y, num_nodes=len(residues))\n",
    "\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                data_list.append(data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdb_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if len(data_list) == 0:\n",
    "            raise RuntimeError(\"No data was successfully processed!\")\n",
    "\n",
    "        # Calculate normalization parameters\n",
    "        all_coords = np.array(all_coords)\n",
    "        all_masses = np.array(all_masses)\n",
    "        all_distances = np.array(all_distances)\n",
    "        all_counts = np.array(all_counts)\n",
    "\n",
    "        # Save normalization parameters\n",
    "        self.coord_mean = float(np.mean(all_coords))\n",
    "        self.coord_std = float(np.std(all_coords))\n",
    "        self.mass_mean = float(np.mean(all_masses))\n",
    "        self.mass_std = float(np.std(all_masses))\n",
    "        self.dist_mean = float(np.mean(all_distances))\n",
    "        self.dist_std = float(np.std(all_distances))\n",
    "        self.count_mean = float(np.mean(all_counts))\n",
    "        self.count_std = float(np.std(all_counts))\n",
    "\n",
    "        # Create processed directory if it doesn't exist\n",
    "        os.makedirs(self.processed_dir, exist_ok=True)\n",
    "\n",
    "        # Save both the processed data and normalization parameters\n",
    "        torch.save({\n",
    "            'data_list': data_list,\n",
    "            'normalization': {\n",
    "                'coord_mean': self.coord_mean,\n",
    "                'coord_std': self.coord_std,\n",
    "                'mass_mean': self.mass_mean,\n",
    "                'mass_std': self.mass_std,\n",
    "                'dist_mean': self.dist_mean,\n",
    "                'dist_std': self.dist_std,\n",
    "                'count_mean': self.count_mean,\n",
    "                'count_std': self.count_std\n",
    "            }\n",
    "        }, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"Return the number of graphs in the dataset.\"\"\"\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "            if not os.path.exists(processed_path):\n",
    "                self.process()\n",
    "\n",
    "            saved_data = torch.load(processed_path, weights_only=False)\n",
    "            self._data_list = saved_data['data_list']\n",
    "\n",
    "            # Load normalization parameters\n",
    "            norm_params = saved_data['normalization']\n",
    "            self.coord_mean = norm_params['coord_mean']\n",
    "            self.coord_std = norm_params['coord_std']\n",
    "            self.mass_mean = norm_params['mass_mean']\n",
    "            self.mass_std = norm_params['mass_std']\n",
    "            self.dist_mean = norm_params['dist_mean']\n",
    "            self.dist_std = norm_params['dist_std']\n",
    "            self.count_mean = norm_params['count_mean']\n",
    "            self.count_std = norm_params['count_std']\n",
    "\n",
    "        return len(self._data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Get a specific graph from the dataset.\"\"\"\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            self.len()  # This will load the data\n",
    "        return self._data_list[idx]\n",
    "\n",
    "    def _get_residue_features(self, residue, neighbors):\n",
    "        \"\"\"Create expanded feature vector for a residue.\"\"\"\n",
    "        # One-hot encode amino acid type\n",
    "        aa_features = np.zeros(21)  # 20 standard amino acids + UNK\n",
    "        aa_name = residue.get_resname()\n",
    "        aa_info = self.amino_acids.get(aa_name, self.amino_acids['UNK'])\n",
    "        aa_features[aa_info['index']] = 1\n",
    "\n",
    "        # Get mass\n",
    "        mass = aa_info['mass']\n",
    "\n",
    "        # Get CA atom coordinates\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            coords = ca_atom.get_coord()\n",
    "        except:\n",
    "            coords = np.zeros(3)\n",
    "\n",
    "        # Calculate neighborhood features\n",
    "        if neighbors:\n",
    "            neighbor_distances = []\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor != residue and 'CA' in neighbor:\n",
    "                    dist = np.linalg.norm(coords - neighbor['CA'].get_coord())\n",
    "                    neighbor_distances.append(dist)\n",
    "\n",
    "            avg_neighbor_dist = np.mean(neighbor_distances) if neighbor_distances else 0\n",
    "            max_neighbor_dist = np.max(neighbor_distances) if neighbor_distances else 0\n",
    "            neighbor_count = len(neighbor_distances)\n",
    "        else:\n",
    "            avg_neighbor_dist = 0\n",
    "            max_neighbor_dist = 0\n",
    "            neighbor_count = 0\n",
    "\n",
    "        # Combine all features\n",
    "        features = np.concatenate([\n",
    "            aa_features,          # Amino acid identity (21)\n",
    "            coords,              # 3D coordinates (3)\n",
    "            [mass],             # Mass (1)\n",
    "            [avg_neighbor_dist], # Average neighbor distance (1)\n",
    "            [max_neighbor_dist], # Maximum neighbor distance (1)\n",
    "            [neighbor_count]     # Number of neighbors (1)\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_amino_acid_name(self, features: torch.Tensor) -> str:\n",
    "        \"\"\"Get amino acid name from one-hot encoded features.\"\"\"\n",
    "        aa_idx = torch.argmax(features[self.feature_indices['aa_onehot']]).item()\n",
    "        for aa, info in self.amino_acids.items():\n",
    "            if info['index'] == aa_idx:\n",
    "                return aa\n",
    "        return 'UNK'\n",
    "\n",
    "    def get_feature_info(self, data: Data, node_idx: int) -> Dict[str, float]:\n",
    "        \"\"\"Get denormalized feature information for a specific node.\"\"\"\n",
    "        features = data.x[node_idx]\n",
    "        aa_name = self.get_amino_acid_name(features)\n",
    "\n",
    "        return {\n",
    "            'amino_acid': aa_name,\n",
    "            'full_name': self.amino_acids[aa_name]['name'],\n",
    "            'coordinates': [\n",
    "                self.denormalize_feature(features[21].item(), 'x'),\n",
    "                self.denormalize_feature(features[22].item(), 'y'),\n",
    "                self.denormalize_feature(features[23].item(), 'z')\n",
    "            ],\n",
    "            'mass': self.denormalize_feature(features[24].item(), 'mass'),\n",
    "            'avg_neighbor_dist': self.denormalize_feature(features[25].item(), 'avg_dist'),\n",
    "            'max_neighbor_dist': self.denormalize_feature(features[26].item(), 'max_dist'),\n",
    "            'neighbor_count': round(self.denormalize_feature(features[27].item(), 'neighbor_count'))\n",
    "        }\n",
    "\n",
    "    def denormalize_feature(self, value: float, feature_name: str) -> float:\n",
    "        \"\"\"Denormalize a single feature value.\"\"\"\n",
    "        if feature_name in ['x', 'y', 'z']:\n",
    "            return value * self.coord_std + self.coord_mean\n",
    "        elif feature_name == 'mass':\n",
    "            return value * self.mass_std + self.mass_mean\n",
    "        elif feature_name in ['avg_dist', 'max_dist']:\n",
    "            return value * self.dist_std + self.dist_mean\n",
    "        elif feature_name == 'neighbor_count':\n",
    "            return value * self.count_std + self.count_mean\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature name: {feature_name}\")\n",
    "\n",
    "    def get_graph_stats(self, data: Data) -> Dict[str, float]:\n",
    "        \"\"\"Get statistical information about the protein graph.\"\"\"\n",
    "        G = nx.Graph()\n",
    "        edge_index = data.edge_index.numpy()\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            G.add_edge(edge_index[0, i], edge_index[1, i])\n",
    "\n",
    "        return {\n",
    "            'num_nodes': G.number_of_nodes(),\n",
    "            'num_edges': G.number_of_edges(),\n",
    "            'average_degree': sum(dict(G.degree()).values()) / G.number_of_nodes(),\n",
    "            'density': nx.density(G),\n",
    "            'is_connected': nx.is_connected(G),\n",
    "            'average_clustering': nx.average_clustering(G),\n",
    "            'average_shortest_path_length': nx.average_shortest_path_length(G) if nx.is_connected(G) else float('inf')\n",
    "        }\n",
    "\n",
    "    def visualize_protein(self, data: Data,\n",
    "                          color_by: str = 'amino_acid',\n",
    "                          node_size: int = 100,\n",
    "                          with_labels: bool = True,\n",
    "                          figure_size: Tuple[int, int] = (12, 8)) -> None:\n",
    "        \"\"\"\n",
    "        Visualize protein structure as a graph.\n",
    "\n",
    "        Args:\n",
    "            data: PyG Data object\n",
    "            color_by: Feature to color nodes by ('amino_acid', 'mass', 'neighbor_count')\n",
    "            node_size: Size of nodes in the visualization\n",
    "            with_labels: Whether to show node labels\n",
    "            figure_size: Size of the figure (width, height)\n",
    "        \"\"\"\n",
    "        # Convert to networkx\n",
    "        G = nx.Graph()\n",
    "        edge_index = data.edge_index.numpy()\n",
    "\n",
    "        # Add edges\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            G.add_edge(edge_index[0, i], edge_index[1, i])\n",
    "\n",
    "        # Prepare node colors and labels\n",
    "        node_colors = []\n",
    "        node_labels = {}\n",
    "\n",
    "        for node in G.nodes():\n",
    "            info = self.get_feature_info(data, node)\n",
    "\n",
    "            if color_by == 'amino_acid':\n",
    "                # Use a hash of amino acid name for color\n",
    "                color = hash(info['amino_acid']) % 20 / 20.0\n",
    "                node_colors.append(plt.cm.tab20(color))\n",
    "            elif color_by == 'mass':\n",
    "                node_colors.append(info['mass'])\n",
    "            elif color_by == 'neighbor_count':\n",
    "                node_colors.append(info['neighbor_count'])\n",
    "\n",
    "            if with_labels:\n",
    "                node_labels[node] = info['amino_acid']\n",
    "\n",
    "        # Create visualization\n",
    "        plt.figure(figsize=figure_size)\n",
    "        pos = nx.spring_layout(G)\n",
    "\n",
    "        nx.draw(G, pos,\n",
    "                node_color=node_colors,\n",
    "                node_size=node_size,\n",
    "                with_labels=with_labels,\n",
    "                labels=node_labels if with_labels else None,\n",
    "                cmap=plt.cm.viridis if color_by != 'amino_acid' else None)\n",
    "\n",
    "        if color_by != 'amino_acid':\n",
    "            sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis)\n",
    "            sm.set_array([])\n",
    "            plt.colorbar(sm, label=color_by.replace('_', ' ').title())\n",
    "\n",
    "        plt.title(f'Protein Structure Graph (colored by {color_by})')\n",
    "        plt.show()\n",
    "\n"
   ],
   "id": "8eee4b4a0d18bcc8",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T21:46:49.894894Z",
     "start_time": "2025-02-16T21:42:25.998134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import os\n",
    "#os.makedirs(os.path.join(data_dir, 'processed'), exist_ok=True)  # Make sure processed dir exists\n",
    "#os.remove(os.path.join(data_dir, 'processed/data.pt'))  # Remove old processed file\n",
    "\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "data = dataset[0]  # Should work now"
   ],
   "id": "dffda15480f19e44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class info file with 3500 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 76386: [Errno 2] No such file or directory: 'data/SCOP/raw/76386.pdb'\n",
      "Error processing 191866: [Errno 2] No such file or directory: 'data/SCOP/raw/191866.pdb'\n",
      "Error processing 246439: [Errno 2] No such file or directory: 'data/SCOP/raw/246439.pdb'\n",
      "Error processing 103823: [Errno 2] No such file or directory: 'data/SCOP/raw/103823.pdb'\n",
      "Error processing 81092: [Errno 2] No such file or directory: 'data/SCOP/raw/81092.pdb'\n",
      "Error processing 73992: [Errno 2] No such file or directory: 'data/SCOP/raw/73992.pdb'\n",
      "Error processing 364407: [Errno 2] No such file or directory: 'data/SCOP/raw/364407.pdb'\n",
      "Error processing 80047: [Errno 2] No such file or directory: 'data/SCOP/raw/80047.pdb'\n",
      "Error processing 308345: [Errno 2] No such file or directory: 'data/SCOP/raw/308345.pdb'\n",
      "Error processing 309195: [Errno 2] No such file or directory: 'data/SCOP/raw/309195.pdb'\n",
      "Error processing 164734: [Errno 2] No such file or directory: 'data/SCOP/raw/164734.pdb'\n",
      "Error processing 60948: [Errno 2] No such file or directory: 'data/SCOP/raw/60948.pdb'\n",
      "Error processing 267201: [Errno 2] No such file or directory: 'data/SCOP/raw/267201.pdb'\n",
      "Error processing 153665: [Errno 2] No such file or directory: 'data/SCOP/raw/153665.pdb'\n",
      "Error processing 339189: [Errno 2] No such file or directory: 'data/SCOP/raw/339189.pdb'\n",
      "Error processing 132994: [Errno 2] No such file or directory: 'data/SCOP/raw/132994.pdb'\n",
      "Error processing 345843: [Errno 2] No such file or directory: 'data/SCOP/raw/345843.pdb'\n",
      "Error processing 132457: [Errno 2] No such file or directory: 'data/SCOP/raw/132457.pdb'\n",
      "Error processing 326305: [Errno 2] No such file or directory: 'data/SCOP/raw/326305.pdb'\n",
      "Error processing 265116: [Errno 2] No such file or directory: 'data/SCOP/raw/265116.pdb'\n",
      "Error processing 91365: [Errno 2] No such file or directory: 'data/SCOP/raw/91365.pdb'\n",
      "Error processing 356662: [Errno 2] No such file or directory: 'data/SCOP/raw/356662.pdb'\n",
      "Error processing 341909: [Errno 2] No such file or directory: 'data/SCOP/raw/341909.pdb'\n",
      "Error processing 377878: [Errno 2] No such file or directory: 'data/SCOP/raw/377878.pdb'\n",
      "Error processing 36580: [Errno 2] No such file or directory: 'data/SCOP/raw/36580.pdb'\n",
      "Error processing 366454: [Errno 2] No such file or directory: 'data/SCOP/raw/366454.pdb'\n",
      "Error processing 236835: [Errno 2] No such file or directory: 'data/SCOP/raw/236835.pdb'\n",
      "Error processing 364435: [Errno 2] No such file or directory: 'data/SCOP/raw/364435.pdb'\n",
      "Error processing 66318: [Errno 2] No such file or directory: 'data/SCOP/raw/66318.pdb'\n",
      "Error processing 154452: [Errno 2] No such file or directory: 'data/SCOP/raw/154452.pdb'\n",
      "Error processing 130942: [Errno 2] No such file or directory: 'data/SCOP/raw/130942.pdb'\n",
      "Error processing 38724: [Errno 2] No such file or directory: 'data/SCOP/raw/38724.pdb'\n",
      "Error processing 220661: [Errno 2] No such file or directory: 'data/SCOP/raw/220661.pdb'\n",
      "Error processing 259537: [Errno 2] No such file or directory: 'data/SCOP/raw/259537.pdb'\n",
      "Error processing 123247: [Errno 2] No such file or directory: 'data/SCOP/raw/123247.pdb'\n",
      "Error processing 414366: [Errno 2] No such file or directory: 'data/SCOP/raw/414366.pdb'\n",
      "Error processing 330126: [Errno 2] No such file or directory: 'data/SCOP/raw/330126.pdb'\n",
      "Error processing 123189: [Errno 2] No such file or directory: 'data/SCOP/raw/123189.pdb'\n",
      "Error processing 340813: [Errno 2] No such file or directory: 'data/SCOP/raw/340813.pdb'\n",
      "Error processing 364126: [Errno 2] No such file or directory: 'data/SCOP/raw/364126.pdb'\n",
      "Error processing 164363: [Errno 2] No such file or directory: 'data/SCOP/raw/164363.pdb'\n",
      "Error processing 37191: [Errno 2] No such file or directory: 'data/SCOP/raw/37191.pdb'\n",
      "Error processing 366299: [Errno 2] No such file or directory: 'data/SCOP/raw/366299.pdb'\n",
      "Error processing 106664: [Errno 2] No such file or directory: 'data/SCOP/raw/106664.pdb'\n",
      "Error processing 39395: [Errno 2] No such file or directory: 'data/SCOP/raw/39395.pdb'\n",
      "Error processing 85006: [Errno 2] No such file or directory: 'data/SCOP/raw/85006.pdb'\n",
      "Error processing 354858: [Errno 2] No such file or directory: 'data/SCOP/raw/354858.pdb'\n",
      "Error processing 213119: [Errno 2] No such file or directory: 'data/SCOP/raw/213119.pdb'\n",
      "Error processing 363398: [Errno 2] No such file or directory: 'data/SCOP/raw/363398.pdb'\n",
      "Error processing 240225: [Errno 2] No such file or directory: 'data/SCOP/raw/240225.pdb'\n",
      "Error processing 181210: [Errno 2] No such file or directory: 'data/SCOP/raw/181210.pdb'\n",
      "Error processing 380972: [Errno 2] No such file or directory: 'data/SCOP/raw/380972.pdb'\n",
      "Error processing 147410: [Errno 2] No such file or directory: 'data/SCOP/raw/147410.pdb'\n",
      "Error processing 204296: [Errno 2] No such file or directory: 'data/SCOP/raw/204296.pdb'\n",
      "Error processing 278604: [Errno 2] No such file or directory: 'data/SCOP/raw/278604.pdb'\n",
      "Error processing 307468: [Errno 2] No such file or directory: 'data/SCOP/raw/307468.pdb'\n",
      "Error processing 40300: [Errno 2] No such file or directory: 'data/SCOP/raw/40300.pdb'\n",
      "Error processing 335336: [Errno 2] No such file or directory: 'data/SCOP/raw/335336.pdb'\n",
      "Error processing 161409: [Errno 2] No such file or directory: 'data/SCOP/raw/161409.pdb'\n",
      "Error processing 354518: [Errno 2] No such file or directory: 'data/SCOP/raw/354518.pdb'\n",
      "Error processing 83344: [Errno 2] No such file or directory: 'data/SCOP/raw/83344.pdb'\n",
      "Error processing 328217: [Errno 2] No such file or directory: 'data/SCOP/raw/328217.pdb'\n",
      "Error processing 154958: [Errno 2] No such file or directory: 'data/SCOP/raw/154958.pdb'\n",
      "Error processing 168337: [Errno 2] No such file or directory: 'data/SCOP/raw/168337.pdb'\n",
      "Error processing 406285: [Errno 2] No such file or directory: 'data/SCOP/raw/406285.pdb'\n",
      "Error processing 261684: [Errno 2] No such file or directory: 'data/SCOP/raw/261684.pdb'\n",
      "Error processing 364304: [Errno 2] No such file or directory: 'data/SCOP/raw/364304.pdb'\n",
      "Error processing 196052: [Errno 2] No such file or directory: 'data/SCOP/raw/196052.pdb'\n",
      "Error processing 264760: [Errno 2] No such file or directory: 'data/SCOP/raw/264760.pdb'\n",
      "Error processing 80401: [Errno 2] No such file or directory: 'data/SCOP/raw/80401.pdb'\n",
      "Error processing 251902: [Errno 2] No such file or directory: 'data/SCOP/raw/251902.pdb'\n",
      "Error processing 103944: [Errno 2] No such file or directory: 'data/SCOP/raw/103944.pdb'\n",
      "Error processing 79385: [Errno 2] No such file or directory: 'data/SCOP/raw/79385.pdb'\n",
      "Error processing 123501: [Errno 2] No such file or directory: 'data/SCOP/raw/123501.pdb'\n",
      "Error processing 374219: [Errno 2] No such file or directory: 'data/SCOP/raw/374219.pdb'\n",
      "Error processing 135050: [Errno 2] No such file or directory: 'data/SCOP/raw/135050.pdb'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T21:46:49.939572Z",
     "start_time": "2025-02-16T21:46:49.923157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stats = dataset.get_graph_stats(data)\n",
    "print(stats)"
   ],
   "id": "8674129ceb6208ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_nodes': 177, 'num_edges': 214, 'average_degree': 2.4180790960451977, 'density': 0.013739085772984078, 'is_connected': True, 'average_clustering': 0.002071563088512241, 'average_shortest_path_length': 24.853428351309706}\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T21:46:49.990560Z",
     "start_time": "2025-02-16T21:46:49.982737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#dataset = SparseSCOPDataset(root=data_dir)\n",
    "#data = dataset[0]\n",
    "\n",
    "# Get info about a specific residue\n",
    "info = dataset.get_feature_info(data, node_idx=0)\n",
    "print(info)\n",
    "\n",
    "# Visualize the protein in different ways\n",
    "#dataset.visualize_protein(data, color_by='amino_acid')\n",
    "#dataset.visualize_protein(data, color_by='mass')\n",
    "#dataset.visualize_protein(data, color_by='neighbor_count')\n",
    "\n",
    "# Get graph statistics\n"
   ],
   "id": "8591d3a1034ccdb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amino_acid': 'GLU', 'full_name': 'Glutamic Acid', 'coordinates': [10233.289051144287, 8598.223906333711, 5686.039750469833], 'mass': 4408.534576972759, 'avg_neighbor_dist': 12.091510059127168, 'max_neighbor_dist': 13.52793481138863, 'neighbor_count': 10}\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:04:25.151127Z",
     "start_time": "2025-02-16T22:04:23.932080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "\n",
    "NUM_CLASSES = 7  # SCOP main classes\n",
    "\n",
    "# Load the processed data\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "saved_data = torch.load(processed_path, weights_only=False)\n",
    "data_list = saved_data['data_list']  # Access the data_list from the dictionary\n",
    "\n",
    "# Analyze protein sizes\n",
    "sizes = [data.num_nodes for data in data_list]\n",
    "\n",
    "print(f\"Protein size statistics:\")\n",
    "print(f\"Min size: {min(sizes)}\")\n",
    "print(f\"Max size: {max(sizes)}\")\n",
    "print(f\"Mean size: {sum(sizes)/len(sizes):.1f}\")\n",
    "print(f\"Median size: {sorted(sizes)[len(sizes)//2]}\")\n",
    "print(f\"Number of proteins > 150 residues: {sum(1 for s in sizes if s > 150)}\")\n",
    "\n",
    "# Set max_nodes to match maximum size found\n",
    "max_nodes = max(sizes) + 20  # Add some buffer\n",
    "\n",
    "# Create dataset\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset[0].num_features}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ],
   "id": "81c665801b8fe044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein size statistics:\n",
      "Min size: 20\n",
      "Max size: 1523\n",
      "Mean size: 204.2\n",
      "Median size: 165\n",
      "Number of proteins > 150 residues: 1863\n",
      "Found class info file with 3500 entries\n",
      "\n",
      "Dataset size: 3424\n",
      "Number of features: 28\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "243e6c1a459fa5a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:04:30.163265Z",
     "start_time": "2025-02-16T22:04:29.845626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the processed data\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "saved_data = torch.load(processed_path, weights_only=False)\n",
    "data_list = saved_data['data_list']  # Access the data_list from the dictionary\n",
    "\n",
    "# Reverse class mapping for readable labels\n",
    "class_mapping_reverse = {\n",
    "    0: 'a (All-alpha)',\n",
    "    1: 'b (All-beta)',\n",
    "    2: 'c (Alpha/beta)',\n",
    "    3: 'd (Alpha+beta)',\n",
    "    4: 'e (Multi-domain)',\n",
    "    5: 'f (Membrane)',\n",
    "    6: 'g (Small proteins)'\n",
    "}\n",
    "\n",
    "# Separate nodes by class\n",
    "nodes_by_class = {}\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in nodes_by_class:\n",
    "        nodes_by_class[class_label] = []\n",
    "    nodes_by_class[class_label].append(data.num_nodes)\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Box plot\n",
    "plt.boxplot([nodes_by_class[key] for key in sorted(nodes_by_class.keys())],\n",
    "            labels=[class_mapping_reverse[key] for key in sorted(nodes_by_class.keys())])\n",
    "\n",
    "plt.title('Number of Nodes per SCOP Class', fontsize=16)\n",
    "plt.xlabel('SCOP Class', fontsize=12)\n",
    "plt.ylabel('Number of Nodes (Residues)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('nodes_per_class_boxplot.png')\n",
    "plt.close()\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Node count statistics per class:\")\n",
    "for class_label, nodes in nodes_by_class.items():\n",
    "    print(f\"\\n{class_mapping_reverse[class_label]}:\")\n",
    "    print(f\"  Count: {len(nodes)}\")\n",
    "    print(f\"  Min nodes: {min(nodes)}\")\n",
    "    print(f\"  Max nodes: {max(nodes)}\")\n",
    "    print(f\"  Mean nodes: {np.mean(nodes):.2f}\")\n",
    "    print(f\"  Median nodes: {np.median(nodes):.2f}\")"
   ],
   "id": "27dc0cc370f2c3dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node count statistics per class:\n",
      "\n",
      "a (All-alpha):\n",
      "  Count: 500\n",
      "  Min nodes: 39\n",
      "  Max nodes: 656\n",
      "  Mean nodes: 172.21\n",
      "  Median nodes: 142.50\n",
      "\n",
      "b (All-beta):\n",
      "  Count: 500\n",
      "  Min nodes: 53\n",
      "  Max nodes: 548\n",
      "  Mean nodes: 170.79\n",
      "  Median nodes: 135.00\n",
      "\n",
      "c (Alpha/beta):\n",
      "  Count: 500\n",
      "  Min nodes: 44\n",
      "  Max nodes: 815\n",
      "  Mean nodes: 255.96\n",
      "  Median nodes: 247.00\n",
      "\n",
      "d (Alpha+beta):\n",
      "  Count: 424\n",
      "  Min nodes: 56\n",
      "  Max nodes: 640\n",
      "  Mean nodes: 183.87\n",
      "  Median nodes: 164.00\n",
      "\n",
      "e (Multi-domain):\n",
      "  Count: 500\n",
      "  Min nodes: 66\n",
      "  Max nodes: 1523\n",
      "  Mean nodes: 396.43\n",
      "  Median nodes: 358.00\n",
      "\n",
      "f (Membrane):\n",
      "  Count: 500\n",
      "  Min nodes: 25\n",
      "  Max nodes: 746\n",
      "  Mean nodes: 184.99\n",
      "  Median nodes: 146.00\n",
      "\n",
      "g (Small proteins):\n",
      "  Count: 500\n",
      "  Min nodes: 20\n",
      "  Max nodes: 157\n",
      "  Mean nodes: 62.37\n",
      "  Median nodes: 56.00\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:04:37.802935Z",
     "start_time": "2025-02-16T22:04:37.414439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "saved_data = torch.load(processed_path, weights_only=False)\n",
    "data_list = saved_data['data_list']  # Access data_list from the dictionary\n",
    "\n",
    "# Count proteins per class and above 300 nodes\n",
    "class_counts = {}\n",
    "above_300_counts = {}\n",
    "\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in class_counts:\n",
    "        class_counts[class_label] = 0\n",
    "        above_300_counts[class_label] = 0\n",
    "\n",
    "    class_counts[class_label] += 1\n",
    "    if data.num_nodes > 300:\n",
    "        above_300_counts[class_label] += 1\n",
    "\n",
    "print(\"\\nTotal proteins per class:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"Class {cls}: {count} total, {above_300_counts[cls]} above 300 nodes ({above_300_counts[cls]/count*100:.2f}%)\")"
   ],
   "id": "e76168280612488f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total proteins per class:\n",
      "Class 0: 500 total, 71 above 300 nodes (14.20%)\n",
      "Class 1: 500 total, 50 above 300 nodes (10.00%)\n",
      "Class 2: 500 total, 131 above 300 nodes (26.20%)\n",
      "Class 3: 424 total, 42 above 300 nodes (9.91%)\n",
      "Class 4: 500 total, 328 above 300 nodes (65.60%)\n",
      "Class 5: 500 total, 81 above 300 nodes (16.20%)\n",
      "Class 6: 500 total, 0 above 300 nodes (0.00%)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:28:56.912199Z",
     "start_time": "2025-02-16T22:28:56.882103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class SparseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "\n",
    "        # First GraphSAGE layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second GraphSAGE layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third GraphSAGE layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Custom pooling implementation\n",
    "        unique_batches = torch.unique(batch)\n",
    "        pooled_features = []\n",
    "\n",
    "        for b in unique_batches:\n",
    "            mask = (batch == b)\n",
    "            graph_features = x[mask]\n",
    "            pooled_features.append(torch.mean(graph_features, dim=0))\n",
    "\n",
    "        x = torch.stack(pooled_features)\n",
    "\n",
    "        # MLP head\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_model(train_loader, val_loader, test_loader, device):\n",
    "    model = SparseGNN(num_features=28, num_classes=7).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, verbose=True)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    test_acc = 0\n",
    "    patience = 10\n",
    "    no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, 151):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = criterion(out, data.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Add gradient clipping\n",
    "                optimizer.step()\n",
    "\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += pred.eq(data.y).sum().item()\n",
    "                total += data.num_graphs\n",
    "                total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if total == 0:  # Skip epoch if all batches failed\n",
    "            continue\n",
    "\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "        val_acc = test_model(model, val_loader, device)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = test_model(model, test_loader, device)\n",
    "            best_model_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, '\n",
    "              f'Test Acc: {test_acc:.4f}, '\n",
    "              f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, best_val_acc, test_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(model, loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for data in loader:\n",
    "        try:\n",
    "            data = data.to(device)\n",
    "            pred = model(data).max(dim=1)[1]\n",
    "            total_correct += pred.eq(data.y).sum().item()\n",
    "            total_examples += data.num_graphs\n",
    "        except Exception as e:\n",
    "            print(f\"Error in testing batch: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if total_examples == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return total_correct / total_examples\n",
    "\n",
    "def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size=8):\n",
    "    # Custom collate function to handle varying batch sizes\n",
    "    def collate_fn(data_list):\n",
    "        return Batch.from_data_list(data_list)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True,\n",
    "                              collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             drop_last=True,\n",
    "                             collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "id": "738471d4eece3147",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:29:39.731661Z",
     "start_time": "2025-02-16T22:29:14.925433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "import os\n",
    "\n",
    "# Load the dataset and create splits\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "saved_data = torch.load(processed_path, weights_only=False)\n",
    "data_list = saved_data['data_list']\n",
    "\n",
    "# Create indices for random split\n",
    "n = len(data_list)\n",
    "indices = torch.randperm(n)\n",
    "train_size = int(0.8 * n)\n",
    "val_size = int(0.1 * n)\n",
    "\n",
    "# Split indices\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "# Create dataset splits\n",
    "train_dataset = [data_list[i] for i in train_indices]\n",
    "val_dataset = [data_list[i] for i in val_indices]\n",
    "test_dataset = [data_list[i] for i in test_indices]\n",
    "\n",
    "# Create data loaders with proper batch handling\n",
    "batch_size = 8  # Reduced batch size for better stability\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else\n",
    "                      'mps' if torch.backends.mps.is_available() else\n",
    "                      'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of validation graphs: {len(val_dataset)}\")\n",
    "print(f\"Number of test graphs: {len(test_dataset)}\")\n",
    "\n",
    "# Train the model\n",
    "model, best_val_acc, test_acc = train_model(train_loader, val_loader, test_loader, device)\n",
    "\n",
    "print(f'\\nFinal results:')\n",
    "print(f'Best validation accuracy: {best_val_acc:.4f}')\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), 'best_model.pt')"
   ],
   "id": "e4d1fde02103aab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Number of training graphs: 2739\n",
      "Number of validation graphs: 342\n",
      "Number of test graphs: 343\n",
      "Error in batch: The shape of the mask [1491] at index 0 does not match the shape of the indexed tensor [1490, 64] at index 0\n",
      "Error in batch: The shape of the mask [1378] at index 0 does not match the shape of the indexed tensor [1377, 64] at index 0\n",
      "Error in batch: The shape of the mask [1003] at index 0 does not match the shape of the indexed tensor [1002, 64] at index 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[84], line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of test graphs: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 45\u001B[0m model, best_val_acc, test_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFinal results:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBest validation accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_val_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[83], line 87\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(train_loader, val_loader, test_loader, device)\u001B[0m\n\u001B[1;32m     85\u001B[0m out \u001B[38;5;241m=\u001B[39m model(data)\n\u001B[1;32m     86\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(out, data\u001B[38;5;241m.\u001B[39my)\n\u001B[0;32m---> 87\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m)  \u001B[38;5;66;03m# Add gradient clipping\u001B[39;00m\n\u001B[1;32m     89\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b7ca5449068203c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
