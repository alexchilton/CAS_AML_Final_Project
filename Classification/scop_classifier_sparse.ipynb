{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T22:35:26.461329Z",
     "start_time": "2025-02-16T22:35:26.397086Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv  # Changed from DenseSAGEConv\n",
    "from torch_geometric.nn import global_mean_pool  # For sparse pooling\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define data directory\n",
    "data_dir = 'data/SCOP'  # Base directory for SCOP data\n",
    "\n",
    "# Rest of the model code remains the same..."
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:36:06.902950Z",
     "start_time": "2025-02-16T22:36:06.837236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SparseSCOPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        # Set up the root path first\n",
    "        self.root = root\n",
    "\n",
    "        # Now we can set up the class_info_path\n",
    "        self.class_info_path = os.path.join(root, 'raw/class_info.csv')\n",
    "\n",
    "        # Dictionary to map SCOP classes to indices\n",
    "        self.class_mapping = {\n",
    "            'a': 0,  # All-alpha\n",
    "            'b': 1,  # All-beta\n",
    "            'c': 2,  # Alpha/beta\n",
    "            'd': 3,  # Alpha+beta\n",
    "            'e': 4,  # Multi-domain\n",
    "            'f': 5,  # Membrane\n",
    "            'g': 6   # Small proteins\n",
    "        }\n",
    "\n",
    "        # Dictionary to map amino acids to indices\n",
    "        self.amino_acids = {\n",
    "            'ALA': 0, 'ARG': 1, 'ASN': 2, 'ASP': 3, 'CYS': 4,\n",
    "            'GLN': 5, 'GLU': 6, 'GLY': 7, 'HIS': 8, 'ILE': 9,\n",
    "            'LEU': 10, 'LYS': 11, 'MET': 12, 'PHE': 13, 'PRO': 14,\n",
    "            'SER': 15, 'THR': 16, 'TRP': 17, 'TYR': 18, 'VAL': 19,\n",
    "            'UNK': 20  # Unknown amino acid\n",
    "        }\n",
    "\n",
    "        # Load class information\n",
    "        if os.path.exists(self.class_info_path):\n",
    "            self.class_info = pd.read_csv(self.class_info_path)\n",
    "            print(f\"Found class info file with {len(self.class_info)} entries\")\n",
    "        else:\n",
    "            print(f\"Warning: class_info.csv not found at {self.class_info_path}\")\n",
    "            self.class_info = None\n",
    "\n",
    "        # Initialize the base class last\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def raw_file_names(self):\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"List of raw file names in the dataset.\"\"\"\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"List of processed file names in the dataset.\"\"\"\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        pass  # We already have the files\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Process the raw data into the internal format with comprehensive logging.\"\"\"\n",
    "        data_list = []\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        skipped_reasons = {}\n",
    "\n",
    "        # Log total number of entries in class_info\n",
    "        print(f\"Total entries in class_info: {len(self.class_info)}\")\n",
    "\n",
    "        # Verify raw directory contents\n",
    "        raw_dir = os.path.join(self.root, 'raw')\n",
    "        raw_files = [f for f in os.listdir(raw_dir) if f.endswith('.pdb')]\n",
    "        print(f\"Total PDB files in raw directory: {len(raw_files)}\")\n",
    "\n",
    "        for idx, row in self.class_info.iterrows():\n",
    "            pdb_id = str(row['scop_id'])\n",
    "\n",
    "            try:\n",
    "                # Validate class mapping\n",
    "                class_label = row['class']\n",
    "                if class_label not in self.class_mapping:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = f\"Invalid class: {class_label}\"\n",
    "                    continue\n",
    "\n",
    "                class_label = self.class_mapping[class_label]\n",
    "\n",
    "                # Load structure\n",
    "                pdb_file = f\"{pdb_id}.pdb\"\n",
    "                pdb_path = os.path.join(raw_dir, pdb_file)\n",
    "\n",
    "                # Check if file exists\n",
    "                if not os.path.exists(pdb_path):\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"PDB file not found\"\n",
    "                    continue\n",
    "\n",
    "                structure = parser.get_structure('protein', pdb_path)\n",
    "                model = structure[0]\n",
    "\n",
    "                # Get residues and create features\n",
    "                residues = list(model.get_residues())\n",
    "\n",
    "                # Skip if too few or too many residues\n",
    "                if len(residues) < 10 or len(residues) > 1400:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = f\"Invalid residue count: {len(residues)}\"\n",
    "                    continue\n",
    "\n",
    "                # Create node features\n",
    "                node_features = []\n",
    "                for residue in residues:\n",
    "                    features = self._get_residue_features(residue)\n",
    "                    node_features.append(features)\n",
    "\n",
    "                # Create edges with 5Å cutoff\n",
    "                edges = []\n",
    "                for i in range(len(residues)):\n",
    "                    for j in range(i+1, len(residues)):\n",
    "                        if 'CA' in residues[i] and 'CA' in residues[j]:\n",
    "                            ca_i = residues[i]['CA'].get_coord()\n",
    "                            ca_j = residues[j]['CA'].get_coord()\n",
    "                            dist = np.linalg.norm(ca_i - ca_j)\n",
    "                            if dist < 5.0:  # 5Å cutoff\n",
    "                                edges.append([i, j])\n",
    "                                edges.append([j, i])  # Add both directions\n",
    "\n",
    "                if len(edges) == 0:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"No edges found\"\n",
    "                    continue\n",
    "\n",
    "                # Create PyG Data object\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "                x = torch.tensor(node_features, dtype=torch.float)\n",
    "                y = torch.tensor([class_label], dtype=torch.long)\n",
    "\n",
    "                data = Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    y=y,\n",
    "                    num_nodes=len(residues)\n",
    "                )\n",
    "\n",
    "                # Additional filtering if needed\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"Failed pre-filter\"\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                data_list.append(data)\n",
    "                processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                skipped_count += 1\n",
    "                skipped_reasons[pdb_id] = f\"Processing error: {str(e)}\"\n",
    "                continue\n",
    "\n",
    "        # Detailed logging of skipped reasons\n",
    "        print(f\"\\nProcessing Summary:\")\n",
    "        print(f\"Total processed: {processed_count}\")\n",
    "        print(f\"Total skipped: {skipped_count}\")\n",
    "        print(\"\\nSkipped Reasons:\")\n",
    "        for reason, count in Counter(skipped_reasons.values()).most_common():\n",
    "            print(f\"{reason}: {count}\")\n",
    "\n",
    "        # Optional: Print some skipped PDB IDs for investigation\n",
    "        print(\"\\nSample of skipped PDB IDs:\")\n",
    "        for reason, pdb_ids in groupby(sorted(skipped_reasons.items(), key=lambda x: x[1]), key=lambda x: x[1]):\n",
    "            print(f\"{reason}: {list(pdb_ids)[:5]}\")\n",
    "\n",
    "        if len(data_list) == 0:\n",
    "            raise RuntimeError(\"No data was successfully processed!\")\n",
    "\n",
    "        # Save processed data\n",
    "        torch.save(data_list, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Get a specific graph from the dataset.\"\"\"\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "            if not os.path.exists(processed_path):\n",
    "                print(\"Warning: Processed data file not found, running processing...\")\n",
    "                self.process()\n",
    "            # Add weights_only=False to allow loading PyG Data objects\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "        return self._data_list[idx]\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"Return the number of graphs in the dataset.\"\"\"\n",
    "        processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "        if not os.path.exists(processed_path):\n",
    "            print(\"Warning: Processed data file not found, running processing...\")\n",
    "            self.process()\n",
    "\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            # Add weights_only=False here as well\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "        print(f\"Actual number of processed samples: {len(self._data_list)}\")\n",
    "        return len(self._data_list)\n",
    "\n",
    "    def _get_residue_features(self, residue):\n",
    "        \"\"\"Create feature vector for a residue.\"\"\"\n",
    "        # One-hot encode amino acid type\n",
    "        aa_features = np.zeros(21)  # 20 standard amino acids + UNK\n",
    "        aa_name = residue.get_resname()\n",
    "        if aa_name in self.amino_acids:\n",
    "            aa_features[self.amino_acids[aa_name]] = 1\n",
    "        else:\n",
    "            aa_features[self.amino_acids['UNK']] = 1\n",
    "\n",
    "        # Get CA atom coordinates\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            coords = ca_atom.get_coord()\n",
    "        except:\n",
    "            coords = np.zeros(3)\n",
    "\n",
    "        # Combine features\n",
    "        features = np.concatenate([\n",
    "            aa_features,  # Amino acid identity (21)\n",
    "            coords,      # 3D coordinates (3)\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Return the number of classes in the dataset.\"\"\"\n",
    "        return len(self.class_mapping)\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        \"\"\"Return the number of node features.\"\"\"\n",
    "        return 24  # 21 for amino acids + 3 for coordinates"
   ],
   "id": "8eee4b4a0d18bcc8",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:35:58.531804Z",
     "start_time": "2025-02-16T22:35:58.409874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "\n",
    "NUM_CLASSES = 7  # SCOP main classes\n",
    "# First, let's check protein sizes\n",
    "# First, analyze protein sizes\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "sizes = [data.num_nodes for data in data_list]\n",
    "\n",
    "print(f\"Protein size statistics:\")\n",
    "print(f\"Min size: {min(sizes)}\")\n",
    "print(f\"Max size: {max(sizes)}\")\n",
    "print(f\"Mean size: {sum(sizes)/len(sizes):.1f}\")\n",
    "print(f\"Median size: {sorted(sizes)[len(sizes)//2]}\")\n",
    "print(f\"Number of proteins > 150 residues: {sum(1 for s in sizes if s > 150)}\")\n",
    "\n",
    "# Set max_nodes to 500 since we have proteins up to 1381 residues\n",
    "max_nodes = 1400\n",
    "\n",
    "# Create dataset\n",
    "#dataset = SparseSCOPDataset(\n",
    "#    root=data_dir,\n",
    "#    pre_filter=lambda data: data.num_nodes <= max_nodes\n",
    "#)\n",
    "\n",
    "#dataset = SparseSCOPDataset(root=data_dir)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n"
   ],
   "id": "81c665801b8fe044",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/SCOP/processed/data.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[89], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# First, let's check protein sizes\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# First, analyze protein sizes\u001B[39;00m\n\u001B[1;32m     14\u001B[0m processed_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed/data.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m data_list \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocessed_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m sizes \u001B[38;5;241m=\u001B[39m [data\u001B[38;5;241m.\u001B[39mnum_nodes \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m data_list]\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProtein size statistics:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/serialization.py:1425\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m   1423\u001B[0m     pickle_load_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1425\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m   1426\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m   1427\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m   1428\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m   1429\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m   1430\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/serialization.py:751\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    750\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 751\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    752\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    753\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/serialization.py:732\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    731\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 732\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/SCOP/processed/data.pt'"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:34:32.260464Z",
     "start_time": "2025-02-16T19:34:32.194108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming your dataset is loaded\n",
    "filtered_dataset = [data for data in dataset if data.num_nodes < 300]\n",
    "\n",
    "# If you want to create a new dataset object\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class FilteredSCOPDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.data_list = [data for data in original_dataset if data.num_nodes < 300]\n",
    "        super().__init__(original_dataset.root)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create the filtered dataset\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Verify the filtering\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Optional: Check distribution across classes\n",
    "class_distribution = {}\n",
    "for data in filtered_dataset:\n",
    "    class_label = data.y.item()\n",
    "    class_distribution[class_label] = class_distribution.get(class_label, 0) + 1\n",
    "\n",
    "print(\"\\nClass distribution in filtered dataset:\")\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"Class {cls}: {count} proteins\")"
   ],
   "id": "755cc2d19aa5e717",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 3420\n",
      "Filtered dataset size: 2717\n",
      "\n",
      "Class distribution in filtered dataset:\n",
      "Class 1: 449 proteins\n",
      "Class 2: 368 proteins\n",
      "Class 0: 428 proteins\n",
      "Class 6: 500 proteins\n",
      "Class 5: 418 proteins\n",
      "Class 3: 382 proteins\n",
      "Class 4: 172 proteins\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:39:09.049191Z",
     "start_time": "2025-02-16T22:36:24.592830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the root directory\n",
    "data_dir = 'data/SCOP'  # Adjust this to your actual data directory path\n",
    "\n",
    "# Create the dataset (this should trigger the process method)\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "# Create the dataset instance\n",
    "\n",
    "# Explicitly call the process method\n",
    "processed_data = dataset.process()"
   ],
   "id": "326dab61158a6c1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class info file with 3500 entries\n",
      "Total entries in class_info: 3500\n",
      "Total PDB files in raw directory: 3424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[91], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/SCOP\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Adjust this to your actual data directory path\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Create the dataset (this should trigger the process method)\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mSparseSCOPDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Create the dataset instance\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Explicitly call the process method\u001B[39;00m\n\u001B[1;32m      9\u001B[0m processed_data \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mprocess()\n",
      "Cell \u001B[0;32mIn[90], line 50\u001B[0m, in \u001B[0;36mSparseSCOPDataset.__init__\u001B[0;34m(self, root, transform, pre_transform, pre_filter)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# Initialize the base class last\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_transform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_filter\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch_geometric/data/dataset.py:115\u001B[0m, in \u001B[0;36mDataset.__init__\u001B[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001B[0m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download()\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_process:\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch_geometric/data/dataset.py:262\u001B[0m, in \u001B[0;36mDataset._process\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mProcessing...\u001B[39m\u001B[38;5;124m'\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mstderr)\n\u001B[1;32m    261\u001B[0m fs\u001B[38;5;241m.\u001B[39mmakedirs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_dir, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 262\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    264\u001B[0m path \u001B[38;5;241m=\u001B[39m osp\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_transform.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    265\u001B[0m fs\u001B[38;5;241m.\u001B[39mtorch_save(_repr(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_transform), path)\n",
      "Cell \u001B[0;32mIn[90], line 147\u001B[0m, in \u001B[0;36mSparseSCOPDataset.process\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    145\u001B[0m ca_i \u001B[38;5;241m=\u001B[39m residues[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCA\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mget_coord()\n\u001B[1;32m    146\u001B[0m ca_j \u001B[38;5;241m=\u001B[39m residues[j][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCA\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mget_coord()\n\u001B[0;32m--> 147\u001B[0m dist \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mca_i\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mca_j\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dist \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m5.0\u001B[39m:  \u001B[38;5;66;03m# 5Å cutoff\u001B[39;00m\n\u001B[1;32m    149\u001B[0m     edges\u001B[38;5;241m.\u001B[39mappend([i, j])\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/numpy/linalg/_linalg.py:2745\u001B[0m, in \u001B[0;36mnorm\u001B[0;34m(x, ord, axis, keepdims)\u001B[0m\n\u001B[1;32m   2743\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2744\u001B[0m     sqnorm \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdot(x)\n\u001B[0;32m-> 2745\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqnorm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m keepdims:\n\u001B[1;32m   2747\u001B[0m     ret \u001B[38;5;241m=\u001B[39m ret\u001B[38;5;241m.\u001B[39mreshape(ndim\u001B[38;5;241m*\u001B[39m[\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:35:39.969356Z",
     "start_time": "2025-02-16T19:35:39.927705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the filtered dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Shuffle and split the filtered dataset\n",
    "filtered_dataset = filtered_dataset.shuffle()\n",
    "n = (len(filtered_dataset) + 9) // 10\n",
    "test_dataset = filtered_dataset[:n]\n",
    "val_dataset = filtered_dataset[n:2 * n]\n",
    "train_dataset = filtered_dataset[2 * n:]\n",
    "\n",
    "# Create data loaders using the filtered datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"\\nFiltered Dataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# The rest of your training loop remains the same\n",
    "# You can use these loaders directly in your existing training script"
   ],
   "id": "510af8d5deb60124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Dataset splits:\n",
      "Training samples: 2173\n",
      "Validation samples: 272\n",
      "Test samples: 272\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:31:11.960641Z",
     "start_time": "2025-02-16T19:31:11.819218Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "243e6c1a459fa5a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 2736\n",
      "Validation samples: 342\n",
      "Test samples: 342\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:04:34.415188Z",
     "start_time": "2025-02-16T19:04:32.496834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you've already loaded the dataset\n",
    "# Load the processed data\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "# Reverse class mapping for readable labels\n",
    "class_mapping_reverse = {\n",
    "    0: 'a (All-alpha)',\n",
    "    1: 'b (All-beta)',\n",
    "    2: 'c (Alpha/beta)',\n",
    "    3: 'd (Alpha+beta)',\n",
    "    4: 'e (Multi-domain)',\n",
    "    5: 'f (Membrane)',\n",
    "    6: 'g (Small proteins)'\n",
    "}\n",
    "\n",
    "# Separate nodes by class\n",
    "nodes_by_class = {}\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in nodes_by_class:\n",
    "        nodes_by_class[class_label] = []\n",
    "    nodes_by_class[class_label].append(data.num_nodes)\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Box plot\n",
    "plt.boxplot([nodes_by_class[key] for key in sorted(nodes_by_class.keys())],\n",
    "            labels=[class_mapping_reverse[key] for key in sorted(nodes_by_class.keys())])\n",
    "\n",
    "plt.title('Number of Nodes per SCOP Class', fontsize=16)\n",
    "plt.xlabel('SCOP Class', fontsize=12)\n",
    "plt.ylabel('Number of Nodes (Residues)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('nodes_per_class_boxplot.png')\n",
    "plt.close()\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Node count statistics per class:\")\n",
    "for class_label, nodes in nodes_by_class.items():\n",
    "    print(f\"\\n{class_mapping_reverse[class_label]}:\")\n",
    "    print(f\"  Count: {len(nodes)}\")\n",
    "    print(f\"  Min nodes: {min(nodes)}\")\n",
    "    print(f\"  Max nodes: {max(nodes)}\")\n",
    "    print(f\"  Mean nodes: {np.mean(nodes):.2f}\")\n",
    "    print(f\"  Median nodes: {np.median(nodes):.2f}\")"
   ],
   "id": "27dc0cc370f2c3dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node count statistics per class:\n",
      "\n",
      "a (All-alpha):\n",
      "  Count: 500\n",
      "  Min nodes: 39\n",
      "  Max nodes: 656\n",
      "  Mean nodes: 172.21\n",
      "  Median nodes: 142.50\n",
      "\n",
      "b (All-beta):\n",
      "  Count: 500\n",
      "  Min nodes: 53\n",
      "  Max nodes: 548\n",
      "  Mean nodes: 170.79\n",
      "  Median nodes: 135.00\n",
      "\n",
      "c (Alpha/beta):\n",
      "  Count: 500\n",
      "  Min nodes: 44\n",
      "  Max nodes: 815\n",
      "  Mean nodes: 255.96\n",
      "  Median nodes: 247.00\n",
      "\n",
      "d (Alpha+beta):\n",
      "  Count: 424\n",
      "  Min nodes: 56\n",
      "  Max nodes: 640\n",
      "  Mean nodes: 183.87\n",
      "  Median nodes: 164.00\n",
      "\n",
      "e (Multi-domain):\n",
      "  Count: 496\n",
      "  Min nodes: 66\n",
      "  Max nodes: 1395\n",
      "  Mean nodes: 387.85\n",
      "  Median nodes: 358.00\n",
      "\n",
      "f (Membrane):\n",
      "  Count: 500\n",
      "  Min nodes: 25\n",
      "  Max nodes: 746\n",
      "  Mean nodes: 184.99\n",
      "  Median nodes: 146.00\n",
      "\n",
      "g (Small proteins):\n",
      "  Count: 500\n",
      "  Min nodes: 20\n",
      "  Max nodes: 157\n",
      "  Mean nodes: 62.37\n",
      "  Median nodes: 56.00\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:08:17.211115Z",
     "start_time": "2025-02-16T19:08:15.571976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "# Count proteins per class and above 300 nodes\n",
    "class_counts = {}\n",
    "above_300_counts = {}\n",
    "\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in class_counts:\n",
    "        class_counts[class_label] = 0\n",
    "        above_300_counts[class_label] = 0\n",
    "\n",
    "    class_counts[class_label] += 1\n",
    "    if data.num_nodes > 300:\n",
    "        above_300_counts[class_label] += 1\n",
    "\n",
    "print(\"Total proteins per class:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"Class {cls}: {count} total, {above_300_counts[cls]} above 300 nodes ({above_300_counts[cls]/count*100:.2f}%)\")"
   ],
   "id": "e76168280612488f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins per class:\n",
      "Class 0: 500 total, 71 above 300 nodes (14.20%)\n",
      "Class 1: 500 total, 50 above 300 nodes (10.00%)\n",
      "Class 2: 500 total, 131 above 300 nodes (26.20%)\n",
      "Class 3: 424 total, 42 above 300 nodes (9.91%)\n",
      "Class 4: 496 total, 324 above 300 nodes (65.32%)\n",
      "Class 5: 500 total, 81 above 300 nodes (16.20%)\n",
      "Class 6: 500 total, 0 above 300 nodes (0.00%)\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T18:51:32.370651Z",
     "start_time": "2025-02-16T18:51:32.357729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from math import ceil\n",
    "\n",
    "class SparseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # GNN layers with sparse representation\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First convolution layer\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = F.relu(h1)\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Second convolution layer\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = F.relu(h2)\n",
    "        h2 = self.dropout(h2)\n",
    "\n",
    "        # Third convolution layer\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "        h3 = self.bn3(h3)\n",
    "        h3 = F.relu(h3)\n",
    "        h3 = self.dropout(h3)\n",
    "\n",
    "        # Global mean pooling\n",
    "        out = global_mean_pool(h3, batch)\n",
    "\n",
    "        # MLP head\n",
    "        out = self.lin1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a single GNN model for classification\n",
    "        self.gnn = SparseGNN(\n",
    "            num_features=dataset.num_features,\n",
    "            hidden_dim=64,\n",
    "            num_classes=dataset.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Single forward pass through the GNN\n",
    "        x = self.gnn(x, edge_index, batch)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"Determine the best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    \"\"\"Training function for the model.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    \"\"\"Evaluation function for the model.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def main(dataset, train_loader, val_loader, test_loader):\n",
    "    \"\"\"Main training and evaluation loop.\"\"\"\n",
    "    device = setup_device()\n",
    "    model = Net(dataset).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for epoch in range(1, 151):\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        val_acc = test(model, val_loader, device)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = test(model, test_loader, device)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return model, best_val_acc, test_acc\n",
    "\n",
    "# Note: This function would be called after setting up the dataset, loaders, etc.\n",
    "# main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "738471d4eece3147",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:04:12.038388Z",
     "start_time": "2025-02-16T18:51:42.864193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data loaders (as you had before)\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Use DataLoader for sparse graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Train the model\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "e4d1fde02103aab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.8976, Val Acc: 0.1930, Test Acc: 0.2515\n",
      "Epoch: 002, Train Loss: 1.8763, Val Acc: 0.2281, Test Acc: 0.2485\n",
      "Epoch: 003, Train Loss: 1.8571, Val Acc: 0.2164, Test Acc: 0.2485\n",
      "Epoch: 004, Train Loss: 1.8424, Val Acc: 0.2047, Test Acc: 0.2485\n",
      "Epoch: 005, Train Loss: 1.8294, Val Acc: 0.2193, Test Acc: 0.2485\n",
      "Epoch: 006, Train Loss: 1.8179, Val Acc: 0.2105, Test Acc: 0.2485\n",
      "Epoch: 007, Train Loss: 1.8021, Val Acc: 0.2368, Test Acc: 0.2719\n",
      "Epoch: 008, Train Loss: 1.7708, Val Acc: 0.2719, Test Acc: 0.3099\n",
      "Epoch: 009, Train Loss: 1.6554, Val Acc: 0.3392, Test Acc: 0.3450\n",
      "Epoch: 010, Train Loss: 1.5773, Val Acc: 0.3187, Test Acc: 0.3450\n",
      "Epoch: 011, Train Loss: 1.5352, Val Acc: 0.3480, Test Acc: 0.3392\n",
      "Epoch: 012, Train Loss: 1.5381, Val Acc: 0.3070, Test Acc: 0.3392\n",
      "Epoch: 013, Train Loss: 1.5107, Val Acc: 0.3509, Test Acc: 0.3567\n",
      "Epoch: 014, Train Loss: 1.4955, Val Acc: 0.3363, Test Acc: 0.3567\n",
      "Epoch: 015, Train Loss: 1.4911, Val Acc: 0.3275, Test Acc: 0.3567\n",
      "Epoch: 016, Train Loss: 1.4781, Val Acc: 0.3392, Test Acc: 0.3567\n",
      "Epoch: 017, Train Loss: 1.4718, Val Acc: 0.3480, Test Acc: 0.3567\n",
      "Epoch: 018, Train Loss: 1.4529, Val Acc: 0.3596, Test Acc: 0.3655\n",
      "Epoch: 019, Train Loss: 1.4404, Val Acc: 0.3538, Test Acc: 0.3655\n",
      "Epoch: 020, Train Loss: 1.4109, Val Acc: 0.3743, Test Acc: 0.3538\n",
      "Epoch: 021, Train Loss: 1.4134, Val Acc: 0.3538, Test Acc: 0.3538\n",
      "Epoch: 022, Train Loss: 1.4245, Val Acc: 0.3655, Test Acc: 0.3538\n",
      "Epoch: 023, Train Loss: 1.3880, Val Acc: 0.3684, Test Acc: 0.3538\n",
      "Epoch: 024, Train Loss: 1.3775, Val Acc: 0.3801, Test Acc: 0.3947\n",
      "Epoch: 025, Train Loss: 1.3611, Val Acc: 0.3713, Test Acc: 0.3947\n",
      "Epoch: 026, Train Loss: 1.3363, Val Acc: 0.3772, Test Acc: 0.3947\n",
      "Epoch: 027, Train Loss: 1.3309, Val Acc: 0.3918, Test Acc: 0.4064\n",
      "Epoch: 028, Train Loss: 1.3281, Val Acc: 0.4094, Test Acc: 0.4181\n",
      "Epoch: 029, Train Loss: 1.3004, Val Acc: 0.4064, Test Acc: 0.4181\n",
      "Epoch: 030, Train Loss: 1.2795, Val Acc: 0.4269, Test Acc: 0.4240\n",
      "Epoch: 031, Train Loss: 1.2683, Val Acc: 0.3977, Test Acc: 0.4240\n",
      "Epoch: 032, Train Loss: 1.2601, Val Acc: 0.4386, Test Acc: 0.4474\n",
      "Epoch: 033, Train Loss: 1.2560, Val Acc: 0.4561, Test Acc: 0.4298\n",
      "Epoch: 034, Train Loss: 1.2354, Val Acc: 0.4386, Test Acc: 0.4298\n",
      "Epoch: 035, Train Loss: 1.2361, Val Acc: 0.4474, Test Acc: 0.4298\n",
      "Epoch: 036, Train Loss: 1.2107, Val Acc: 0.4357, Test Acc: 0.4298\n",
      "Epoch: 037, Train Loss: 1.1834, Val Acc: 0.4591, Test Acc: 0.4561\n",
      "Epoch: 038, Train Loss: 1.1855, Val Acc: 0.4561, Test Acc: 0.4561\n",
      "Epoch: 039, Train Loss: 1.1910, Val Acc: 0.4444, Test Acc: 0.4561\n",
      "Epoch: 040, Train Loss: 1.1552, Val Acc: 0.4766, Test Acc: 0.4357\n",
      "Epoch: 041, Train Loss: 1.1459, Val Acc: 0.4386, Test Acc: 0.4357\n",
      "Epoch: 042, Train Loss: 1.1388, Val Acc: 0.4474, Test Acc: 0.4357\n",
      "Epoch: 043, Train Loss: 1.1491, Val Acc: 0.4649, Test Acc: 0.4357\n",
      "Epoch: 044, Train Loss: 1.1359, Val Acc: 0.4444, Test Acc: 0.4357\n",
      "Epoch: 045, Train Loss: 1.1030, Val Acc: 0.4532, Test Acc: 0.4357\n",
      "Epoch: 046, Train Loss: 1.0972, Val Acc: 0.4883, Test Acc: 0.4386\n",
      "Epoch: 047, Train Loss: 1.0849, Val Acc: 0.4649, Test Acc: 0.4386\n",
      "Epoch: 048, Train Loss: 1.0869, Val Acc: 0.4561, Test Acc: 0.4386\n",
      "Epoch: 049, Train Loss: 1.0627, Val Acc: 0.4620, Test Acc: 0.4386\n",
      "Epoch: 050, Train Loss: 1.0455, Val Acc: 0.4708, Test Acc: 0.4386\n",
      "Epoch: 051, Train Loss: 1.0543, Val Acc: 0.4649, Test Acc: 0.4386\n",
      "Epoch: 052, Train Loss: 1.0361, Val Acc: 0.4620, Test Acc: 0.4386\n",
      "Epoch: 053, Train Loss: 1.0262, Val Acc: 0.4649, Test Acc: 0.4386\n",
      "Epoch: 054, Train Loss: 1.0140, Val Acc: 0.4795, Test Acc: 0.4386\n",
      "Epoch: 055, Train Loss: 0.9968, Val Acc: 0.4795, Test Acc: 0.4386\n",
      "Epoch: 056, Train Loss: 1.0052, Val Acc: 0.4561, Test Acc: 0.4386\n",
      "Epoch: 057, Train Loss: 1.0071, Val Acc: 0.4678, Test Acc: 0.4386\n",
      "Epoch: 058, Train Loss: 0.9939, Val Acc: 0.4795, Test Acc: 0.4386\n",
      "Epoch: 059, Train Loss: 0.9603, Val Acc: 0.4532, Test Acc: 0.4386\n",
      "Epoch: 060, Train Loss: 0.9506, Val Acc: 0.4825, Test Acc: 0.4386\n",
      "Epoch: 061, Train Loss: 0.9563, Val Acc: 0.4766, Test Acc: 0.4386\n",
      "Epoch: 062, Train Loss: 0.9373, Val Acc: 0.4766, Test Acc: 0.4386\n",
      "Epoch: 063, Train Loss: 0.9330, Val Acc: 0.4854, Test Acc: 0.4386\n",
      "Epoch: 064, Train Loss: 0.9462, Val Acc: 0.4766, Test Acc: 0.4386\n",
      "Epoch: 065, Train Loss: 0.9269, Val Acc: 0.4942, Test Acc: 0.5000\n",
      "Epoch: 066, Train Loss: 0.9252, Val Acc: 0.5175, Test Acc: 0.5088\n",
      "Epoch: 067, Train Loss: 0.8887, Val Acc: 0.5029, Test Acc: 0.5088\n",
      "Epoch: 068, Train Loss: 0.9041, Val Acc: 0.5175, Test Acc: 0.5088\n",
      "Epoch: 069, Train Loss: 0.9152, Val Acc: 0.5088, Test Acc: 0.5088\n",
      "Epoch: 070, Train Loss: 0.8976, Val Acc: 0.5000, Test Acc: 0.5088\n",
      "Epoch: 071, Train Loss: 0.8819, Val Acc: 0.5117, Test Acc: 0.5088\n",
      "Epoch: 072, Train Loss: 0.8860, Val Acc: 0.4766, Test Acc: 0.5088\n",
      "Epoch: 073, Train Loss: 0.8690, Val Acc: 0.5234, Test Acc: 0.5058\n",
      "Epoch: 074, Train Loss: 0.8568, Val Acc: 0.5205, Test Acc: 0.5058\n",
      "Epoch: 075, Train Loss: 0.8625, Val Acc: 0.5234, Test Acc: 0.5058\n",
      "Epoch: 076, Train Loss: 0.8372, Val Acc: 0.5175, Test Acc: 0.5058\n",
      "Epoch: 077, Train Loss: 0.8650, Val Acc: 0.4883, Test Acc: 0.5058\n",
      "Epoch: 078, Train Loss: 0.8330, Val Acc: 0.4971, Test Acc: 0.5058\n",
      "Epoch: 079, Train Loss: 0.8252, Val Acc: 0.5175, Test Acc: 0.5058\n",
      "Epoch: 080, Train Loss: 0.8303, Val Acc: 0.5000, Test Acc: 0.5058\n",
      "Epoch: 081, Train Loss: 0.8189, Val Acc: 0.5146, Test Acc: 0.5058\n",
      "Epoch: 082, Train Loss: 0.8022, Val Acc: 0.5175, Test Acc: 0.5058\n",
      "Epoch: 083, Train Loss: 0.8069, Val Acc: 0.4942, Test Acc: 0.5058\n",
      "Epoch: 084, Train Loss: 0.8038, Val Acc: 0.4912, Test Acc: 0.5058\n",
      "Epoch: 085, Train Loss: 0.8381, Val Acc: 0.4971, Test Acc: 0.5058\n",
      "Epoch: 086, Train Loss: 0.8077, Val Acc: 0.5351, Test Acc: 0.5117\n",
      "Epoch: 087, Train Loss: 0.8016, Val Acc: 0.5292, Test Acc: 0.5117\n",
      "Epoch: 088, Train Loss: 0.7858, Val Acc: 0.4737, Test Acc: 0.5117\n",
      "Epoch: 089, Train Loss: 0.7930, Val Acc: 0.4883, Test Acc: 0.5117\n",
      "Epoch: 090, Train Loss: 0.7869, Val Acc: 0.5058, Test Acc: 0.5117\n",
      "Epoch: 091, Train Loss: 0.7465, Val Acc: 0.4883, Test Acc: 0.5117\n",
      "Epoch: 092, Train Loss: 0.7673, Val Acc: 0.4942, Test Acc: 0.5117\n",
      "Epoch: 093, Train Loss: 0.7636, Val Acc: 0.5117, Test Acc: 0.5117\n",
      "Epoch: 094, Train Loss: 0.7701, Val Acc: 0.5088, Test Acc: 0.5117\n",
      "Epoch: 095, Train Loss: 0.7686, Val Acc: 0.4971, Test Acc: 0.5117\n",
      "Epoch: 096, Train Loss: 0.7376, Val Acc: 0.4795, Test Acc: 0.5117\n",
      "Epoch: 097, Train Loss: 0.7549, Val Acc: 0.4942, Test Acc: 0.5117\n",
      "Epoch: 098, Train Loss: 0.7283, Val Acc: 0.4942, Test Acc: 0.5117\n",
      "Epoch: 099, Train Loss: 0.7420, Val Acc: 0.4795, Test Acc: 0.5117\n",
      "Epoch: 100, Train Loss: 0.7293, Val Acc: 0.4825, Test Acc: 0.5117\n",
      "Epoch: 101, Train Loss: 0.7464, Val Acc: 0.5205, Test Acc: 0.5117\n",
      "Epoch: 102, Train Loss: 0.7219, Val Acc: 0.4766, Test Acc: 0.5117\n",
      "Epoch: 103, Train Loss: 0.7020, Val Acc: 0.4942, Test Acc: 0.5117\n",
      "Epoch: 104, Train Loss: 0.7011, Val Acc: 0.4854, Test Acc: 0.5117\n",
      "Epoch: 105, Train Loss: 0.7210, Val Acc: 0.4942, Test Acc: 0.5117\n",
      "Epoch: 106, Train Loss: 0.7252, Val Acc: 0.4971, Test Acc: 0.5117\n",
      "Epoch: 107, Train Loss: 0.7009, Val Acc: 0.4795, Test Acc: 0.5117\n",
      "Epoch: 108, Train Loss: 0.6756, Val Acc: 0.5000, Test Acc: 0.5117\n",
      "Epoch: 109, Train Loss: 0.6913, Val Acc: 0.4942, Test Acc: 0.5117\n",
      "Epoch: 110, Train Loss: 0.7124, Val Acc: 0.5088, Test Acc: 0.5117\n",
      "Epoch: 111, Train Loss: 0.6824, Val Acc: 0.4883, Test Acc: 0.5117\n",
      "Epoch: 112, Train Loss: 0.6537, Val Acc: 0.4795, Test Acc: 0.5117\n",
      "Epoch: 113, Train Loss: 0.7002, Val Acc: 0.5088, Test Acc: 0.5117\n",
      "Epoch: 114, Train Loss: 0.6550, Val Acc: 0.4795, Test Acc: 0.5117\n",
      "Epoch: 115, Train Loss: 0.6744, Val Acc: 0.4825, Test Acc: 0.5117\n",
      "Epoch: 116, Train Loss: 0.6687, Val Acc: 0.4854, Test Acc: 0.5117\n",
      "Epoch: 117, Train Loss: 0.6507, Val Acc: 0.4912, Test Acc: 0.5117\n",
      "Epoch: 118, Train Loss: 0.6632, Val Acc: 0.5146, Test Acc: 0.5117\n",
      "Epoch: 119, Train Loss: 0.6334, Val Acc: 0.4912, Test Acc: 0.5117\n",
      "Epoch: 120, Train Loss: 0.6513, Val Acc: 0.4912, Test Acc: 0.5117\n",
      "Epoch: 121, Train Loss: 0.6451, Val Acc: 0.4649, Test Acc: 0.5117\n",
      "Epoch: 122, Train Loss: 0.6448, Val Acc: 0.5029, Test Acc: 0.5117\n",
      "Epoch: 123, Train Loss: 0.6263, Val Acc: 0.5029, Test Acc: 0.5117\n",
      "Epoch: 124, Train Loss: 0.6210, Val Acc: 0.4883, Test Acc: 0.5117\n",
      "Epoch: 125, Train Loss: 0.6099, Val Acc: 0.4591, Test Acc: 0.5117\n",
      "Epoch: 126, Train Loss: 0.6281, Val Acc: 0.4708, Test Acc: 0.5117\n",
      "Epoch: 127, Train Loss: 0.6501, Val Acc: 0.5117, Test Acc: 0.5117\n",
      "Epoch: 128, Train Loss: 0.5959, Val Acc: 0.5058, Test Acc: 0.5117\n",
      "Epoch: 129, Train Loss: 0.5979, Val Acc: 0.4854, Test Acc: 0.5117\n",
      "Epoch: 130, Train Loss: 0.5962, Val Acc: 0.5029, Test Acc: 0.5117\n",
      "Epoch: 131, Train Loss: 0.5975, Val Acc: 0.4942, Test Acc: 0.5117\n",
      "Epoch: 132, Train Loss: 0.6004, Val Acc: 0.5058, Test Acc: 0.5117\n",
      "Epoch: 133, Train Loss: 0.6166, Val Acc: 0.4825, Test Acc: 0.5117\n",
      "Epoch: 134, Train Loss: 0.6422, Val Acc: 0.5117, Test Acc: 0.5117\n",
      "Epoch: 135, Train Loss: 0.5871, Val Acc: 0.5263, Test Acc: 0.5117\n",
      "Epoch: 136, Train Loss: 0.5971, Val Acc: 0.5029, Test Acc: 0.5117\n",
      "Epoch: 137, Train Loss: 0.5865, Val Acc: 0.5000, Test Acc: 0.5117\n",
      "Epoch: 138, Train Loss: 0.5758, Val Acc: 0.5058, Test Acc: 0.5117\n",
      "Epoch: 139, Train Loss: 0.5811, Val Acc: 0.4912, Test Acc: 0.5117\n",
      "Epoch: 140, Train Loss: 0.5451, Val Acc: 0.4883, Test Acc: 0.5117\n",
      "Epoch: 141, Train Loss: 0.6355, Val Acc: 0.4854, Test Acc: 0.5117\n",
      "Epoch: 142, Train Loss: 0.5734, Val Acc: 0.5058, Test Acc: 0.5117\n",
      "Epoch: 143, Train Loss: 0.5444, Val Acc: 0.4795, Test Acc: 0.5117\n",
      "Epoch: 144, Train Loss: 0.5585, Val Acc: 0.4942, Test Acc: 0.5117\n",
      "Epoch: 145, Train Loss: 0.5505, Val Acc: 0.4912, Test Acc: 0.5117\n",
      "Epoch: 146, Train Loss: 0.5460, Val Acc: 0.5146, Test Acc: 0.5117\n",
      "Epoch: 147, Train Loss: 0.5423, Val Acc: 0.5058, Test Acc: 0.5117\n",
      "Epoch: 148, Train Loss: 0.5474, Val Acc: 0.4971, Test Acc: 0.5117\n",
      "Epoch: 149, Train Loss: 0.5828, Val Acc: 0.5000, Test Acc: 0.5117\n",
      "Epoch: 150, Train Loss: 0.5538, Val Acc: 0.4854, Test Acc: 0.5117\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:48:58.737307Z",
     "start_time": "2025-02-16T19:37:25.805222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Replace your existing DataLoader with this\n",
    "train_loader = DataLoader(train_dataset,  batch_size=20)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=20)\n",
    "test_loader = DataLoader(test_dataset,  batch_size=20)\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)\n"
   ],
   "id": "db76b9390eadc048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 2736\n",
      "Validation samples: 342\n",
      "Test samples: 342\n",
      "Epoch: 001, Train Loss: 1.9036, Val Acc: 0.2135, Test Acc: 0.2164\n",
      "Epoch: 002, Train Loss: 1.8695, Val Acc: 0.2047, Test Acc: 0.2164\n",
      "Epoch: 003, Train Loss: 1.8478, Val Acc: 0.2251, Test Acc: 0.2485\n",
      "Epoch: 004, Train Loss: 1.8373, Val Acc: 0.2515, Test Acc: 0.2251\n",
      "Epoch: 005, Train Loss: 1.8174, Val Acc: 0.2368, Test Acc: 0.2251\n",
      "Epoch: 006, Train Loss: 1.7879, Val Acc: 0.2924, Test Acc: 0.2749\n",
      "Epoch: 007, Train Loss: 1.7467, Val Acc: 0.3304, Test Acc: 0.3041\n",
      "Epoch: 008, Train Loss: 1.6779, Val Acc: 0.3421, Test Acc: 0.3333\n",
      "Epoch: 009, Train Loss: 1.6030, Val Acc: 0.3509, Test Acc: 0.3304\n",
      "Epoch: 010, Train Loss: 1.5866, Val Acc: 0.3509, Test Acc: 0.3304\n",
      "Epoch: 011, Train Loss: 1.5378, Val Acc: 0.3626, Test Acc: 0.3626\n",
      "Epoch: 012, Train Loss: 1.5079, Val Acc: 0.4035, Test Acc: 0.4006\n",
      "Epoch: 013, Train Loss: 1.4732, Val Acc: 0.3801, Test Acc: 0.4006\n",
      "Epoch: 014, Train Loss: 1.4386, Val Acc: 0.4006, Test Acc: 0.4006\n",
      "Epoch: 015, Train Loss: 1.4277, Val Acc: 0.4240, Test Acc: 0.4006\n",
      "Epoch: 016, Train Loss: 1.4177, Val Acc: 0.4211, Test Acc: 0.4006\n",
      "Epoch: 017, Train Loss: 1.3916, Val Acc: 0.3947, Test Acc: 0.4006\n",
      "Epoch: 018, Train Loss: 1.3780, Val Acc: 0.4094, Test Acc: 0.4006\n",
      "Epoch: 019, Train Loss: 1.3587, Val Acc: 0.4357, Test Acc: 0.4444\n",
      "Epoch: 020, Train Loss: 1.3565, Val Acc: 0.4561, Test Acc: 0.4327\n",
      "Epoch: 021, Train Loss: 1.3410, Val Acc: 0.4211, Test Acc: 0.4327\n",
      "Epoch: 022, Train Loss: 1.3174, Val Acc: 0.4708, Test Acc: 0.4240\n",
      "Epoch: 023, Train Loss: 1.3072, Val Acc: 0.4737, Test Acc: 0.4386\n",
      "Epoch: 024, Train Loss: 1.3058, Val Acc: 0.4649, Test Acc: 0.4386\n",
      "Epoch: 025, Train Loss: 1.2837, Val Acc: 0.4620, Test Acc: 0.4386\n",
      "Epoch: 026, Train Loss: 1.2760, Val Acc: 0.4240, Test Acc: 0.4386\n",
      "Epoch: 027, Train Loss: 1.2704, Val Acc: 0.4825, Test Acc: 0.4444\n",
      "Epoch: 028, Train Loss: 1.2637, Val Acc: 0.4854, Test Acc: 0.4211\n",
      "Epoch: 029, Train Loss: 1.2681, Val Acc: 0.4795, Test Acc: 0.4211\n",
      "Epoch: 030, Train Loss: 1.2465, Val Acc: 0.4795, Test Acc: 0.4211\n",
      "Epoch: 031, Train Loss: 1.2396, Val Acc: 0.4912, Test Acc: 0.4678\n",
      "Epoch: 032, Train Loss: 1.2342, Val Acc: 0.4912, Test Acc: 0.4678\n",
      "Epoch: 033, Train Loss: 1.2027, Val Acc: 0.5175, Test Acc: 0.4561\n",
      "Epoch: 034, Train Loss: 1.2337, Val Acc: 0.4971, Test Acc: 0.4561\n",
      "Epoch: 035, Train Loss: 1.1982, Val Acc: 0.5146, Test Acc: 0.4561\n",
      "Epoch: 036, Train Loss: 1.1789, Val Acc: 0.4942, Test Acc: 0.4561\n",
      "Epoch: 037, Train Loss: 1.1866, Val Acc: 0.5000, Test Acc: 0.4561\n",
      "Epoch: 038, Train Loss: 1.1838, Val Acc: 0.4942, Test Acc: 0.4561\n",
      "Epoch: 039, Train Loss: 1.1495, Val Acc: 0.5205, Test Acc: 0.4883\n",
      "Epoch: 040, Train Loss: 1.1514, Val Acc: 0.4942, Test Acc: 0.4883\n",
      "Epoch: 041, Train Loss: 1.1490, Val Acc: 0.4678, Test Acc: 0.4883\n",
      "Epoch: 042, Train Loss: 1.1307, Val Acc: 0.5205, Test Acc: 0.4883\n",
      "Epoch: 043, Train Loss: 1.1339, Val Acc: 0.5146, Test Acc: 0.4883\n",
      "Epoch: 044, Train Loss: 1.1335, Val Acc: 0.4971, Test Acc: 0.4883\n",
      "Epoch: 045, Train Loss: 1.0958, Val Acc: 0.5088, Test Acc: 0.4883\n",
      "Epoch: 046, Train Loss: 1.0909, Val Acc: 0.5205, Test Acc: 0.4883\n",
      "Epoch: 047, Train Loss: 1.0848, Val Acc: 0.5029, Test Acc: 0.4883\n",
      "Epoch: 048, Train Loss: 1.0793, Val Acc: 0.5146, Test Acc: 0.4883\n",
      "Epoch: 049, Train Loss: 1.0927, Val Acc: 0.5117, Test Acc: 0.4883\n",
      "Epoch: 050, Train Loss: 1.0615, Val Acc: 0.5088, Test Acc: 0.4883\n",
      "Epoch: 051, Train Loss: 1.0655, Val Acc: 0.5292, Test Acc: 0.5205\n",
      "Epoch: 052, Train Loss: 1.0447, Val Acc: 0.5409, Test Acc: 0.5088\n",
      "Epoch: 053, Train Loss: 1.0443, Val Acc: 0.5292, Test Acc: 0.5088\n",
      "Epoch: 054, Train Loss: 1.0561, Val Acc: 0.5380, Test Acc: 0.5088\n",
      "Epoch: 055, Train Loss: 1.0379, Val Acc: 0.5468, Test Acc: 0.5088\n",
      "Epoch: 056, Train Loss: 1.0110, Val Acc: 0.5556, Test Acc: 0.5351\n",
      "Epoch: 057, Train Loss: 1.0136, Val Acc: 0.5614, Test Acc: 0.5175\n",
      "Epoch: 058, Train Loss: 1.0012, Val Acc: 0.5439, Test Acc: 0.5175\n",
      "Epoch: 059, Train Loss: 1.0050, Val Acc: 0.4912, Test Acc: 0.5175\n",
      "Epoch: 060, Train Loss: 0.9902, Val Acc: 0.5146, Test Acc: 0.5175\n",
      "Epoch: 061, Train Loss: 0.9922, Val Acc: 0.5351, Test Acc: 0.5175\n",
      "Epoch: 062, Train Loss: 0.9635, Val Acc: 0.5146, Test Acc: 0.5175\n",
      "Epoch: 063, Train Loss: 0.9693, Val Acc: 0.5175, Test Acc: 0.5175\n",
      "Epoch: 064, Train Loss: 0.9778, Val Acc: 0.5292, Test Acc: 0.5175\n",
      "Epoch: 065, Train Loss: 0.9585, Val Acc: 0.5409, Test Acc: 0.5175\n",
      "Epoch: 066, Train Loss: 0.9497, Val Acc: 0.5351, Test Acc: 0.5175\n",
      "Epoch: 067, Train Loss: 0.9505, Val Acc: 0.5497, Test Acc: 0.5175\n",
      "Epoch: 068, Train Loss: 0.9674, Val Acc: 0.5058, Test Acc: 0.5175\n",
      "Epoch: 069, Train Loss: 0.9687, Val Acc: 0.5146, Test Acc: 0.5175\n",
      "Epoch: 070, Train Loss: 0.9693, Val Acc: 0.5263, Test Acc: 0.5175\n",
      "Epoch: 071, Train Loss: 0.9541, Val Acc: 0.5088, Test Acc: 0.5175\n",
      "Epoch: 072, Train Loss: 0.9644, Val Acc: 0.5292, Test Acc: 0.5175\n",
      "Epoch: 073, Train Loss: 0.9307, Val Acc: 0.5292, Test Acc: 0.5175\n",
      "Epoch: 074, Train Loss: 0.9507, Val Acc: 0.5351, Test Acc: 0.5175\n",
      "Epoch: 075, Train Loss: 0.9282, Val Acc: 0.5175, Test Acc: 0.5175\n",
      "Epoch: 076, Train Loss: 0.9250, Val Acc: 0.5175, Test Acc: 0.5175\n",
      "Epoch: 077, Train Loss: 0.8837, Val Acc: 0.5468, Test Acc: 0.5175\n",
      "Epoch: 078, Train Loss: 0.9109, Val Acc: 0.5263, Test Acc: 0.5175\n",
      "Epoch: 079, Train Loss: 0.9374, Val Acc: 0.5409, Test Acc: 0.5175\n",
      "Epoch: 080, Train Loss: 0.9177, Val Acc: 0.5322, Test Acc: 0.5175\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[85], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation samples: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(val_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest samples: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m model, best_val_acc, test_acc \u001B[38;5;241m=\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[77], line 129\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(dataset, train_loader, val_loader, test_loader)\u001B[0m\n\u001B[1;32m    126\u001B[0m test_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m151\u001B[39m):\n\u001B[0;32m--> 129\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    130\u001B[0m     val_acc \u001B[38;5;241m=\u001B[39m test(model, val_loader, device)\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m val_acc \u001B[38;5;241m>\u001B[39m best_val_acc:\n",
      "Cell \u001B[0;32mIn[77], line 96\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, optimizer, device)\u001B[0m\n\u001B[1;32m     94\u001B[0m     loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnll_loss(output, data\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     95\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 96\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m data\u001B[38;5;241m.\u001B[39mnum_graphs\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader\u001B[38;5;241m.\u001B[39mdataset)\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    489\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    490\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m             )\n\u001B[0;32m--> 493\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/optim/adam.py:244\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    232\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    234\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    235\u001B[0m         group,\n\u001B[1;32m    236\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    241\u001B[0m         state_steps,\n\u001B[1;32m    242\u001B[0m     )\n\u001B[0;32m--> 244\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/optim/adam.py:876\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    873\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    874\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 876\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/optim/adam.py:425\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m    423\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mlerp_(grad, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m device_beta1)\n\u001B[0;32m--> 425\u001B[0m \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddcmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconj\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n\u001B[1;32m    428\u001B[0m     step \u001B[38;5;241m=\u001B[39m step_t\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 85
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
