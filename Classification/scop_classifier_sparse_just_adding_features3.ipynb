{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T23:11:54.448433Z",
     "start_time": "2025-02-16T23:11:51.937078Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv  # Changed from DenseSAGEConv\n",
    "from torch_geometric.nn import global_mean_pool  # For sparse pooling\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define data directory\n",
    "data_dir = 'data/SCOP'  # Base directory for SCOP data\n",
    "\n",
    "# Rest of the model code remains the same..."
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:26:51.176147Z",
     "start_time": "2025-02-16T23:26:51.139326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SparseSCOPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        # Set up the root path first\n",
    "        self.root = root\n",
    "\n",
    "        # Now we can set up the class_info_path\n",
    "        self.class_info_path = os.path.join(root, 'raw/class_info.csv')\n",
    "\n",
    "        # Dictionary to map SCOP classes to indices\n",
    "        self.class_mapping = {\n",
    "            'a': 0,  # All-alpha\n",
    "            'b': 1,  # All-beta\n",
    "            'c': 2,  # Alpha/beta\n",
    "            'd': 3,  # Alpha+beta\n",
    "            'e': 4,  # Multi-domain\n",
    "            'f': 5,  # Membrane\n",
    "            'g': 6   # Small proteins\n",
    "        }\n",
    "\n",
    "        # Dictionary to map amino acids to indices\n",
    "        self.amino_acids = {\n",
    "            'ALA': 0, 'ARG': 1, 'ASN': 2, 'ASP': 3, 'CYS': 4,\n",
    "            'GLN': 5, 'GLU': 6, 'GLY': 7, 'HIS': 8, 'ILE': 9,\n",
    "            'LEU': 10, 'LYS': 11, 'MET': 12, 'PHE': 13, 'PRO': 14,\n",
    "            'SER': 15, 'THR': 16, 'TRP': 17, 'TYR': 18, 'VAL': 19,\n",
    "            'UNK': 20  # Unknown amino acid\n",
    "        }\n",
    "\n",
    "        # Load class information\n",
    "        if os.path.exists(self.class_info_path):\n",
    "            self.class_info = pd.read_csv(self.class_info_path)\n",
    "            print(f\"Found class info file with {len(self.class_info)} entries\")\n",
    "        else:\n",
    "            print(f\"Warning: class_info.csv not found at {self.class_info_path}\")\n",
    "            self.class_info = None\n",
    "\n",
    "        # Initialize the base class last\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def raw_file_names(self):\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"List of raw file names in the dataset.\"\"\"\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"List of processed file names in the dataset.\"\"\"\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        pass  # We already have the files\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Process the raw data into the internal format with comprehensive logging.\"\"\"\n",
    "        data_list = []\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        skipped_reasons = {}\n",
    "\n",
    "        # Log total number of entries in class_info\n",
    "        print(f\"Total entries in class_info: {len(self.class_info)}\")\n",
    "\n",
    "        # Verify raw directory contents\n",
    "        raw_dir = os.path.join(self.root, 'raw')\n",
    "        raw_files = [f for f in os.listdir(raw_dir) if f.endswith('.pdb')]\n",
    "        print(f\"Total PDB files in raw directory: {len(raw_files)}\")\n",
    "\n",
    "        for idx, row in self.class_info.iterrows():\n",
    "            pdb_id = str(row['scop_id'])\n",
    "\n",
    "            try:\n",
    "                # Validate class mapping\n",
    "                class_label = row['class']\n",
    "                if class_label not in self.class_mapping:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = f\"Invalid class: {class_label}\"\n",
    "                    continue\n",
    "\n",
    "                class_label = self.class_mapping[class_label]\n",
    "\n",
    "                # Load structure\n",
    "                pdb_file = f\"{pdb_id}.pdb\"\n",
    "                pdb_path = os.path.join(raw_dir, pdb_file)\n",
    "\n",
    "                # Check if file exists\n",
    "                if not os.path.exists(pdb_path):\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"PDB file not found\"\n",
    "                    continue\n",
    "\n",
    "                structure = parser.get_structure('protein', pdb_path)\n",
    "                model = structure[0]\n",
    "\n",
    "                # Get residues and create features\n",
    "                residues = list(model.get_residues())\n",
    "\n",
    "                # Skip if too few or too many residues\n",
    "                if len(residues) < 10 or len(residues) > 1400:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = f\"Invalid residue count: {len(residues)}\"\n",
    "                    continue\n",
    "\n",
    "                # Create node features\n",
    "                node_features = []\n",
    "                for residue in residues:\n",
    "                    features = self._get_residue_features(residue)\n",
    "                    node_features.append(features)\n",
    "\n",
    "                # Create edges with 5Å cutoff\n",
    "                edges = []\n",
    "                for i in range(len(residues)):\n",
    "                    for j in range(i+1, len(residues)):\n",
    "                        if 'CA' in residues[i] and 'CA' in residues[j]:\n",
    "                            ca_i = residues[i]['CA'].get_coord()\n",
    "                            ca_j = residues[j]['CA'].get_coord()\n",
    "                            dist = np.linalg.norm(ca_i - ca_j)\n",
    "                            if dist < 5.0:  # 5Å cutoff\n",
    "                                edges.append([i, j])\n",
    "                                edges.append([j, i])  # Add both directions\n",
    "\n",
    "                if len(edges) == 0:\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"No edges found\"\n",
    "                    continue\n",
    "\n",
    "                # Create PyG Data object\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "                x = torch.tensor(node_features, dtype=torch.float)\n",
    "                y = torch.tensor([class_label], dtype=torch.long)\n",
    "\n",
    "                data = Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    y=y,\n",
    "                    num_nodes=len(residues)\n",
    "                )\n",
    "\n",
    "                # Additional filtering if needed\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    skipped_count += 1\n",
    "                    skipped_reasons[pdb_id] = \"Failed pre-filter\"\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                data_list.append(data)\n",
    "                processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                skipped_count += 1\n",
    "                skipped_reasons[pdb_id] = f\"Processing error: {str(e)}\"\n",
    "                continue\n",
    "\n",
    "        # Detailed logging of skipped reasons\n",
    "        print(f\"\\nProcessing Summary:\")\n",
    "        print(f\"Total processed: {processed_count}\")\n",
    "        print(f\"Total skipped: {skipped_count}\")\n",
    "        print(\"\\nSkipped Reasons:\")\n",
    "        for reason, count in Counter(skipped_reasons.values()).most_common():\n",
    "            print(f\"{reason}: {count}\")\n",
    "\n",
    "        # Optional: Print some skipped PDB IDs for investigation\n",
    "        print(\"\\nSample of skipped PDB IDs:\")\n",
    "        for reason, pdb_ids in groupby(sorted(skipped_reasons.items(), key=lambda x: x[1]), key=lambda x: x[1]):\n",
    "            print(f\"{reason}: {list(pdb_ids)[:5]}\")\n",
    "\n",
    "        if len(data_list) == 0:\n",
    "            raise RuntimeError(\"No data was successfully processed!\")\n",
    "\n",
    "        # Save processed data\n",
    "        torch.save(data_list, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Get a specific graph from the dataset.\"\"\"\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "            if not os.path.exists(processed_path):\n",
    "                print(\"Warning: Processed data file not found, running processing...\")\n",
    "                self.process()\n",
    "            # Add weights_only=False to allow loading PyG Data objects\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "        return self._data_list[idx]\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"Return the number of graphs in the dataset.\"\"\"\n",
    "        processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "        if not os.path.exists(processed_path):\n",
    "            print(\"Warning: Processed data file not found, running processing...\")\n",
    "            self.process()\n",
    "\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            # Add weights_only=False here as well\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "        print(f\"Actual number of processed samples: {len(self._data_list)}\")\n",
    "        return len(self._data_list)\n",
    "\n",
    "    def _get_residue_features(self, residue):\n",
    "        \"\"\"Create feature vector for a residue.\"\"\"\n",
    "        # One-hot encode amino acid type\n",
    "        aa_features = np.zeros(21)  # 20 standard amino acids + UNK\n",
    "        aa_name = residue.get_resname()\n",
    "        if aa_name in self.amino_acids:\n",
    "            aa_features[self.amino_acids[aa_name]] = 1\n",
    "        else:\n",
    "            aa_features[self.amino_acids['UNK']] = 1\n",
    "\n",
    "        # Get CA atom coordinates\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            coords = ca_atom.get_coord()\n",
    "        except:\n",
    "            coords = np.zeros(3)\n",
    "\n",
    "        # Define scalar features as floats\n",
    "        mass = 1.0             # Mass (1)\n",
    "        avg_neighbor_dist = 2.0 # Average neighbor distance (1)\n",
    "        max_neighbor_dist = 3.0 # Maximum neighbor distance (1)\n",
    "        neighbor_count = 1.0    # Neighbor count (1)\n",
    "\n",
    "        # Combine features into a single 1D array\n",
    "        features = np.hstack([\n",
    "            aa_features,        # Shape: (21,)\n",
    "            coords,             # Shape: (3,)\n",
    "            mass,               # Shape: scalar (automatically treated as (1,))\n",
    "            avg_neighbor_dist,  # Shape: scalar\n",
    "            max_neighbor_dist,  # Shape: scalar\n",
    "            neighbor_count      # Shape: scalar\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Return the number of classes in the dataset.\"\"\"\n",
    "        return len(self.class_mapping)\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        \"\"\"Return the number of node features.\"\"\"\n",
    "        return 28  # 21 for amino acids + 3 for coordinates"
   ],
   "id": "8eee4b4a0d18bcc8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:27:17.683839Z",
     "start_time": "2025-02-16T23:26:53.691566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the root directory\n",
    "data_dir = 'data/SCOP'  # Adjust this to your actual data directory path\n",
    "\n",
    "# Create the dataset (this should trigger the process method)\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "# Create the dataset instance\n",
    "\n",
    "# Explicitly call the process method\n",
    "processed_data = dataset.process()"
   ],
   "id": "b40a91a41180d6ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class info file with 350 entries\n",
      "Total entries in class_info: 350\n",
      "Total PDB files in raw directory: 350\n",
      "\n",
      "Processing Summary:\n",
      "Total processed: 349\n",
      "Total skipped: 1\n",
      "\n",
      "Skipped Reasons:\n",
      "Processing error: 0: 1\n",
      "\n",
      "Sample of skipped PDB IDs:\n",
      "Processing error: 0: [('410195', 'Processing error: 0')]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:27:17.738539Z",
     "start_time": "2025-02-16T23:27:17.691638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "NUM_CLASSES = 7  # SCOP main classes\n",
    "# First, let's check protein sizes\n",
    "# First, analyze protein sizes\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "sizes = [data.num_nodes for data in data_list]\n",
    "\n",
    "print(f\"Protein size statistics:\")\n",
    "print(f\"Min size: {min(sizes)}\")\n",
    "print(f\"Max size: {max(sizes)}\")\n",
    "print(f\"Mean size: {sum(sizes)/len(sizes):.1f}\")\n",
    "print(f\"Median size: {sorted(sizes)[len(sizes)//2]}\")\n",
    "print(f\"Number of proteins > 150 residues: {sum(1 for s in sizes if s > 150)}\")\n",
    "\n",
    "# Set max_nodes to 500 since we have proteins up to 1381 residues\n",
    "max_nodes = 1400\n",
    "\n",
    "# Create dataset\n",
    "#dataset = SparseSCOPDataset(\n",
    "#    root=data_dir,\n",
    "#    pre_filter=lambda data: data.num_nodes <= max_nodes\n",
    "#)\n",
    "\n",
    "#dataset = SparseSCOPDataset(root=data_dir)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n"
   ],
   "id": "81c665801b8fe044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein size statistics:\n",
      "Min size: 25\n",
      "Max size: 1381\n",
      "Mean size: 204.4\n",
      "Median size: 153\n",
      "Number of proteins > 150 residues: 177\n",
      "Actual number of processed samples: 349\n",
      "Dataset size: 349\n",
      "Actual number of processed samples: 349\n",
      "\n",
      "Dataset size: 349\n",
      "Number of features: 28\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:34:32.260464Z",
     "start_time": "2025-02-16T19:34:32.194108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming your dataset is loaded\n",
    "filtered_dataset = [data for data in dataset if data.num_nodes < 300]\n",
    "\n",
    "# If you want to create a new dataset object\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class FilteredSCOPDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.data_list = [data for data in original_dataset if data.num_nodes < 300]\n",
    "        super().__init__(original_dataset.root)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create the filtered dataset\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Verify the filtering\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Optional: Check distribution across classes\n",
    "class_distribution = {}\n",
    "for data in filtered_dataset:\n",
    "    class_label = data.y.item()\n",
    "    class_distribution[class_label] = class_distribution.get(class_label, 0) + 1\n",
    "\n",
    "print(\"\\nClass distribution in filtered dataset:\")\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"Class {cls}: {count} proteins\")"
   ],
   "id": "755cc2d19aa5e717",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 3420\n",
      "Filtered dataset size: 2717\n",
      "\n",
      "Class distribution in filtered dataset:\n",
      "Class 1: 449 proteins\n",
      "Class 2: 368 proteins\n",
      "Class 0: 428 proteins\n",
      "Class 6: 500 proteins\n",
      "Class 5: 418 proteins\n",
      "Class 3: 382 proteins\n",
      "Class 4: 172 proteins\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:47:54.763162Z",
     "start_time": "2025-02-16T22:46:59.257804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the root directory\n",
    "data_dir = 'data/SCOP'  # Adjust this to your actual data directory path\n",
    "\n",
    "# Create the dataset (this should trigger the process method)\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "# Create the dataset instance\n",
    "\n",
    "# Explicitly call the process method\n",
    "processed_data = dataset.process()"
   ],
   "id": "326dab61158a6c1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class info file with 350 entries\n",
      "Total entries in class_info: 350\n",
      "Total PDB files in raw directory: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "Total processed: 345\n",
      "Total skipped: 5\n",
      "\n",
      "Skipped Reasons:\n",
      "Processing error: cannot access local variable 'randtest' where it is not associated with a value: 4\n",
      "Processing error: 0: 1\n",
      "\n",
      "Sample of skipped PDB IDs:\n",
      "Processing error: 0: [('410195', 'Processing error: 0')]\n",
      "Processing error: cannot access local variable 'randtest' where it is not associated with a value: [('86659', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('276088', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('309746', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('152769', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\")]\n",
      "Total entries in class_info: 350\n",
      "Total PDB files in raw directory: 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "Total processed: 345\n",
      "Total skipped: 5\n",
      "\n",
      "Skipped Reasons:\n",
      "Processing error: cannot access local variable 'randtest' where it is not associated with a value: 4\n",
      "Processing error: 0: 1\n",
      "\n",
      "Sample of skipped PDB IDs:\n",
      "Processing error: 0: [('410195', 'Processing error: 0')]\n",
      "Processing error: cannot access local variable 'randtest' where it is not associated with a value: [('86659', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('276088', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('309746', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\"), ('152769', \"Processing error: cannot access local variable 'randtest' where it is not associated with a value\")]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:35:39.969356Z",
     "start_time": "2025-02-16T19:35:39.927705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the filtered dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Shuffle and split the filtered dataset\n",
    "filtered_dataset = filtered_dataset.shuffle()\n",
    "n = (len(filtered_dataset) + 9) // 10\n",
    "test_dataset = filtered_dataset[:n]\n",
    "val_dataset = filtered_dataset[n:2 * n]\n",
    "train_dataset = filtered_dataset[2 * n:]\n",
    "\n",
    "# Create data loaders using the filtered datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"\\nFiltered Dataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# The rest of your training loop remains the same\n",
    "# You can use these loaders directly in your existing training script"
   ],
   "id": "510af8d5deb60124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Dataset splits:\n",
      "Training samples: 2173\n",
      "Validation samples: 272\n",
      "Test samples: 272\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:31:11.960641Z",
     "start_time": "2025-02-16T19:31:11.819218Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "243e6c1a459fa5a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 2736\n",
      "Validation samples: 342\n",
      "Test samples: 342\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:27:28.116130Z",
     "start_time": "2025-02-16T23:27:27.660711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you've already loaded the dataset\n",
    "# Load the processed data\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "# Reverse class mapping for readable labels\n",
    "class_mapping_reverse = {\n",
    "    0: 'a (All-alpha)',\n",
    "    1: 'b (All-beta)',\n",
    "    2: 'c (Alpha/beta)',\n",
    "    3: 'd (Alpha+beta)',\n",
    "    4: 'e (Multi-domain)',\n",
    "    5: 'f (Membrane)',\n",
    "    6: 'g (Small proteins)'\n",
    "}\n",
    "\n",
    "# Separate nodes by class\n",
    "nodes_by_class = {}\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in nodes_by_class:\n",
    "        nodes_by_class[class_label] = []\n",
    "    nodes_by_class[class_label].append(data.num_nodes)\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Box plot\n",
    "plt.boxplot([nodes_by_class[key] for key in sorted(nodes_by_class.keys())],\n",
    "            labels=[class_mapping_reverse[key] for key in sorted(nodes_by_class.keys())])\n",
    "\n",
    "plt.title('Number of Nodes per SCOP Class', fontsize=16)\n",
    "plt.xlabel('SCOP Class', fontsize=12)\n",
    "plt.ylabel('Number of Nodes (Residues)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('nodes_per_class_boxplot.png')\n",
    "plt.close()\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Node count statistics per class:\")\n",
    "for class_label, nodes in nodes_by_class.items():\n",
    "    print(f\"\\n{class_mapping_reverse[class_label]}:\")\n",
    "    print(f\"  Count: {len(nodes)}\")\n",
    "    print(f\"  Min nodes: {min(nodes)}\")\n",
    "    print(f\"  Max nodes: {max(nodes)}\")\n",
    "    print(f\"  Mean nodes: {np.mean(nodes):.2f}\")\n",
    "    print(f\"  Median nodes: {np.median(nodes):.2f}\")"
   ],
   "id": "27dc0cc370f2c3dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node count statistics per class:\n",
      "\n",
      "a (All-alpha):\n",
      "  Count: 50\n",
      "  Min nodes: 43\n",
      "  Max nodes: 522\n",
      "  Mean nodes: 159.62\n",
      "  Median nodes: 133.00\n",
      "\n",
      "b (All-beta):\n",
      "  Count: 50\n",
      "  Min nodes: 55\n",
      "  Max nodes: 520\n",
      "  Mean nodes: 159.54\n",
      "  Median nodes: 118.00\n",
      "\n",
      "c (Alpha/beta):\n",
      "  Count: 50\n",
      "  Min nodes: 77\n",
      "  Max nodes: 551\n",
      "  Mean nodes: 275.04\n",
      "  Median nodes: 252.00\n",
      "\n",
      "d (Alpha+beta):\n",
      "  Count: 50\n",
      "  Min nodes: 58\n",
      "  Max nodes: 416\n",
      "  Mean nodes: 161.46\n",
      "  Median nodes: 129.00\n",
      "\n",
      "e (Multi-domain):\n",
      "  Count: 50\n",
      "  Min nodes: 186\n",
      "  Max nodes: 1381\n",
      "  Mean nodes: 399.54\n",
      "  Median nodes: 358.00\n",
      "\n",
      "f (Membrane):\n",
      "  Count: 49\n",
      "  Min nodes: 25\n",
      "  Max nodes: 691\n",
      "  Mean nodes: 212.33\n",
      "  Median nodes: 146.00\n",
      "\n",
      "g (Small proteins):\n",
      "  Count: 50\n",
      "  Min nodes: 29\n",
      "  Max nodes: 131\n",
      "  Mean nodes: 63.32\n",
      "  Median nodes: 53.00\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:27:35.162594Z",
     "start_time": "2025-02-16T23:27:35.123528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "# Count proteins per class and above 300 nodes\n",
    "class_counts = {}\n",
    "above_300_counts = {}\n",
    "\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in class_counts:\n",
    "        class_counts[class_label] = 0\n",
    "        above_300_counts[class_label] = 0\n",
    "\n",
    "    class_counts[class_label] += 1\n",
    "    if data.num_nodes > 300:\n",
    "        above_300_counts[class_label] += 1\n",
    "\n",
    "print(\"Total proteins per class:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"Class {cls}: {count} total, {above_300_counts[cls]} above 300 nodes ({above_300_counts[cls]/count*100:.2f}%)\")"
   ],
   "id": "e76168280612488f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins per class:\n",
      "Class 0: 50 total, 4 above 300 nodes (8.00%)\n",
      "Class 1: 50 total, 5 above 300 nodes (10.00%)\n",
      "Class 2: 50 total, 18 above 300 nodes (36.00%)\n",
      "Class 3: 50 total, 2 above 300 nodes (4.00%)\n",
      "Class 4: 50 total, 34 above 300 nodes (68.00%)\n",
      "Class 5: 49 total, 12 above 300 nodes (24.49%)\n",
      "Class 6: 50 total, 0 above 300 nodes (0.00%)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:27:39.668325Z",
     "start_time": "2025-02-16T23:27:39.655311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from math import ceil\n",
    "\n",
    "class SparseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # GNN layers with sparse representation\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First convolution layer\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = F.relu(h1)\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Second convolution layer\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = F.relu(h2)\n",
    "        h2 = self.dropout(h2)\n",
    "\n",
    "        # Third convolution layer\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "        h3 = self.bn3(h3)\n",
    "        h3 = F.relu(h3)\n",
    "        h3 = self.dropout(h3)\n",
    "\n",
    "        # Global mean pooling\n",
    "        out = global_mean_pool(h3, batch)\n",
    "\n",
    "        # MLP head\n",
    "        out = self.lin1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a single GNN model for classification\n",
    "        self.gnn = SparseGNN(\n",
    "            num_features=28,\n",
    "            hidden_dim=64,\n",
    "            num_classes=dataset.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Single forward pass through the GNN\n",
    "        x = self.gnn(x, edge_index, batch)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"Determine the best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    \"\"\"Training function for the model.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    \"\"\"Evaluation function for the model.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def main(dataset, train_loader, val_loader, test_loader):\n",
    "    \"\"\"Main training and evaluation loop.\"\"\"\n",
    "    device = setup_device()\n",
    "    model = Net(dataset).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for epoch in range(1, 151):\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        val_acc = test(model, val_loader, device)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = test(model, test_loader, device)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return model, best_val_acc, test_acc\n",
    "\n",
    "# Note: This function would be called after setting up the dataset, loaders, etc.\n",
    "# main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "738471d4eece3147",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:27:46.202506Z",
     "start_time": "2025-02-16T23:27:46.171366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data loaders (as you had before)\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Use DataLoader for sparse graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Train the model\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "e4d1fde02103aab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual number of processed samples: 349\n",
      "Actual number of processed samples: 349\n",
      "Actual number of processed samples: 349\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m n:]\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Use DataLoader for sparse graphs\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m \u001B[43mDataLoader\u001B[49m(train_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m)\n\u001B[1;32m     10\u001B[0m val_loader \u001B[38;5;241m=\u001B[39m DataLoader(val_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m)\n\u001B[1;32m     11\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:28:50.453311Z",
     "start_time": "2025-02-16T23:28:02.512543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Replace your existing DataLoader with this\n",
    "train_loader = DataLoader(train_dataset,  batch_size=20)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=20)\n",
    "test_loader = DataLoader(test_dataset,  batch_size=20)\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)\n"
   ],
   "id": "db76b9390eadc048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 279\n",
      "Validation samples: 35\n",
      "Test samples: 35\n",
      "Epoch: 001, Train Loss: 1.9530, Val Acc: 0.0857, Test Acc: 0.2000\n",
      "Epoch: 002, Train Loss: 1.8774, Val Acc: 0.0571, Test Acc: 0.2000\n",
      "Epoch: 003, Train Loss: 1.8656, Val Acc: 0.0571, Test Acc: 0.2000\n",
      "Epoch: 004, Train Loss: 1.8357, Val Acc: 0.0571, Test Acc: 0.2000\n",
      "Epoch: 005, Train Loss: 1.8044, Val Acc: 0.0286, Test Acc: 0.2000\n",
      "Epoch: 006, Train Loss: 1.7825, Val Acc: 0.0857, Test Acc: 0.2000\n",
      "Epoch: 007, Train Loss: 1.7745, Val Acc: 0.0571, Test Acc: 0.2000\n",
      "Epoch: 008, Train Loss: 1.7326, Val Acc: 0.1143, Test Acc: 0.2571\n",
      "Epoch: 009, Train Loss: 1.6973, Val Acc: 0.0857, Test Acc: 0.2571\n",
      "Epoch: 010, Train Loss: 1.6691, Val Acc: 0.1143, Test Acc: 0.2571\n",
      "Epoch: 011, Train Loss: 1.6266, Val Acc: 0.1143, Test Acc: 0.2571\n",
      "Epoch: 012, Train Loss: 1.6316, Val Acc: 0.0857, Test Acc: 0.2571\n",
      "Epoch: 013, Train Loss: 1.5804, Val Acc: 0.1429, Test Acc: 0.2857\n",
      "Epoch: 014, Train Loss: 1.5418, Val Acc: 0.1429, Test Acc: 0.2857\n",
      "Epoch: 015, Train Loss: 1.5155, Val Acc: 0.1429, Test Acc: 0.2857\n",
      "Epoch: 016, Train Loss: 1.4794, Val Acc: 0.2000, Test Acc: 0.3143\n",
      "Epoch: 017, Train Loss: 1.4688, Val Acc: 0.2286, Test Acc: 0.2571\n",
      "Epoch: 018, Train Loss: 1.4120, Val Acc: 0.1714, Test Acc: 0.2571\n",
      "Epoch: 019, Train Loss: 1.3970, Val Acc: 0.1714, Test Acc: 0.2571\n",
      "Epoch: 020, Train Loss: 1.3657, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 021, Train Loss: 1.3173, Val Acc: 0.1429, Test Acc: 0.2571\n",
      "Epoch: 022, Train Loss: 1.2984, Val Acc: 0.2286, Test Acc: 0.2571\n",
      "Epoch: 023, Train Loss: 1.2795, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 024, Train Loss: 1.2448, Val Acc: 0.1429, Test Acc: 0.2571\n",
      "Epoch: 025, Train Loss: 1.2360, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 026, Train Loss: 1.1821, Val Acc: 0.1143, Test Acc: 0.2571\n",
      "Epoch: 027, Train Loss: 1.1736, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 028, Train Loss: 1.1464, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 029, Train Loss: 1.1344, Val Acc: 0.1714, Test Acc: 0.2000\n",
      "Epoch: 030, Train Loss: 1.0766, Val Acc: 0.1429, Test Acc: 0.2000\n",
      "Epoch: 031, Train Loss: 1.0564, Val Acc: 0.1143, Test Acc: 0.2000\n",
      "Epoch: 032, Train Loss: 0.9906, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 033, Train Loss: 0.9789, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 034, Train Loss: 0.9699, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 035, Train Loss: 0.8977, Val Acc: 0.1714, Test Acc: 0.2000\n",
      "Epoch: 036, Train Loss: 0.8812, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 037, Train Loss: 0.8753, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 038, Train Loss: 0.8362, Val Acc: 0.1143, Test Acc: 0.2000\n",
      "Epoch: 039, Train Loss: 0.8274, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 040, Train Loss: 0.8275, Val Acc: 0.2857, Test Acc: 0.2571\n",
      "Epoch: 041, Train Loss: 0.8106, Val Acc: 0.1714, Test Acc: 0.2571\n",
      "Epoch: 042, Train Loss: 0.7706, Val Acc: 0.2571, Test Acc: 0.2571\n",
      "Epoch: 043, Train Loss: 0.7401, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 044, Train Loss: 0.7490, Val Acc: 0.1714, Test Acc: 0.2571\n",
      "Epoch: 045, Train Loss: 0.7200, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 046, Train Loss: 0.7392, Val Acc: 0.2857, Test Acc: 0.2571\n",
      "Epoch: 047, Train Loss: 0.6922, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 048, Train Loss: 0.6550, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 049, Train Loss: 0.6241, Val Acc: 0.2286, Test Acc: 0.2571\n",
      "Epoch: 050, Train Loss: 0.6421, Val Acc: 0.2571, Test Acc: 0.2571\n",
      "Epoch: 051, Train Loss: 0.6008, Val Acc: 0.2571, Test Acc: 0.2571\n",
      "Epoch: 052, Train Loss: 0.5367, Val Acc: 0.2286, Test Acc: 0.2571\n",
      "Epoch: 053, Train Loss: 0.5407, Val Acc: 0.2857, Test Acc: 0.2571\n",
      "Epoch: 054, Train Loss: 0.5245, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 055, Train Loss: 0.5368, Val Acc: 0.2286, Test Acc: 0.2571\n",
      "Epoch: 056, Train Loss: 0.5120, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 057, Train Loss: 0.5813, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 058, Train Loss: 0.5409, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 059, Train Loss: 0.4884, Val Acc: 0.1714, Test Acc: 0.2571\n",
      "Epoch: 060, Train Loss: 0.4998, Val Acc: 0.2286, Test Acc: 0.2571\n",
      "Epoch: 061, Train Loss: 0.4531, Val Acc: 0.2000, Test Acc: 0.2571\n",
      "Epoch: 062, Train Loss: 0.4289, Val Acc: 0.2286, Test Acc: 0.2571\n",
      "Epoch: 063, Train Loss: 0.3916, Val Acc: 0.3143, Test Acc: 0.3429\n",
      "Epoch: 064, Train Loss: 0.4320, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 065, Train Loss: 0.4061, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 066, Train Loss: 0.4298, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 067, Train Loss: 0.3939, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 068, Train Loss: 0.3852, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 069, Train Loss: 0.3911, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 070, Train Loss: 0.3821, Val Acc: 0.1714, Test Acc: 0.3429\n",
      "Epoch: 071, Train Loss: 0.3341, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 072, Train Loss: 0.4323, Val Acc: 0.1714, Test Acc: 0.3429\n",
      "Epoch: 073, Train Loss: 0.4001, Val Acc: 0.1429, Test Acc: 0.3429\n",
      "Epoch: 074, Train Loss: 0.3791, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 075, Train Loss: 0.3675, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 076, Train Loss: 0.3708, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 077, Train Loss: 0.3329, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 078, Train Loss: 0.3272, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 079, Train Loss: 0.3122, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 080, Train Loss: 0.3070, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 081, Train Loss: 0.3021, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 082, Train Loss: 0.2545, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 083, Train Loss: 0.2611, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 084, Train Loss: 0.2270, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 085, Train Loss: 0.2664, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 086, Train Loss: 0.2809, Val Acc: 0.3143, Test Acc: 0.3429\n",
      "Epoch: 087, Train Loss: 0.2417, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 088, Train Loss: 0.2209, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 089, Train Loss: 0.2000, Val Acc: 0.3143, Test Acc: 0.3429\n",
      "Epoch: 090, Train Loss: 0.2142, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 091, Train Loss: 0.2470, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 092, Train Loss: 0.2472, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 093, Train Loss: 0.2266, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 094, Train Loss: 0.2186, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 095, Train Loss: 0.2213, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 096, Train Loss: 0.2578, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 097, Train Loss: 0.2619, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 098, Train Loss: 0.3507, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 099, Train Loss: 0.3455, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 100, Train Loss: 0.3999, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 101, Train Loss: 0.3967, Val Acc: 0.1429, Test Acc: 0.3429\n",
      "Epoch: 102, Train Loss: 0.2871, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 103, Train Loss: 0.1981, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 104, Train Loss: 0.1677, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 105, Train Loss: 0.1640, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 106, Train Loss: 0.1777, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 107, Train Loss: 0.1430, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 108, Train Loss: 0.1518, Val Acc: 0.3143, Test Acc: 0.3429\n",
      "Epoch: 109, Train Loss: 0.1392, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 110, Train Loss: 0.1629, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 111, Train Loss: 0.1615, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 112, Train Loss: 0.1436, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 113, Train Loss: 0.1627, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 114, Train Loss: 0.1626, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 115, Train Loss: 0.2447, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 116, Train Loss: 0.2083, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 117, Train Loss: 0.3278, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 118, Train Loss: 0.3124, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 119, Train Loss: 0.2001, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 120, Train Loss: 0.2080, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 121, Train Loss: 0.1717, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 122, Train Loss: 0.1530, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 123, Train Loss: 0.1791, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 124, Train Loss: 0.1558, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 125, Train Loss: 0.1507, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 126, Train Loss: 0.1156, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 127, Train Loss: 0.1051, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 128, Train Loss: 0.1064, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 129, Train Loss: 0.1041, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 130, Train Loss: 0.0927, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 131, Train Loss: 0.1059, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 132, Train Loss: 0.1095, Val Acc: 0.3143, Test Acc: 0.3429\n",
      "Epoch: 133, Train Loss: 0.0884, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 134, Train Loss: 0.0966, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 135, Train Loss: 0.0874, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 136, Train Loss: 0.0947, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 137, Train Loss: 0.0814, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 138, Train Loss: 0.1061, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 139, Train Loss: 0.1065, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 140, Train Loss: 0.0876, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 141, Train Loss: 0.0739, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 142, Train Loss: 0.0816, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 143, Train Loss: 0.0962, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 144, Train Loss: 0.1201, Val Acc: 0.3143, Test Acc: 0.3429\n",
      "Epoch: 145, Train Loss: 0.0911, Val Acc: 0.2857, Test Acc: 0.3429\n",
      "Epoch: 146, Train Loss: 0.1451, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 147, Train Loss: 0.1645, Val Acc: 0.2571, Test Acc: 0.3429\n",
      "Epoch: 148, Train Loss: 0.1815, Val Acc: 0.2286, Test Acc: 0.3429\n",
      "Epoch: 149, Train Loss: 0.2216, Val Acc: 0.2000, Test Acc: 0.3429\n",
      "Epoch: 150, Train Loss: 0.1935, Val Acc: 0.3143, Test Acc: 0.3429\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
