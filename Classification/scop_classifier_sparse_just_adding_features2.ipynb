{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-16T22:58:12.134790Z",
     "start_time": "2025-02-16T22:58:10.153946Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv  # Changed from DenseSAGEConv\n",
    "from torch_geometric.nn import global_mean_pool  # For sparse pooling\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define data directory\n",
    "data_dir = 'data/SCOP'  # Base directory for SCOP data\n",
    "\n",
    "# Rest of the model code remains the same..."
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:03:56.269433Z",
     "start_time": "2025-02-16T23:03:56.229091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from Bio.PDB import NeighborSearch, Selection\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SparseSCOPDataset(Dataset):\n",
    "    def __init__(self, root: str, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.root = root\n",
    "        self.class_info_path = os.path.join(root, 'raw/class_info.csv')\n",
    "\n",
    "        # Add normalization parameters\n",
    "        self.coord_mean: Optional[float] = None\n",
    "        self.coord_std: Optional[float] = None\n",
    "        self.mass_mean: Optional[float] = None\n",
    "        self.mass_std: Optional[float] = None\n",
    "        self.dist_mean: Optional[float] = None\n",
    "        self.dist_std: Optional[float] = None\n",
    "        self.count_mean: Optional[float] = None\n",
    "        self.count_std: Optional[float] = None\n",
    "\n",
    "        # Feature indices for easy access\n",
    "        self.feature_indices = {\n",
    "            'aa_onehot': slice(0, 21),\n",
    "            'coords': slice(21, 24),\n",
    "            'mass': 24,\n",
    "            'avg_dist': 25,\n",
    "            'max_dist': 26,\n",
    "            'neighbor_count': 27\n",
    "        }\n",
    "\n",
    "        # Dictionary to map SCOP classes to indices\n",
    "        self.class_mapping = {\n",
    "            'a': 0,  # All-alpha\n",
    "            'b': 1,  # All-beta\n",
    "            'c': 2,  # Alpha/beta\n",
    "            'd': 3,  # Alpha+beta\n",
    "            'e': 4,  # Multi-domain\n",
    "            'f': 5,  # Membrane\n",
    "            'g': 6   # Small proteins\n",
    "        }\n",
    "\n",
    "        # Dictionary to map amino acids to indices and their properties\n",
    "        self.amino_acids = {\n",
    "            'ALA': {'index': 0, 'mass': 89.1, 'name': 'Alanine'},\n",
    "            'ARG': {'index': 1, 'mass': 174.2, 'name': 'Arginine'},\n",
    "            'ASN': {'index': 2, 'mass': 132.1, 'name': 'Asparagine'},\n",
    "            'ASP': {'index': 3, 'mass': 133.1, 'name': 'Aspartic Acid'},\n",
    "            'CYS': {'index': 4, 'mass': 121.2, 'name': 'Cysteine'},\n",
    "            'GLN': {'index': 5, 'mass': 146.2, 'name': 'Glutamine'},\n",
    "            'GLU': {'index': 6, 'mass': 147.1, 'name': 'Glutamic Acid'},\n",
    "            'GLY': {'index': 7, 'mass': 75.1, 'name': 'Glycine'},\n",
    "            'HIS': {'index': 8, 'mass': 155.2, 'name': 'Histidine'},\n",
    "            'ILE': {'index': 9, 'mass': 131.2, 'name': 'Isoleucine'},\n",
    "            'LEU': {'index': 10, 'mass': 131.2, 'name': 'Leucine'},\n",
    "            'LYS': {'index': 11, 'mass': 146.2, 'name': 'Lysine'},\n",
    "            'MET': {'index': 12, 'mass': 149.2, 'name': 'Methionine'},\n",
    "            'PHE': {'index': 13, 'mass': 165.2, 'name': 'Phenylalanine'},\n",
    "            'PRO': {'index': 14, 'mass': 115.1, 'name': 'Proline'},\n",
    "            'SER': {'index': 15, 'mass': 105.1, 'name': 'Serine'},\n",
    "            'THR': {'index': 16, 'mass': 119.1, 'name': 'Threonine'},\n",
    "            'TRP': {'index': 17, 'mass': 204.2, 'name': 'Tryptophan'},\n",
    "            'TYR': {'index': 18, 'mass': 181.2, 'name': 'Tyrosine'},\n",
    "            'VAL': {'index': 19, 'mass': 117.1, 'name': 'Valine'},\n",
    "            'UNK': {'index': 20, 'mass': 0.0, 'name': 'Unknown'}\n",
    "        }\n",
    "\n",
    "        # Load class information before calling super().__init__()\n",
    "        if os.path.exists(self.class_info_path):\n",
    "            self.class_info = pd.read_csv(self.class_info_path)\n",
    "            print(f\"Found class info file with {len(self.class_info)} entries\")\n",
    "        else:\n",
    "            print(f\"Warning: class_info.csv not found at {self.class_info_path}\")\n",
    "            self.class_info = None\n",
    "\n",
    "        # Initialize the base class after setting up our attributes\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "    @property  # This decorator was missing!\n",
    "    def raw_file_names(self):\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property  # This decorator was missing!\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"List of raw file names in the dataset.\"\"\"\n",
    "        if not os.path.exists(os.path.join(self.root, 'raw')):\n",
    "            return []\n",
    "        return [f for f in os.listdir(os.path.join(self.root, 'raw'))\n",
    "                if f.endswith('.pdb')]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"List of processed file names in the dataset.\"\"\"\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        pass  # We already have the files\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Process the raw data into the internal format.\"\"\"\n",
    "        if self.class_info is None:\n",
    "            raise RuntimeError(\"No class information available. Cannot process data.\")\n",
    "\n",
    "        data_list = []\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "\n",
    "        # Collect statistics for normalization\n",
    "        all_coords = []\n",
    "        all_masses = []\n",
    "        all_distances = []\n",
    "        all_counts = []\n",
    "\n",
    "        for idx, row in self.class_info.iterrows():\n",
    "            pdb_id = str(row['scop_id'])\n",
    "            class_label = self.class_mapping[row['class']]\n",
    "\n",
    "            try:\n",
    "                # Load structure\n",
    "                pdb_file = f\"{pdb_id}.pdb\"\n",
    "                pdb_path = os.path.join(self.root, 'raw', pdb_file)\n",
    "                structure = parser.get_structure('protein', pdb_path)\n",
    "                model = structure[0]\n",
    "\n",
    "                # Get residues and create features\n",
    "                residues = list(model.get_residues())\n",
    "\n",
    "                # Create neighbor search for the entire structure\n",
    "                atom_list = Selection.unfold_entities(model, 'A')\n",
    "                ns = NeighborSearch(atom_list)\n",
    "\n",
    "                # Process residues and collect statistics\n",
    "                node_features = []\n",
    "                for residue in residues:\n",
    "                    center = residue['CA'].get_coord() if 'CA' in residue else None\n",
    "                    if center is not None:\n",
    "                        neighbors = ns.search(center, 5.0, level='R')\n",
    "                        features = self._get_residue_features(residue, neighbors)\n",
    "                        node_features.append(features)\n",
    "\n",
    "                        # Collect statistics\n",
    "                        all_coords.extend(features[21:24])\n",
    "                        all_masses.append(features[24])\n",
    "                        all_distances.extend(features[25:27])\n",
    "                        all_counts.append(features[27])\n",
    "\n",
    "                # Create edges with 5Ã… cutoff\n",
    "                edges = []\n",
    "                for i in range(len(residues)):\n",
    "                    for j in range(i+1, len(residues)):\n",
    "                        if 'CA' in residues[i] and 'CA' in residues[j]:\n",
    "                            ca_i = residues[i]['CA'].get_coord()\n",
    "                            ca_j = residues[j]['CA'].get_coord()\n",
    "                            dist = np.linalg.norm(ca_i - ca_j)\n",
    "                            if dist < 5.0:\n",
    "                                edges.append([i, j])\n",
    "                                edges.append([j, i])\n",
    "\n",
    "                if len(edges) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Create PyG Data object\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "                x = torch.tensor(node_features, dtype=torch.float)\n",
    "                y = torch.tensor([class_label], dtype=torch.long)\n",
    "\n",
    "                data = Data(x=x, edge_index=edge_index, y=y, num_nodes=len(residues))\n",
    "\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                data_list.append(data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdb_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if len(data_list) == 0:\n",
    "            raise RuntimeError(\"No data was successfully processed!\")\n",
    "\n",
    "        # Calculate normalization parameters\n",
    "        all_coords = np.array(all_coords)\n",
    "        all_masses = np.array(all_masses)\n",
    "        all_distances = np.array(all_distances)\n",
    "        all_counts = np.array(all_counts)\n",
    "\n",
    "        # Save normalization parameters\n",
    "        self.coord_mean = float(np.mean(all_coords))\n",
    "        self.coord_std = float(np.std(all_coords))\n",
    "        self.mass_mean = float(np.mean(all_masses))\n",
    "        self.mass_std = float(np.std(all_masses))\n",
    "        self.dist_mean = float(np.mean(all_distances))\n",
    "        self.dist_std = float(np.std(all_distances))\n",
    "        self.count_mean = float(np.mean(all_counts))\n",
    "        self.count_std = float(np.std(all_counts))\n",
    "\n",
    "        # Create processed directory if it doesn't exist\n",
    "        os.makedirs(self.processed_dir, exist_ok=True)\n",
    "\n",
    "        # Save both the processed data and normalization parameters\n",
    "        torch.save({\n",
    "            'data_list': data_list,\n",
    "            'normalization': {\n",
    "                'coord_mean': self.coord_mean,\n",
    "                'coord_std': self.coord_std,\n",
    "                'mass_mean': self.mass_mean,\n",
    "                'mass_std': self.mass_std,\n",
    "                'dist_mean': self.dist_mean,\n",
    "                'dist_std': self.dist_std,\n",
    "                'count_mean': self.count_mean,\n",
    "                'count_std': self.count_std\n",
    "            }\n",
    "        }, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Get a specific graph from the dataset.\"\"\"\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "            if not os.path.exists(processed_path):\n",
    "                print(\"Warning: Processed data file not found, running processing...\")\n",
    "                self.process()\n",
    "            # Add weights_only=False to allow loading PyG Data objects\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "        return self._data_list[idx]\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"Return the number of graphs in the dataset.\"\"\"\n",
    "        processed_path = os.path.join(self.processed_dir, 'data.pt')\n",
    "        if not os.path.exists(processed_path):\n",
    "            print(\"Warning: Processed data file not found, running processing...\")\n",
    "            self.process()\n",
    "\n",
    "        if not hasattr(self, '_data_list'):\n",
    "            # Add weights_only=False here as well\n",
    "            self._data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "        print(f\"Actual number of processed samples: {len(self._data_list)}\")\n",
    "        return len(self._data_list)\n",
    "\n",
    "    def _get_residue_features(self, residue, neighbors):\n",
    "        \"\"\"Create expanded feature vector for a residue.\"\"\"\n",
    "        # One-hot encode amino acid type\n",
    "        aa_features = np.zeros(21)  # 20 standard amino acids + UNK\n",
    "        aa_name = residue.get_resname()\n",
    "        aa_info = self.amino_acids.get(aa_name, self.amino_acids['UNK'])\n",
    "        aa_features[aa_info['index']] = 1\n",
    "\n",
    "        # Get mass\n",
    "        mass = aa_info['mass']\n",
    "\n",
    "        # Get CA atom coordinates\n",
    "        try:\n",
    "            ca_atom = residue['CA']\n",
    "            coords = ca_atom.get_coord()\n",
    "        except:\n",
    "            coords = np.zeros(3)\n",
    "\n",
    "        # Calculate neighborhood features\n",
    "        if neighbors:\n",
    "            neighbor_distances = []\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor != residue and 'CA' in neighbor:\n",
    "                    dist = np.linalg.norm(coords - neighbor['CA'].get_coord())\n",
    "                    neighbor_distances.append(dist)\n",
    "\n",
    "            avg_neighbor_dist = np.mean(neighbor_distances) if neighbor_distances else 0\n",
    "            max_neighbor_dist = np.max(neighbor_distances) if neighbor_distances else 0\n",
    "            neighbor_count = len(neighbor_distances)\n",
    "        else:\n",
    "            avg_neighbor_dist = 0\n",
    "            max_neighbor_dist = 0\n",
    "            neighbor_count = 0\n",
    "\n",
    "        # Combine all features\n",
    "        features = np.concatenate([\n",
    "            aa_features,          # Amino acid identity (21)\n",
    "            coords,              # 3D coordinates (3)\n",
    "            [mass],             # Mass (1)\n",
    "            [avg_neighbor_dist], # Average neighbor distance (1)\n",
    "            [max_neighbor_dist], # Maximum neighbor distance (1)\n",
    "            [neighbor_count]     # Number of neighbors (1)\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Return the number of classes in the dataset.\"\"\"\n",
    "        return len(self.class_mapping)\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        \"\"\"Return the number of node features.\"\"\"\n",
    "        return 28  # 21 for amino acids + 3 for coordinates"
   ],
   "id": "8eee4b4a0d18bcc8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:04:51.462843Z",
     "start_time": "2025-02-16T23:03:59.335559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the root directory\n",
    "data_dir = 'data/SCOP'  # Adjust this to your actual data directory path\n",
    "\n",
    "# Create the dataset (this should trigger the process method)\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "# Create the dataset instance\n",
    "\n",
    "# Explicitly call the process method\n",
    "processed_data = dataset.process()"
   ],
   "id": "b40a91a41180d6ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class info file with 350 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 410195: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 410195: 0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:06:49.780594Z",
     "start_time": "2025-02-16T23:06:49.705058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "NUM_CLASSES = 7  # SCOP main classes\n",
    "# First, let's check protein sizes\n",
    "# First, analyze protein sizes\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "sizes = [data.num_nodes for data in data_list]\n",
    "\n",
    "print(f\"Protein size statistics:\")\n",
    "print(f\"Min size: {min(sizes)}\")\n",
    "print(f\"Max size: {max(sizes)}\")\n",
    "print(f\"Mean size: {sum(sizes)/len(sizes):.1f}\")\n",
    "print(f\"Median size: {sorted(sizes)[len(sizes)//2]}\")\n",
    "print(f\"Number of proteins > 150 residues: {sum(1 for s in sizes if s > 150)}\")\n",
    "\n",
    "# Set max_nodes to 500 since we have proteins up to 1381 residues\n",
    "max_nodes = 1400\n",
    "\n",
    "# Create dataset\n",
    "#dataset = SparseSCOPDataset(\n",
    "#    root=data_dir,\n",
    "#    pre_filter=lambda data: data.num_nodes <= max_nodes\n",
    "#)\n",
    "\n",
    "#dataset = SparseSCOPDataset(root=data_dir)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n"
   ],
   "id": "81c665801b8fe044",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'num_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m processed_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed/data.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     15\u001B[0m data_list \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(processed_path, weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 16\u001B[0m sizes \u001B[38;5;241m=\u001B[39m [\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_nodes\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m data_list]\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProtein size statistics:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMin size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mmin\u001B[39m(sizes)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'num_nodes'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:34:32.260464Z",
     "start_time": "2025-02-16T19:34:32.194108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming your dataset is loaded\n",
    "filtered_dataset = [data for data in dataset if data.num_nodes < 300]\n",
    "\n",
    "# If you want to create a new dataset object\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class FilteredSCOPDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.data_list = [data for data in original_dataset if data.num_nodes < 300]\n",
    "        super().__init__(original_dataset.root)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create the filtered dataset\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Verify the filtering\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Optional: Check distribution across classes\n",
    "class_distribution = {}\n",
    "for data in filtered_dataset:\n",
    "    class_label = data.y.item()\n",
    "    class_distribution[class_label] = class_distribution.get(class_label, 0) + 1\n",
    "\n",
    "print(\"\\nClass distribution in filtered dataset:\")\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"Class {cls}: {count} proteins\")"
   ],
   "id": "755cc2d19aa5e717",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 3420\n",
      "Filtered dataset size: 2717\n",
      "\n",
      "Class distribution in filtered dataset:\n",
      "Class 1: 449 proteins\n",
      "Class 2: 368 proteins\n",
      "Class 0: 428 proteins\n",
      "Class 6: 500 proteins\n",
      "Class 5: 418 proteins\n",
      "Class 3: 382 proteins\n",
      "Class 4: 172 proteins\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:05:38.228841Z",
     "start_time": "2025-02-16T23:05:12.061154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the root directory\n",
    "data_dir = 'data/SCOP'  # Adjust this to your actual data directory path\n",
    "\n",
    "# Create the dataset (this should trigger the process method)\n",
    "dataset = SparseSCOPDataset(root=data_dir)\n",
    "# Create the dataset instance\n",
    "\n",
    "# Explicitly call the process method\n",
    "processed_data = dataset.process()"
   ],
   "id": "326dab61158a6c1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class info file with 350 entries\n",
      "Error processing 410195: 0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:35:39.969356Z",
     "start_time": "2025-02-16T19:35:39.927705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the filtered dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "filtered_dataset = FilteredSCOPDataset(dataset)\n",
    "\n",
    "# Shuffle and split the filtered dataset\n",
    "filtered_dataset = filtered_dataset.shuffle()\n",
    "n = (len(filtered_dataset) + 9) // 10\n",
    "test_dataset = filtered_dataset[:n]\n",
    "val_dataset = filtered_dataset[n:2 * n]\n",
    "train_dataset = filtered_dataset[2 * n:]\n",
    "\n",
    "# Create data loaders using the filtered datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"\\nFiltered Dataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# The rest of your training loop remains the same\n",
    "# You can use these loaders directly in your existing training script"
   ],
   "id": "510af8d5deb60124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Dataset splits:\n",
      "Training samples: 2173\n",
      "Validation samples: 272\n",
      "Test samples: 272\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T19:31:11.960641Z",
     "start_time": "2025-02-16T19:31:11.819218Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "243e6c1a459fa5a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 2736\n",
      "Validation samples: 342\n",
      "Test samples: 342\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T23:06:21.803095Z",
     "start_time": "2025-02-16T23:06:21.275142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you've already loaded the dataset\n",
    "# Load the processed data\n",
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "# Reverse class mapping for readable labels\n",
    "class_mapping_reverse = {\n",
    "    0: 'a (All-alpha)',\n",
    "    1: 'b (All-beta)',\n",
    "    2: 'c (Alpha/beta)',\n",
    "    3: 'd (Alpha+beta)',\n",
    "    4: 'e (Multi-domain)',\n",
    "    5: 'f (Membrane)',\n",
    "    6: 'g (Small proteins)'\n",
    "}\n",
    "\n",
    "# Separate nodes by class\n",
    "nodes_by_class = {}\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in nodes_by_class:\n",
    "        nodes_by_class[class_label] = []\n",
    "    nodes_by_class[class_label].append(data.num_nodes)\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Box plot\n",
    "plt.boxplot([nodes_by_class[key] for key in sorted(nodes_by_class.keys())],\n",
    "            labels=[class_mapping_reverse[key] for key in sorted(nodes_by_class.keys())])\n",
    "\n",
    "plt.title('Number of Nodes per SCOP Class', fontsize=16)\n",
    "plt.xlabel('SCOP Class', fontsize=12)\n",
    "plt.ylabel('Number of Nodes (Residues)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('nodes_per_class_boxplot.png')\n",
    "plt.close()\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Node count statistics per class:\")\n",
    "for class_label, nodes in nodes_by_class.items():\n",
    "    print(f\"\\n{class_mapping_reverse[class_label]}:\")\n",
    "    print(f\"  Count: {len(nodes)}\")\n",
    "    print(f\"  Min nodes: {min(nodes)}\")\n",
    "    print(f\"  Max nodes: {max(nodes)}\")\n",
    "    print(f\"  Mean nodes: {np.mean(nodes):.2f}\")\n",
    "    print(f\"  Median nodes: {np.median(nodes):.2f}\")"
   ],
   "id": "27dc0cc370f2c3dc",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'y'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 24\u001B[0m\n\u001B[1;32m     22\u001B[0m nodes_by_class \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m data_list:\n\u001B[0;32m---> 24\u001B[0m     class_label \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my\u001B[49m\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m class_label \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m nodes_by_class:\n\u001B[1;32m     26\u001B[0m         nodes_by_class[class_label] \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'y'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:48:42.206202Z",
     "start_time": "2025-02-16T22:48:41.954187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_path = os.path.join(data_dir, 'processed/data.pt')\n",
    "data_list = torch.load(processed_path, weights_only=False)\n",
    "\n",
    "# Count proteins per class and above 300 nodes\n",
    "class_counts = {}\n",
    "above_300_counts = {}\n",
    "\n",
    "for data in data_list:\n",
    "    class_label = data.y.item()\n",
    "    if class_label not in class_counts:\n",
    "        class_counts[class_label] = 0\n",
    "        above_300_counts[class_label] = 0\n",
    "\n",
    "    class_counts[class_label] += 1\n",
    "    if data.num_nodes > 300:\n",
    "        above_300_counts[class_label] += 1\n",
    "\n",
    "print(\"Total proteins per class:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"Class {cls}: {count} total, {above_300_counts[cls]} above 300 nodes ({above_300_counts[cls]/count*100:.2f}%)\")"
   ],
   "id": "e76168280612488f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins per class:\n",
      "Class 0: 50 total, 4 above 300 nodes (8.00%)\n",
      "Class 1: 50 total, 5 above 300 nodes (10.00%)\n",
      "Class 2: 50 total, 18 above 300 nodes (36.00%)\n",
      "Class 3: 48 total, 2 above 300 nodes (4.17%)\n",
      "Class 4: 50 total, 34 above 300 nodes (68.00%)\n",
      "Class 5: 48 total, 12 above 300 nodes (25.00%)\n",
      "Class 6: 49 total, 0 above 300 nodes (0.00%)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:53:20.350917Z",
     "start_time": "2025-02-16T22:53:20.325944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from math import ceil\n",
    "\n",
    "class SparseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # GNN layers with sparse representation\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First convolution layer\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = F.relu(h1)\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Second convolution layer\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = F.relu(h2)\n",
    "        h2 = self.dropout(h2)\n",
    "\n",
    "        # Third convolution layer\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "        h3 = self.bn3(h3)\n",
    "        h3 = F.relu(h3)\n",
    "        h3 = self.dropout(h3)\n",
    "\n",
    "        # Global mean pooling\n",
    "        out = global_mean_pool(h3, batch)\n",
    "\n",
    "        # MLP head\n",
    "        out = self.lin1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a single GNN model for classification\n",
    "        self.gnn = SparseGNN(\n",
    "            num_features=30,\n",
    "            hidden_dim=64,\n",
    "            num_classes=dataset.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Single forward pass through the GNN\n",
    "        x = self.gnn(x, edge_index, batch)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"Determine the best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    \"\"\"Training function for the model.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    \"\"\"Evaluation function for the model.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def main(dataset, train_loader, val_loader, test_loader):\n",
    "    \"\"\"Main training and evaluation loop.\"\"\"\n",
    "    device = setup_device()\n",
    "    model = Net(dataset).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for epoch in range(1, 151):\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        val_acc = test(model, val_loader, device)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc = test(model, test_loader, device)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return model, best_val_acc, test_acc\n",
    "\n",
    "# Note: This function would be called after setting up the dataset, loaders, etc.\n",
    "# main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "738471d4eece3147",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:54:11.471599Z",
     "start_time": "2025-02-16T22:53:23.141164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data loaders (as you had before)\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Use DataLoader for sparse graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20)\n",
    "\n",
    "# Train the model\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)"
   ],
   "id": "e4d1fde02103aab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.9375, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 002, Train Loss: 1.8775, Val Acc: 0.1143, Test Acc: 0.2857\n",
      "Epoch: 003, Train Loss: 1.8617, Val Acc: 0.1143, Test Acc: 0.2857\n",
      "Epoch: 004, Train Loss: 1.8325, Val Acc: 0.1429, Test Acc: 0.2857\n",
      "Epoch: 005, Train Loss: 1.8148, Val Acc: 0.1429, Test Acc: 0.2857\n",
      "Epoch: 006, Train Loss: 1.7917, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 007, Train Loss: 1.7633, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 008, Train Loss: 1.7381, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 009, Train Loss: 1.7183, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 010, Train Loss: 1.6819, Val Acc: 0.1714, Test Acc: 0.2857\n",
      "Epoch: 011, Train Loss: 1.6677, Val Acc: 0.2286, Test Acc: 0.1714\n",
      "Epoch: 012, Train Loss: 1.6343, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 013, Train Loss: 1.6001, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 014, Train Loss: 1.6009, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 015, Train Loss: 1.5384, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 016, Train Loss: 1.4962, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 017, Train Loss: 1.4695, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 018, Train Loss: 1.4716, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 019, Train Loss: 1.4231, Val Acc: 0.1143, Test Acc: 0.1714\n",
      "Epoch: 020, Train Loss: 1.4162, Val Acc: 0.1143, Test Acc: 0.1714\n",
      "Epoch: 021, Train Loss: 1.3827, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 022, Train Loss: 1.3600, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 023, Train Loss: 1.3062, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 024, Train Loss: 1.3085, Val Acc: 0.1143, Test Acc: 0.1714\n",
      "Epoch: 025, Train Loss: 1.2988, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 026, Train Loss: 1.2620, Val Acc: 0.1143, Test Acc: 0.1714\n",
      "Epoch: 027, Train Loss: 1.2401, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 028, Train Loss: 1.2024, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 029, Train Loss: 1.1772, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 030, Train Loss: 1.1690, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 031, Train Loss: 1.1348, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 032, Train Loss: 1.1407, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 033, Train Loss: 1.1550, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 034, Train Loss: 1.1415, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 035, Train Loss: 1.0784, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 036, Train Loss: 1.0096, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 037, Train Loss: 1.0115, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 038, Train Loss: 0.9874, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 039, Train Loss: 0.9416, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 040, Train Loss: 0.9455, Val Acc: 0.2286, Test Acc: 0.1714\n",
      "Epoch: 041, Train Loss: 0.9117, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 042, Train Loss: 0.9201, Val Acc: 0.0857, Test Acc: 0.1714\n",
      "Epoch: 043, Train Loss: 0.9544, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 044, Train Loss: 0.9302, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 045, Train Loss: 0.8988, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 046, Train Loss: 0.8509, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 047, Train Loss: 0.8282, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 048, Train Loss: 0.8299, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 049, Train Loss: 0.7431, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 050, Train Loss: 0.7765, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 051, Train Loss: 0.8092, Val Acc: 0.2286, Test Acc: 0.1714\n",
      "Epoch: 052, Train Loss: 0.7862, Val Acc: 0.1429, Test Acc: 0.1714\n",
      "Epoch: 053, Train Loss: 0.7794, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 054, Train Loss: 0.7305, Val Acc: 0.2286, Test Acc: 0.1714\n",
      "Epoch: 055, Train Loss: 0.6998, Val Acc: 0.1714, Test Acc: 0.1714\n",
      "Epoch: 056, Train Loss: 0.6977, Val Acc: 0.2000, Test Acc: 0.1714\n",
      "Epoch: 057, Train Loss: 0.6751, Val Acc: 0.2857, Test Acc: 0.2000\n",
      "Epoch: 058, Train Loss: 0.7125, Val Acc: 0.1714, Test Acc: 0.2000\n",
      "Epoch: 059, Train Loss: 0.7041, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 060, Train Loss: 0.6797, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 061, Train Loss: 0.6399, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 062, Train Loss: 0.6575, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 063, Train Loss: 0.5973, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 064, Train Loss: 0.5941, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 065, Train Loss: 0.5696, Val Acc: 0.2857, Test Acc: 0.2000\n",
      "Epoch: 066, Train Loss: 0.5835, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 067, Train Loss: 0.5391, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 068, Train Loss: 0.6008, Val Acc: 0.1429, Test Acc: 0.2000\n",
      "Epoch: 069, Train Loss: 0.5829, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 070, Train Loss: 0.5375, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 071, Train Loss: 0.5632, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 072, Train Loss: 0.5779, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 073, Train Loss: 0.5881, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 074, Train Loss: 0.5550, Val Acc: 0.2000, Test Acc: 0.2000\n",
      "Epoch: 075, Train Loss: 0.5740, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 076, Train Loss: 0.6838, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 077, Train Loss: 0.5191, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 078, Train Loss: 0.4836, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 079, Train Loss: 0.4418, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 080, Train Loss: 0.4254, Val Acc: 0.2571, Test Acc: 0.2000\n",
      "Epoch: 081, Train Loss: 0.3855, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 082, Train Loss: 0.3799, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 083, Train Loss: 0.3897, Val Acc: 0.2286, Test Acc: 0.2000\n",
      "Epoch: 084, Train Loss: 0.3477, Val Acc: 0.1714, Test Acc: 0.2000\n",
      "Epoch: 085, Train Loss: 0.3901, Val Acc: 0.3143, Test Acc: 0.1143\n",
      "Epoch: 086, Train Loss: 0.3904, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 087, Train Loss: 0.4044, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 088, Train Loss: 0.3705, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 089, Train Loss: 0.3734, Val Acc: 0.2857, Test Acc: 0.1143\n",
      "Epoch: 090, Train Loss: 0.4343, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 091, Train Loss: 0.4867, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 092, Train Loss: 0.4475, Val Acc: 0.3143, Test Acc: 0.1143\n",
      "Epoch: 093, Train Loss: 0.4218, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 094, Train Loss: 0.3893, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 095, Train Loss: 0.3811, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 096, Train Loss: 0.3832, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 097, Train Loss: 0.3125, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 098, Train Loss: 0.3632, Val Acc: 0.1429, Test Acc: 0.1143\n",
      "Epoch: 099, Train Loss: 0.4473, Val Acc: 0.3143, Test Acc: 0.1143\n",
      "Epoch: 100, Train Loss: 0.3927, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 101, Train Loss: 0.3672, Val Acc: 0.1429, Test Acc: 0.1143\n",
      "Epoch: 102, Train Loss: 0.2746, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 103, Train Loss: 0.3444, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 104, Train Loss: 0.3002, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 105, Train Loss: 0.2761, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 106, Train Loss: 0.2938, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 107, Train Loss: 0.3067, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 108, Train Loss: 0.3132, Val Acc: 0.1143, Test Acc: 0.1143\n",
      "Epoch: 109, Train Loss: 0.3515, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 110, Train Loss: 0.2988, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 111, Train Loss: 0.3137, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 112, Train Loss: 0.2974, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 113, Train Loss: 0.2643, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 114, Train Loss: 0.2354, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 115, Train Loss: 0.2369, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 116, Train Loss: 0.2382, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 117, Train Loss: 0.2527, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 118, Train Loss: 0.2557, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 119, Train Loss: 0.2670, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 120, Train Loss: 0.2646, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 121, Train Loss: 0.2532, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 122, Train Loss: 0.2767, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 123, Train Loss: 0.2864, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 124, Train Loss: 0.2352, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 125, Train Loss: 0.2151, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 126, Train Loss: 0.2408, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 127, Train Loss: 0.2405, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 128, Train Loss: 0.2357, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 129, Train Loss: 0.1975, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 130, Train Loss: 0.1896, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 131, Train Loss: 0.1919, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 132, Train Loss: 0.2190, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 133, Train Loss: 0.2935, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 134, Train Loss: 0.1734, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 135, Train Loss: 0.1619, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 136, Train Loss: 0.1743, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 137, Train Loss: 0.1820, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 138, Train Loss: 0.1646, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 139, Train Loss: 0.1563, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 140, Train Loss: 0.1570, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 141, Train Loss: 0.2084, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 142, Train Loss: 0.1483, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 143, Train Loss: 0.1295, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 144, Train Loss: 0.1283, Val Acc: 0.2571, Test Acc: 0.1143\n",
      "Epoch: 145, Train Loss: 0.1470, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 146, Train Loss: 0.1818, Val Acc: 0.2000, Test Acc: 0.1143\n",
      "Epoch: 147, Train Loss: 0.2037, Val Acc: 0.1429, Test Acc: 0.1143\n",
      "Epoch: 148, Train Loss: 0.2743, Val Acc: 0.2286, Test Acc: 0.1143\n",
      "Epoch: 149, Train Loss: 0.2431, Val Acc: 0.1714, Test Acc: 0.1143\n",
      "Epoch: 150, Train Loss: 0.2162, Val Acc: 0.2286, Test Acc: 0.1143\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T22:50:48.551762Z",
     "start_time": "2025-02-16T22:50:48.342214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "\n",
    "# Replace your existing DataLoader with this\n",
    "train_loader = DataLoader(train_dataset,  batch_size=20)\n",
    "val_loader = DataLoader(val_dataset,  batch_size=20)\n",
    "test_loader = DataLoader(test_dataset,  batch_size=20)\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "model, best_val_acc, test_acc = main(dataset, train_loader, val_loader, test_loader)\n"
   ],
   "id": "db76b9390eadc048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training samples: 275\n",
      "Validation samples: 35\n",
      "Test samples: 35\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (4338x30 and 24x64)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation samples: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(val_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest samples: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m model, best_val_acc, test_acc \u001B[38;5;241m=\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[16], line 129\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(dataset, train_loader, val_loader, test_loader)\u001B[0m\n\u001B[1;32m    126\u001B[0m test_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m151\u001B[39m):\n\u001B[0;32m--> 129\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    130\u001B[0m     val_acc \u001B[38;5;241m=\u001B[39m test(model, val_loader, device)\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m val_acc \u001B[38;5;241m>\u001B[39m best_val_acc:\n",
      "Cell \u001B[0;32mIn[16], line 93\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, optimizer, device)\u001B[0m\n\u001B[1;32m     91\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     92\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 93\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnll_loss(output, data\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     95\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[16], line 71\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, x, edge_index, batch)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index, batch):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;66;03m# Single forward pass through the GNN\u001B[39;00m\n\u001B[0;32m---> 71\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlog_softmax(x, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[16], line 29\u001B[0m, in \u001B[0;36mSparseGNN.forward\u001B[0;34m(self, x, edge_index, batch)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index, batch):\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m# First convolution layer\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m     h1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m     h1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(h1)\n\u001B[1;32m     31\u001B[0m     h1 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(h1)\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch_geometric/nn/conv/sage_conv.py:135\u001B[0m, in \u001B[0;36mSAGEConv.forward\u001B[0;34m(self, x, edge_index, size)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001B[39;00m\n\u001B[1;32m    134\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpropagate(edge_index, x\u001B[38;5;241m=\u001B[39mx, size\u001B[38;5;241m=\u001B[39msize)\n\u001B[0;32m--> 135\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlin_l\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m x_r \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_weight \u001B[38;5;129;01mand\u001B[39;00m x_r \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Downloads/Graph_Project/Graph_project/venv/lib/python3.12/site-packages/torch_geometric/nn/dense/linear.py:147\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    142\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Forward pass.\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \n\u001B[1;32m    144\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;124;03m        x (torch.Tensor): The input features.\u001B[39;00m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: linear(): input and weight.T shapes cannot be multiplied (4338x30 and 24x64)"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
